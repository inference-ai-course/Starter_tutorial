{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Function Calling and Structured Outputs\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Implement function calling with JSON Schema validation\n",
    "- Create structured output generators with proper constraints\n",
    "- Build cross-platform solutions using HuggingFace and local endpoints\n",
    "- Evaluate and compare different AI providers\n",
    "- Handle errors and edge cases gracefully\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai huggingface_hub transformers jsonschema requests python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Let's start by setting up our environment and API credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from typing import Dict, Any, List, Optional\n",
    "from jsonschema import validate, ValidationError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API Keys (use environment variables in production)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"your-openai-key-here\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"your-huggingface-token-here\")\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"OpenAI API Key configured: {'Yes' if OPENAI_API_KEY != 'your-openai-key-here' else 'No'}\")\n",
    "print(f\"HuggingFace Token configured: {'Yes' if HF_TOKEN != 'your-huggingface-token-here' else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Basic Function Calling Setup\n",
    "\n",
    "Let's start with a simple function calling example using a coffee recipe generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our function implementations\n",
    "def make_coffee(coffee_type: str) -> Dict[str, Any]:\n",
    "    \"\"\"Generate a coffee recipe based on type.\"\"\"\n",
    "    recipes = {\n",
    "        \"espresso\": {\n",
    "            \"coffee_grams\": 18,\n",
    "            \"water_ml\": 36,\n",
    "            \"brew_time_seconds\": 25,\n",
    "            \"temperature_celsius\": 93,\n",
    "            \"pressure_bar\": 9\n",
    "        },\n",
    "        \"cappuccino\": {\n",
    "            \"coffee_grams\": 18,\n",
    "            \"water_ml\": 36,\n",
    "            \"milk_ml\": 120,\n",
    "            \"milk_foam\": \"thick\",\n",
    "            \"brew_time_seconds\": 25\n",
    "        },\n",
    "        \"latte\": {\n",
    "            \"coffee_grams\": 18,\n",
    "            \"water_ml\": 36,\n",
    "            \"milk_ml\": 240,\n",
    "            \"milk_foam\": \"thin\",\n",
    "            \"brew_time_seconds\": 25\n",
    "        },\n",
    "        \"americano\": {\n",
    "            \"coffee_grams\": 18,\n",
    "            \"water_ml\": 36,\n",
    "            \"additional_water_ml\": 120,\n",
    "            \"brew_time_seconds\": 25\n",
    "        }\n",
    "    }\n",
    "    return recipes.get(coffee_type, {\"error\": \"Recipe not found\"})\n",
    "\n",
    "def random_coffee_fact() -> Dict[str, Any]:\n",
    "    \"\"\"Return a random coffee fact.\"\"\"\n",
    "    facts = [\n",
    "        {\"fact\": \"Coffee was first discovered in Ethiopia around 850 AD\", \"source\": \"Historical records\"},\n",
    "        {\"fact\": \"Espresso means 'pressed out' in Italian\", \"source\": \"Italian etymology\"},\n",
    "        {\"fact\": \"Coffee is the world's second-most traded commodity after oil\", \"source\": \"Commodity markets\"},\n",
    "        {\"fact\": \"The average American consumes 3.1 cups of coffee per day\", \"source\": \"National Coffee Association\"},\n",
    "        {\"fact\": \"Coffee beans are actually seeds from coffee cherries\", \"source\": \"Botanical classification\"}\n",
    "    ]\n",
    "    return random.choice(facts)\n",
    "\n",
    "# Define tool schemas for the AI model\n",
    "coffee_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"make_coffee\",\n",
    "            \"description\": \"Generate a coffee recipe for the specified type\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"coffee_type\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"espresso\", \"cappuccino\", \"latte\", \"americano\"],\n",
    "                        \"description\": \"Type of coffee to make\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"coffee_type\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"random_coffee_fact\",\n",
    "            \"description\": \"Get a random interesting fact about coffee\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Function definitions created!\")\n",
    "print(f\"Available tools: {[tool['function']['name'] for tool in coffee_tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: OpenAI Function Calling Implementation\n",
    "\n",
    "Now let's implement function calling with OpenAI's API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from openai import OpenAI\n",
    "    \n",
    "    # Initialize OpenAI client\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    def handle_openai_function_call(messages, tools):\n",
    "        \"\"\"Handle function calling with OpenAI API.\"\"\"\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "        \n",
    "        response_message = response.choices[0].message\n",
    "        \n",
    "        # Check if the model wants to call a function\n",
    "        if response_message.tool_calls:\n",
    "            messages.append(response_message)\n",
    "            \n",
    "            # Handle each function call\n",
    "            for tool_call in response_message.tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                \n",
    "                print(f\"Calling function: {function_name}\")\n",
    "                print(f\"Arguments: {function_args}\")\n",
    "                \n",
    "                # Call the actual function\n",
    "                if function_name == \"make_coffee\":\n",
    "                    function_response = make_coffee(**function_args)\n",
    "                elif function_name == \"random_coffee_fact\":\n",
    "                    function_response = random_coffee_fact()\n",
    "                else:\n",
    "                    function_response = {\"error\": f\"Unknown function: {function_name}\"}\n",
    "                \n",
    "                # Add function response to messages\n",
    "                messages.append({\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": json.dumps(function_response)\n",
    "                })\n",
    "            \n",
    "            # Get final response from model\n",
    "            final_response = openai_client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=messages\n",
    "            )\n",
    "            \n",
    "            return final_response.choices[0].message.content\n",
    "        \n",
    "        return response_message.content\n",
    "    \n",
    "    # Test the function calling\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful coffee assistant. Use the available tools to help users with coffee-related questions.\"},\n",
    "        {\"role\": \"user\", \"content\": \"I'd like to make a cappuccino. Can you give me the recipe?\"}\n",
    "    ]\n",
    "    \n",
    "    result = handle_openai_function_call(messages, coffee_tools)\n",
    "    print(\"\\nFinal response:\")\n",
    "    print(result)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"OpenAI library not installed. Skipping this exercise.\")\n",
    "except Exception as e:\n",
    "    print(f\"OpenAI API error: {e}\")\n",
    "    print(\"Skipping OpenAI exercise. Proceeding with HuggingFace alternatives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: HuggingFace Implementation\n",
    "\n",
    "Let's implement the same functionality using HuggingFace's open-source tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from huggingface_hub import InferenceClient\n",
    "    \n",
    "    # Initialize HuggingFace client\n",
    "    hf_client = InferenceClient(token=HF_TOKEN)\n",
    "    \n",
    "    def create_structured_prompt(user_query: str, tools: List[Dict]) -> str:\n",
    "        \"\"\"Create a structured prompt for HuggingFace models.\"\"\"\n",
    "        tool_descriptions = []\n",
    "        for tool in tools:\n",
    "            func = tool['function']\n",
    "            tool_descriptions.append(f\"\"\"\n",
    "Tool: {func['name']}\n",
    "Description: {func['description']}\n",
    "Parameters: {json.dumps(func['parameters'], indent=2)}\n",
    "            \"\"\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a helpful assistant with access to the following tools:\n",
    "\n",
    "{chr(10).join(tool_descriptions)}\n",
    "\n",
    "User Query: {user_query}\n",
    "\n",
    "If you need to use a tool, respond with exactly this format:\n",
    "TOOL_CALL: {{\"name\": \"tool_name\", \"arguments\": {{\"param1\": \"value1\"}}}}\n",
    "\n",
    "If no tool is needed, respond naturally.\n",
    "        \"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def parse_tool_call(response_text: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Parse tool call from model response.\"\"\"\n",
    "        if \"TOOL_CALL:\" in response_text:\n",
    "            try:\n",
    "                # Extract JSON after TOOL_CALL:\n",
    "                json_start = response_text.find(\"TOOL_CALL:\") + len(\"TOOL_CALL:\")\n",
    "                json_str = response_text[json_start:].strip()\n",
    "                return json.loads(json_str)\n",
    "            except json.JSONDecodeError:\n",
    "                return None\n",
    "        return None\n",
    "    \n",
    "    def handle_hf_function_call(user_query: str, tools: List[Dict], available_functions: Dict) -> str:\n",
    "        \"\"\"Handle function calling with HuggingFace models.\"\"\"\n",
    "        prompt = create_structured_prompt(user_query, tools)\n",
    "        \n",
    "        # Use a conversational model\n",
    "        response = hf_client.text_generation(\n",
    "            prompt,\n",
    "            model=\"microsoft/DialoGPT-medium\",\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        print(f\"Model response: {response}\")\n",
    "        \n",
    "        # Check for tool call\n",
    "        tool_call = parse_tool_call(response)\n",
    "        if tool_call:\n",
    "            function_name = tool_call['name']\n",
    "            function_args = tool_call['arguments']\n",
    "            \n",
    "            print(f\"Calling function: {function_name}\")\n",
    "            print(f\"Arguments: {function_args}\")\n",
    "            \n",
    "            # Call the actual function\n",
    "            if function_name in available_functions:\n",
    "                function_response = available_functions[function_name](**function_args)\n",
    "                \n",
    "                # Create follow-up prompt with function result\n",
    "                follow_up_prompt = f\"\"\"\n",
    "                User Query: {user_query}\n",
    "                Function Result: {json.dumps(function_response)}\n",
    "                \n",
    "                Provide a natural response to the user based on the function result.\n",
    "                \"\"\"\n",
    "                \n",
    "                final_response = hf_client.text_generation(\n",
    "                    follow_up_prompt,\n",
    "                    model=\"microsoft/DialoGPT-medium\",\n",
    "                    max_new_tokens=150,\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                \n",
    "                return final_response\n",
    "            else:\n",
    "                return f\"Error: Unknown function {function_name}\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    # Test with HuggingFace\n",
    "    available_functions = {\n",
    "        \"make_coffee\": make_coffee,\n",
    "        \"random_coffee_fact\": random_coffee_fact\n",
    "    }\n",
    "    \n",
    "    result = handle_hf_function_call(\n",
    "        \"I'd like to make a latte. Can you give me the recipe?\",\n",
    "        coffee_tools,\n",
    "        available_functions\n",
    "    )\n",
    "    \n",
    "    print(\"\\nHuggingFace result:\")\n",
    "    print(result)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"HuggingFace Hub library not installed. Skipping this exercise.\")\n",
    "except Exception as e:\n",
    "    print(f\"HuggingFace API error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Structured Output Validation\n",
    "\n",
    "Now let's create a robust validation system for structured outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_json_schema(data: Dict[str, Any], schema: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Validate data against JSON schema.\"\"\"\n",
    "    try:\n",
    "        validate(instance=data, schema=schema)\n",
    "        return True\n",
    "    except ValidationError as e:\n",
    "        print(f\"Validation error: {e.message}\")\n",
    "        return False\n",
    "\n",
    "def safe_json_parse(response_text: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Extract and parse JSON from potentially mixed responses.\"\"\"\n",
    "    import re\n",
    "    \n",
    "    try:\n",
    "        # Try direct parsing first\n",
    "        return json.loads(response_text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Extract JSON from code blocks\n",
    "        json_match = re.search(r'```json\\n(.*?)\\n```', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            return json.loads(json_match.group(1))\n",
    "        # Try to find JSON-like content\n",
    "        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            return json.loads(json_match.group(0))\n",
    "        return None\n",
    "\n",
    "# Define a schema for game character generation\n",
    "game_character_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"character\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"name\": {\"type\": \"string\", \"minLength\": 2, \"maxLength\": 20},\n",
    "                \"class\": {\"type\": \"string\", \"enum\": [\"warrior\", \"mage\", \"rogue\", \"cleric\"]},\n",
    "                \"health\": {\"type\": \"integer\", \"minimum\": 50, \"maximum\": 100},\n",
    "                \"mana\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 100},\n",
    "                \"strength\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 20},\n",
    "                \"intelligence\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 20}\n",
    "            },\n",
    "            \"required\": [\"name\", \"class\", \"health\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"backstory\": {\n",
    "            \"type\": \"string\",\n",
    "            \"minLength\": 50,\n",
    "            \"maxLength\": 500\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"character\", \"backstory\"],\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "\n",
    "print(\"Schema validation functions created!\")\n",
    "print(f\"Game character schema requires: {game_character_schema['required']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Provider Comparison Framework\n",
    "\n",
    "Let's create a framework to compare different AI providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_provider_performance(client, test_prompts: List[str], schema: Dict[str, Any], provider_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate provider performance across multiple metrics.\"\"\"\n",
    "    results = {\n",
    "        'provider': provider_name,\n",
    "        'total_tests': len(test_prompts),\n",
    "        'valid_json_count': 0,\n",
    "        'schema_compliant_count': 0,\n",
    "        'total_latency': 0,\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"Testing prompt {i+1}/{len(test_prompts)}: {prompt[:50]}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # This is a simplified version - adapt based on your client type\n",
    "            if hasattr(client, 'chat_completions'):\n",
    "                # OpenAI-style client\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": f\"Generate a game character. {prompt}\"}],\n",
    "                    response_format={\"type\": \"json_object\"}\n",
    "                )\n",
    "                response_text = response.choices[0].message.content\n",
    "            else:\n",
    "                # HuggingFace-style client\n",
    "                structured_prompt = f\"\"\"\n",
    "                Generate a game character based on this request: {prompt}\n",
    "                \n",
    "                Respond with valid JSON that matches this schema:\n",
    "                {json.dumps(game_character_schema, indent=2)}\n",
    "                \n",
    "                Output only the JSON, no additional text.\n",
    "                \"\"\"\n",
    "                response_text = client.text_generation(structured_prompt, max_new_tokens=500)\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            results['total_latency'] += latency\n",
    "            \n",
    "            # Parse and validate response\n",
    "            parsed_data = safe_json_parse(response_text)\n",
    "            if parsed_data:\n",
    "                results['valid_json_count'] += 1\n",
    "                if validate_json_schema(parsed_data, schema):\n",
    "                    results['schema_compliant_count'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            results['errors'].append(str(e))\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    # Calculate averages and percentages\n",
    "    if results['total_tests'] > 0:\n",
    "        results['valid_json_rate'] = (results['valid_json_count'] / results['total_tests']) * 100\n",
    "        results['schema_compliance_rate'] = (results['schema_compliant_count'] / results['total_tests']) * 100\n",
    "        results['avg_latency'] = results['total_latency'] / results['total_tests']\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test prompts for evaluation\n",
    "test_prompts = [\n",
    "    \"Create a brave warrior with high health and strength.\",\n",
    "    \"Generate an intelligent mage with powerful magic abilities.\",\n",
    "    \"Create a stealthy rogue character.\",\n",
    "    \"Make a wise cleric with healing powers.\",\n",
    "    \"Generate a balanced character with moderate stats.\"\n",
    "]\n",
    "\n",
    "print(\"Provider comparison framework created!\")\n",
    "print(f\"Test prompts prepared: {len(test_prompts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Local Model Integration with Ollama\n",
    "\n",
    "Let's test with a local Ollama instance (if available):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ollama_integration():\n",
    "    \"\"\"Test integration with local Ollama instance.\"\"\"\n",
    "    try:\n",
    "        # Check if Ollama is running\n",
    "        ollama_response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "        if ollama_response.status_code == 200:\n",
    "            print(\"Ollama is running!\")\n",
    "            models = ollama_response.json().get('models', [])\n",
    "            print(f\"Available models: {[model['name'] for model in models]}\")\n",
    "            \n",
    "            # Test with a simple prompt\n",
    "            test_payload = {\n",
    "                \"model\": \"llama2\" if any('llama2' in m['name'] for m in models) else models[0]['name'],\n",
    "                \"prompt\": \"Generate a simple JSON object with 'name' and 'age' fields.\",\n",
    "                \"format\": \"json\",\n",
    "                \"stream\": False\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                \"http://localhost:11434/api/generate\",\n",
    "                json=test_payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                print(f\"Ollama response: {result.get('response', 'No response')}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Ollama API error: {response.status_code}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"Ollama is not responding properly\")\n",
    "            return False\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Ollama is not running. Start it with: ollama serve\")\n",
    "        return False\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Ollama connection timed out\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Ollama: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test Ollama integration\n",
    "print(\"Testing Ollama integration...\")\n",
    "ollama_available = test_ollama_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Final Project - Game Character Generator\n",
    "\n",
    "Create a complete game character generator with validation and error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameCharacterGenerator:\n",
    "    \"\"\"A robust game character generator with multiple provider support.\"\"\"\n",
    "    \n",
    "    def __init__(self, provider: str = \"openai\", api_key: str = None):\n",
    "        self.provider = provider\n",
    "        self.api_key = api_key\n",
    "        self.client = self._initialize_client()\n",
    "    \n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initialize the appropriate client based on provider.\"\"\"\n",
    "        if self.provider == \"openai\":\n",
    "            try:\n",
    "                from openai import OpenAI\n",
    "                return OpenAI(api_key=self.api_key)\n",
    "            except ImportError:\n",
    "                raise ImportError(\"OpenAI library not installed\")\n",
    "        elif self.provider == \"huggingface\":\n",
    "            try:\n",
    "                from huggingface_hub import InferenceClient\n",
    "                return InferenceClient(token=self.api_key)\n",
    "            except ImportError:\n",
    "                raise ImportError(\"HuggingFace Hub library not installed\")\n",
    "        elif self.provider == \"ollama\":\n",
    "            return None  # Ollama uses HTTP requests\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown provider: {self.provider}\")\n",
    "    \n",
    "    def generate_character(self, user_request: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a game character with validation.\"\"\"\n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                if self.provider == \"openai\":\n",
    "                    response = self._generate_openai_character(user_request)\n",
    "                elif self.provider == \"huggingface\":\n",
    "                    response = self._generate_hf_character(user_request)\n",
    "                elif self.provider == \"ollama\":\n",
    "                    response = self._generate_ollama_character(user_request)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown provider: {self.provider}\")\n",
    "                \n",
    "                # Validate the response\n",
    "                if validate_json_schema(response, game_character_schema):\n",
    "                    return {\n",
    "                        \"success\": True,\n",
    "                        \"character\": response,\n",
    "                        \"attempts\": attempt + 1\n",
    "                    }\n",
    "                else:\n",
    "                    print(f\"Attempt {attempt + 1}: Schema validation failed\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1}: Error - {e}\")\n",
    "        \n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"Failed to generate valid character after maximum attempts\",\n",
    "            \"attempts\": max_attempts\n",
    "        }\n",
    "    \n",
    "    def _generate_openai_character(self, user_request: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate character using OpenAI.\"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"Generate a game character based on the user's request. Respond with valid JSON that matches this schema: {json.dumps(game_character_schema)}\"\n",
    "            }, {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_request\n",
    "            }],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        return safe_json_parse(response_text)\n",
    "    \n",
    "    def _generate_hf_character(self, user_request: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate character using HuggingFace.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Generate a game character based on this request: {user_request}\n",
    "        \n",
    "        Respond with valid JSON that matches this schema:\n",
    "        {json.dumps(game_character_schema, indent=2)}\n",
    "        \n",
    "        Output only the JSON, no additional text.\n",
    "        \"\"\"\n",
    "        \n",
    "        response_text = self.client.text_generation(prompt, max_new_tokens=500)\n",
    "        return safe_json_parse(response_text)\n",
    "    \n",
    "    def _generate_ollama_character(self, user_request: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate character using Ollama.\"\"\"\n",
    "        payload = {\n",
    "            \"model\": \"llama2\",\n",
    "            \"prompt\": f\"Generate a game character based on this request: {user_request}\",\n",
    "            \"format\": \"json\",\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\"http://localhost:11434/api/generate\", json=payload)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return safe_json_parse(result.get('response', '{}'))\n",
    "        else:\n",
    "            raise Exception(f\"Ollama API error: {response.status_code}\")\n",
    "\n",
    "# Test the character generator\n",
    "print(\"Testing Game Character Generator...\")\n",
    "\n",
    "# Test with different providers\n",
    "providers_to_test = []\n",
    "if OPENAI_API_KEY != \"your-openai-key-here\":\n",
    "    providers_to_test.append(\"openai\")\n",
    "if HF_TOKEN != \"your-huggingface-token-here\":\n",
    "    providers_to_test.append(\"huggingface\")\n",
    "if ollama_available:\n",
    "    providers_to_test.append(\"ollama\")\n",
    "\n",
    "for provider in providers_to_test:\n",
    "    try:\n",
    "        print(f\"\\n--- Testing {provider.upper()} ---\")\n",
    "        generator = GameCharacterGenerator(provider=provider, api_key=OPENAI_API_KEY if provider == \"openai\" else HF_TOKEN)\n",
    "        result = generator.generate_character(\"Create a brave warrior with high strength and good health.\")\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"✅ Character generated successfully in {result['attempts']} attempt(s)\")\n",
    "            character = result['character']['character']\n",
    "            print(f\"Name: {character['name']}\")\n",
    "            print(f\"Class: {character['class']}\")\n",
    "            print(f\"Health: {character['health']}\")\n",
    "            print(f\"Strength: {character['strength']}\")\n",
    "        else:\n",
    "            print(f\"❌ Failed to generate character: {result['error']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing {provider}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed the Function Calling and Structured Outputs lab. Here's what you've learned:\n",
    "\n",
    "### Key Skills Acquired:\n",
    "- ✅ Function calling implementation with JSON Schema\n",
    "- ✅ Cross-platform AI provider integration (OpenAI, HuggingFace, Ollama)\n",
    "- ✅ Structured output validation and error handling\n",
    "- ✅ Provider performance comparison and evaluation\n",
    "- ✅ Building robust AI applications with retry logic\n",
    "\n",
    "### Best Practices:\n",
    "- Always validate AI outputs against schemas\n",
    "- Implement proper error handling and retry mechanisms\n",
    "- Use environment variables for API keys\n",
    "- Test across multiple providers for reliability\n",
    "- Handle JSON parsing edge cases gracefully\n",
    "\n",
    "### Next Steps:\n",
    "1. Experiment with different schemas for your use cases\n",
    "2. Try integrating with other AI providers\n",
    "3. Build more complex function calling scenarios\n",
    "4. Implement caching for better performance\n",
    "5. Add monitoring and logging for production use\n",
    "\n",
    "### Additional Resources:\n",
    "- [JSON Schema Documentation](https://json-schema.org/)\n",
    "- [OpenAI Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)\n",
    "- [HuggingFace Inference API](https://huggingface.co/docs/api-inference/index)\n",
    "- [Ollama Documentation](https://ollama.ai/)\n",
    "\n",
    "Remember: The key to reliable AI applications is robust validation, proper error handling, and thorough testing across different scenarios and providers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
