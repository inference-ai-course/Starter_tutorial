{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Prompt Engineering and Evaluation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Implement effective prompt engineering techniques\n",
    "- Build guardrails against AI hallucinations\n",
    "- Create evaluation frameworks for AI outputs\n",
    "- Optimize model parameters for different use cases\n",
    "- Develop production-ready prompt templates\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai huggingface_hub transformers numpy scikit-learn python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Let's start by setting up our environment and API credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API Keys (use environment variables in production)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"your-openai-key-here\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"your-huggingface-token-here\")\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"OpenAI API Key configured: {'Yes' if OPENAI_API_KEY != 'your-openai-key-here' else 'No'}\")\n",
    "print(f\"HuggingFace Token configured: {'Yes' if HF_TOKEN != 'your-huggingface-token-here' else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: System Prompts and Role Design\n",
    "\n",
    "Create effective system prompts that define model behavior and constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SystemPromptDesigner:\n",
    "    \"\"\"Design effective system prompts for different use cases.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.prompt_templates = {\n",
    "            'technical_writer': self.create_technical_writer_prompt(),\n",
    "            'customer_support': self.create_customer_support_prompt(),\n",
    "            'code_reviewer': self.create_code_reviewer_prompt(),\n",
    "            'data_analyst': self.create_data_analyst_prompt()\n",
    "        }\n",
    "    \n",
    "    def create_technical_writer_prompt(self) -> str:\n",
    "        \"\"\"Create prompt for technical documentation.\"\"\"\n",
    "        return \"\"\"\n",
    "You are a senior technical writer with 10 years of experience in software documentation.\n",
    "You specialize in creating clear, concise, and accurate technical documentation for developers.\n",
    "\n",
    "Your characteristics:\n",
    "- Write in a professional but approachable tone\n",
    "- Use active voice and present tense\n",
    "- Include practical examples and code snippets\n",
    "- Structure information logically with clear headings\n",
    "\n",
    "Your constraints:\n",
    "- Avoid jargon unless necessary (define when used)\n",
    "- Keep sentences under 25 words when possible\n",
    "- Provide step-by-step instructions for complex tasks\n",
    "- Include troubleshooting sections for common issues\n",
    "\n",
    "Your output format:\n",
    "- Use Markdown formatting\n",
    "- Include a brief overview section\n",
    "- Structure content with hierarchical headings\n",
    "- End with a summary or next steps section\n",
    "\"\"\"\n",
    "    \n",
    "    def create_customer_support_prompt(self) -> str:\n",
    "        \"\"\"Create prompt for customer support.\"\"\"\n",
    "        return \"\"\"\n",
    "You are a helpful customer support assistant for TechCorp.\n",
    "You have access to order information, product details, and troubleshooting guides.\n",
    "\n",
    "Guidelines:\n",
    "- Be polite and professional\n",
    "- Ask clarifying questions when needed\n",
    "- Provide step-by-step solutions\n",
    "- Escalate to human agent for complex issues\n",
    "- Never make promises about refunds or compensation\n",
    "\"\"\"\n",
    "    \n",
    "    def create_code_reviewer_prompt(self) -> str:\n",
    "        \"\"\"Create prompt for code review.\"\"\"\n",
    "        return \"\"\"\n",
    "You are an experienced software engineer specializing in code review and best practices.\n",
    "You have expertise in Python, JavaScript, and software architecture.\n",
    "\n",
    "Review criteria:\n",
    "- Check for code readability and maintainability\n",
    "- Identify potential bugs or security issues\n",
    "- Suggest performance improvements\n",
    "- Ensure proper error handling\n",
    "- Verify adherence to coding standards\n",
    "\n",
    "Provide constructive feedback with specific examples and suggestions.\n",
    "\"\"\"\n",
    "    \n",
    "    def create_data_analyst_prompt(self) -> str:\n",
    "        \"\"\"Create prompt for data analysis.\"\"\"\n",
    "        return \"\"\"\n",
    "You are a senior data analyst with expertise in statistical analysis and data visualization.\n",
    "You specialize in extracting insights from complex datasets.\n",
    "\n",
    "Analysis approach:\n",
    "- Start with data quality assessment\n",
    "- Identify key patterns and trends\n",
    "- Provide statistical significance where applicable\n",
    "- Suggest actionable recommendations\n",
    "- Include appropriate visualizations\n",
    "\n",
    "Always explain your methodology and assumptions clearly.\n",
    "\"\"\"\n",
    "\n",
    "# Test system prompt effectiveness\n",
    "print(\"Testing System Prompt Designer...\")\n",
    "designer = SystemPromptDesigner()\n",
    "\n",
    "for role, prompt in designer.prompt_templates.items():\n",
    "    print(f\"\\n--- {role.replace('_', ' ').title()} Prompt ---\")\n",
    "    print(f\"Length: {len(prompt)} characters\")\n",
    "    print(f\"First 200 chars: {prompt[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: The CLEAR Framework Implementation\n",
    "\n",
    "Implement the CLEAR framework for creating effective prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClearFramework:\n",
    "    \"\"\"Implement the CLEAR framework for prompt engineering.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_prompt(task: str, context: str, length: str, examples: str, \n",
    "                     audience: str, requirements: str) -> str:\n",
    "        \"\"\"Create a prompt using the CLEAR framework.\"\"\"\n",
    "        return f\"\"\"\n",
    "Context: {context}\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Requirements:\n",
    "- Length: {length}\n",
    "- Audience: {audience}\n",
    "- {requirements}\n",
    "\n",
    "Examples:\n",
    "{examples}\n",
    "\n",
    "Remember to follow all requirements and match the example format.\n",
    "\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_product_description_prompt(product_name: str, features: List[str], \n",
    "                                        target_audience: str) -> str:\n",
    "        \"\"\"Create a product description prompt using CLEAR framework.\"\"\"\n",
    "        context = f\"You are writing product descriptions for an e-commerce website.\"\n",
    "        task = f\"Write a compelling product description for {product_name}\"\n",
    "        length = \"100-150 words\"\n",
    "        audience = target_audience\n",
    "        requirements = f\"Highlight these features: {', '.join(features)}. Use persuasive language.\"\n",
    "        \n",
    "        examples = \"\"\"\n",
    "Input: Wireless Bluetooth Headphones\n",
    "Output: Experience premium sound quality with our wireless Bluetooth headphones. \n",
    "Featuring active noise cancellation, 30-hour battery life, and comfortable over-ear design. \n",
    "Perfect for music lovers and professionals who demand exceptional audio performance.\n",
    "\n",
    "Input: Stainless Steel Water Bottle\n",
    "Output: Stay hydrated with our durable stainless steel water bottle. Double-wall vacuum \n",
    "insulation keeps drinks cold for 24 hours or hot for 12 hours. Leak-proof lid and sweat-free \n",
    "design make it perfect for gym, office, or outdoor adventures.\n",
    "\"\"\"\n",
    "        \n",
    "        return ClearFramework.create_prompt(task, context, length, examples, audience, requirements)\n",
    "\n",
    "# Test CLEAR framework\n",
    "print(\"Testing CLEAR Framework...\")\n",
    "\n",
    "# Create product description prompt\n",
    "product_prompt = ClearFramework.create_product_description_prompt(\n",
    "    product_name=\"Organic Cotton T-Shirt\",\n",
    "    features=[\"100% organic cotton\", \"breathable fabric\", \"sustainable production\", \"comfortable fit\"],\n",
    "    target_audience=\"environmentally conscious consumers aged 25-45\"\n",
    ")\n",
    "\n",
    "print(\"Generated Product Description Prompt:\")\n",
    "print(product_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Parameter Tuning and Evaluation\n",
    "\n",
    "Implement parameter optimization and evaluation frameworks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterOptimizer:\n",
    "    \"\"\"Optimize model parameters for different use cases.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parameter_profiles = {\n",
    "            'accuracy_focused': {'temperature': 0.1, 'top_p': 0.3, 'description': 'High accuracy, low creativity'},\n",
    "            'balanced': {'temperature': 0.5, 'top_p': 0.7, 'description': 'Balanced creativity and accuracy'},\n",
    "            'creative': {'temperature': 0.8, 'top_p': 0.9, 'description': 'High creativity, varied outputs'},\n",
    "            'deterministic': {'temperature': 0.0, 'top_p': 0.1, 'description': 'Maximum consistency'}\n",
    "        }\n",
    "    \n",
    "    def get_optimal_parameters(self, use_case: str) -> Dict[str, float]:\n",
    "        \"\"\"Get optimal parameters for specific use case.\"\"\"\n",
    "        return self.parameter_profiles.get(use_case, self.parameter_profiles['balanced'])\n",
    "    \n",
    "    def test_parameter_sensitivity(self, client, test_prompts: List[str], \n",
    "                                 param_grid: List[Tuple[float, float]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Test multiple parameter combinations.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for temp, top_p in param_grid:\n",
    "            combination_results = {\n",
    "                'temperature': temp,\n",
    "                'top_p': top_p,\n",
    "                'outputs': [],\n",
    "                'metrics': {}\n",
    "            }\n",
    "            \n",
    "            for prompt in test_prompts:\n",
    "                try:\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=\"gpt-3.5-turbo\",\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        temperature=temp,\n",
    "                        top_p=top_p,\n",
    "                        max_tokens=150\n",
    "                    )\n",
    "                    \n",
    "                    combination_results['outputs'].append({\n",
    "                        'prompt': prompt,\n",
    "                        'response': response.choices[0].message.content\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with temp={temp}, top_p={top_p}: {e}\")\n",
    "            \n",
    "            # Calculate metrics for this combination\n",
    "            combination_results['metrics'] = self.calculate_metrics(combination_results['outputs'])\n",
    "            results.append(combination_results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_metrics(self, outputs: List[Dict[str, Any]]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate stability and quality metrics.\"\"\"\n",
    "        responses = [output['response'] for output in outputs]\n",
    "        \n",
    "        return {\n",
    "            'avg_length': np.mean([len(r) for r in responses]),\n",
    "            'length_variance': np.var([len(r) for r in responses]),\n",
    "            'lexical_diversity': self.calculate_lexical_diversity(responses),\n",
    "            'consistency_score': self.calculate_consistency(responses)\n",
    "        }\n",
    "    \n",
    "    def calculate_lexical_diversity(self, responses: List[str]) -> float:\n",
    "        \"\"\"Calculate lexical diversity (unique words / total words).\"\"\"\n",
    "        all_words = []\n",
    "        for response in responses:\n",
    "            words = re.findall(r'\\b\\w+\\b', response.lower())\n",
    "            all_words.extend(words)\n",
    "        \n",
    "        if not all_words:\n",
    "            return 0.0\n",
    "        \n",
    "        unique_words = len(set(all_words))\n",
    "        total_words = len(all_words)\n",
    "        return unique_words / total_words\n",
    "    \n",
    "    def calculate_consistency(self, responses: List[str]) -> float:\n",
    "        \"\"\"Calculate consistency across responses.\"\"\"\n",
    "        if len(responses) < 2:\n",
    "            return 1.0\n",
    "        \n",
    "        # Use TF-IDF to measure similarity\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform(responses)\n",
    "            similarities = cosine_similarity(tfidf_matrix)\n",
    "            \n",
    "            # Calculate average similarity (excluding diagonal)\n",
    "            n = len(responses)\n",
    "            total_similarity = 0\n",
    "            count = 0\n",
    "            for i in range(n):\n",
    "                for j in range(i+1, n):\n",
    "                    total_similarity += similarities[i][j]\n",
    "                    count += 1\n",
    "            \n",
    "            return total_similarity / count if count > 0 else 0.0\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "# Test parameter optimization\n",
    "print(\"Testing Parameter Optimization...\")\n",
    "\n",
    "optimizer = ParameterOptimizer()\n",
    "\n",
    "# Test different parameter profiles\n",
    "for profile_name, params in optimizer.parameter_profiles.items():\n",
    "    print(f\"\\n--- {profile_name.replace('_', ' ').title()} ---\")\n",
    "    print(f\"Temperature: {params['temperature']}\")\n",
    "    print(f\"Top_p: {params['top_p']}\")\n",
    "    print(f\"Description: {params['description']}\")\n",
    "\n",
    "# Test parameter sensitivity with sample prompts\n",
    "test_prompts = [\n",
    "    \"Write a creative story about artificial intelligence.\",\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"What are the benefits of renewable energy?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: LLM-as-Judge Evaluation Framework\n",
    "\n",
    "Implement automated evaluation using LLMs as judges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMJudge:\n",
    "    \"\"\"Implement LLM-as-a-judge evaluation system.\"\"\"\n",
    "    \n",
    "    def __init__(self, judge_model_client):\n",
    "        self.judge_model = judge_model_client\n",
    "    \n",
    "    def evaluate_response(self, response: str, criteria: List[str], reference: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate a response using LLM-as-judge.\"\"\"\n",
    "        evaluation_prompt = f\"\"\"\n",
    "You are an expert evaluator. Assess the following response based on these criteria:\n",
    "\n",
    "Criteria: {', '.join(criteria)}\n",
    "\n",
    "{f'Reference answer: {reference}' if reference else ''}\n",
    "\n",
    "Response to evaluate: {response}\n",
    "\n",
    "Provide scores (1-10) for each criterion and brief justifications:\n",
    "- Relevance: [score] - [justification]\n",
    "- Accuracy: [score] - [justification] \n",
    "- Clarity: [score] - [justification]\n",
    "- Completeness: [score] - [justification]\n",
    "- Style: [score] - [justification]\n",
    "\n",
    "Overall score: [average]/10\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            evaluation = self.judge_model.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": evaluation_prompt}]\n",
    "            )\n",
    "            \n",
    "            return self.parse_evaluation(evaluation.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            return {'error': str(e), 'overall_score': 0}\n",
    "    \n",
    "    def parse_evaluation(self, evaluation_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse evaluation results from judge response.\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        # Extract individual scores\n",
    "        score_patterns = {\n",
    "            'relevance': r'Relevance:\\s*(\\d+)',\n",
    "            'accuracy': r'Accuracy:\\s*(\\d+)',\n",
    "            'clarity': r'Clarity:\\s*(\\d+)',\n",
    "            'completeness': r'Completeness:\\s*(\\d+)',\n",
    "            'style': r'Style:\\s*(\\d+)'\n",
    "        }\n",
    "        \n",
    "        for criterion, pattern in score_patterns.items():\n",
    "            match = re.search(pattern, evaluation_text)\n",
    "            if match:\n",
    "                scores[criterion] = int(match.group(1))\n",
    "        \n",
    "        # Extract overall score\n",
    "        overall_match = re.search(r'Overall score:\\s*(\\d+(?:\\.\\d+)?)', evaluation_text)\n",
    "        overall_score = float(overall_match.group(1)) if overall_match else 0\n",
    "        \n",
    "        return {\n",
    "            'individual_scores': scores,\n",
    "            'overall_score': overall_score,\n",
    "            'full_evaluation': evaluation_text\n",
    "        }\n",
    "\n",
    "class ProductionPromptTemplate:\n",
    "    \"\"\"Production-ready prompt template management.\"\"\"\n",
    "    \n",
    "    def __init__(self, template_dir: str = \"templates\"):\n",
    "        self.template_dir = template_dir\n",
    "        self.templates = {}\n",
    "        self.version_history = {}\n",
    "    \n",
    "    def create_template(self, name: str, template: str, parameters: List[Dict[str, Any]], \n",
    "                       metadata: Dict[str, Any] = None) -> str:\n",
    "        \"\"\"Create a new prompt template.\"\"\"\n",
    "        template_data = {\n",
    "            'name': name,\n",
    "            'version': '1.0.0',\n",
    "            'template': template,\n",
    "            'parameters': parameters,\n",
    "            'metadata': metadata or {},\n",
    "            'created_at': time.time(),\n",
    "            'usage_count': 0\n",
    "        }\n",
    "        \n",
    "        self.templates[name] = template_data\n",
    "        self.version_history[f\"{name}_v1.0.0\"] = template_data.copy()\n",
    "        \n",
    "        return f\"{name}_v1.0.0\"\n",
    "    \n",
    "    def render_template(self, name: str, **kwargs) -> str:\n",
    "        \"\"\"Render template with provided parameters.\"\"\"\n",
    "        if name not in self.templates:\n",
    "            raise ValueError(f\"Template '{name}' not found\")\n",
    "        \n",
    "        template_data = self.templates[name]\n",
    "        template_str = template_data['template']\n",
    "        \n",
    "        # Validate required parameters\n",
    "        required_params = [p['name'] for p in template_data['parameters'] if p.get('required', True)]\n",
    "        missing_params = [p for p in required_params if p not in kwargs]\n",
    "        \n",
    "        if missing_params:\n",
    "            raise ValueError(f\"Missing required parameters: {missing_params}\")\n",
    "        \n",
    "        # Render template\n",
    "        try:\n",
    "            rendered = template_str.format(**kwargs)\n",
    "            self.templates[name]['usage_count'] += 1\n",
    "            return rendered\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Missing template parameter: {e}\")\n",
    "\n",
    "# Test the evaluation framework\n",
    "print(\"Testing LLM-as-Judge Evaluation Framework...\")\n",
    "\n",
    "# Create sample responses for evaluation\n",
    "sample_responses = [\n",
    "    \"The capital of France is Paris. It is known for the Eiffel Tower and Louvre Museum.\",\n",
    "    \"Paris is the capital city of France, located in Western Europe. It has a population of over 2 million people.\",\n",
    "    \"France's capital is Paris, which is famous for landmarks like the Eiffel Tower.\"\n",
    "]\n",
    "\n",
    "print(\"Sample responses created for evaluation\")\n",
    "\n",
    "# Test production template system\n",
    "print(\"\\nTesting Production Template System...\")\n",
    "\n",
    "template_manager = ProductionPromptTemplate()\n",
    "\n",
    "# Create a customer support template\n",
    "support_template = \"\"\"\n",
    "You are a customer support agent for {company_name}.\n",
    "You specialize in helping customers with {product_type} products.\n",
    "\n",
    "Guidelines:\n",
    "- Be polite and professional\n",
    "- Ask clarifying questions when needed\n",
    "- Provide step-by-step solutions\n",
    "- Escalate complex issues when necessary\n",
    "\n",
    "Customer Issue: {customer_issue}\n",
    "\"\"\"\n",
    "\n",
    "template_params = [\n",
    "    {'name': 'company_name', 'type': 'string', 'required': True, 'description': 'Company name'},\n",
    "    {'name': 'product_type', 'type': 'string', 'required': True, 'description': 'Type of product'},\n",
    "    {'name': 'customer_issue', 'type': 'string', 'required': True, 'description': 'Customer issue description'}\n",
    "]\n",
    "\n",
    "template_version = template_manager.create_template(\n",
    "    name=\"customer_support\",\n",
    "    template=support_template,\n",
    "    parameters=template_params,\n",
    "    metadata={'description': 'Customer support response template', 'category': 'support'}\n",
    ")\n",
    "\n",
    "print(f\"Created template: {template_version}\")\n",
    "\n",
    "# Test rendering the template\n",
    "rendered_prompt = template_manager.render_template(\n",
    "    name=\"customer_support\",\n",
    "    company_name=\"TechCorp\",\n",
    "    product_type=\"software\",\n",
    "    customer_issue=\"I can't log into my account\"\n",
    ")\n",
    "\n",
    "print(\"\\nRendered Template:\")\n",
    "print(rendered_prompt)\n",
    "\n",
    "# Test with missing parameter (should raise error)\n",
    "try:\n",
    "    template_manager.render_template(\n",
    "        name=\"customer_support\",\n",
    "        company_name=\"TechCorp\",\n",
    "        product_type=\"software\"\n",
    "        # Missing customer_issue parameter\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"\\nExpected error for missing parameter: {e}\")\n",
    "\n",
    "print(\"\\n✅ Prompt Engineering Lab Completed!\")\n",
    "print(\"\\nKey Skills Learned:\")\n",
    "print(\"- System prompt design for different roles\")\n",
    "print(\"- CLEAR framework implementation\")\n",
    "print(\"- Parameter optimization and tuning\")\n",
    "print(\"- LLM-as-judge evaluation framework\")\n",
    "print(\"- Production-ready template management\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Experiment with different parameter combinations\")\n",
    "print(\"2. Create custom evaluation criteria for your use case\")\n",
    "print(\"3. Build a comprehensive prompt template library\")\n",
    "print(\"4. Implement A/B testing for prompt variations\")\n",
    "print(\"5. Add monitoring and analytics for production use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed the Prompt Engineering and Evaluation lab. Here's what you've learned:\n",
    "\n",
    "### Key Skills Acquired:\n",
    "- ✅ System prompt design for different professional roles\n",
    "- ✅ CLEAR framework implementation for structured prompts\n",
    "- ✅ Parameter optimization (temperature, top_p) for different use cases\n",
    "- ✅ LLM-as-judge evaluation framework for automated assessment\n",
    "- ✅ Production-ready prompt template management with versioning\n",
    "\n",
    "### Best Practices:\n",
    "- Always define clear roles and constraints in system prompts\n",
    "- Use the CLEAR framework for complex prompt requirements\n",
    "- Test parameter sensitivity for your specific use case\n",
    "- Implement automated evaluation for quality assurance\n",
    "- Version control your prompt templates for production use\n",
    "\n",
    "### Next Steps:\n",
    "1. Experiment with different parameter combinations for your specific use cases\n",
    "2. Create custom evaluation criteria tailored to your domain\n",
    "3. Build a comprehensive library of prompt templates\n",
    "4. Implement A/B testing frameworks for prompt optimization\n",
    "5. Add monitoring and analytics for production prompt performance\n",
    "\n",
    "Remember: Effective prompt engineering is an iterative process. Continuously test, evaluate, and refine your prompts based on real-world performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
