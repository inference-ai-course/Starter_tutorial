{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Model Interfaces and Deployment\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Set up and configure local inference engines (Ollama, vLLM)\n",
    "- Implement OpenAI-compatible interfaces across different providers\n",
    "- Build production-ready deployment architectures\n",
    "- Compare performance metrics across deployment options\n",
    "- Implement security, monitoring, and scaling solutions\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai huggingface_hub vllm requests python-dotenv psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Let's start by setting up our environment and API credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import psutil\n",
    "from typing import Dict, Any, List, Optional\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API Keys (use environment variables in production)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"your-openai-key-here\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"your-huggingface-token-here\")\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"OpenAI API Key configured: {'Yes' if OPENAI_API_KEY != 'your-openai-key-here' else 'No'}\")\n",
    "print(f\"HuggingFace Token configured: {'Yes' if HF_TOKEN != 'your-huggingface-token-here' else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Universal OpenAI-Compatible Client\n",
    "\n",
    "Create a universal client that works with multiple AI providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class UniversalAIClient:\n",
    "    \"\"\"Universal client for multiple AI providers with OpenAI-compatible APIs.\"\"\"\n",
    "    \n",
    "    def __init__(self, provider: str, api_key: str, base_url: str = None):\n",
    "        self.provider = provider\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.client = self._initialize_client()\n",
    "    \n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initialize the appropriate client based on provider.\"\"\"\n",
    "        if self.provider == \"openai\":\n",
    "            return OpenAI(api_key=self.api_key)\n",
    "        elif self.provider in [\"huggingface\", \"hf\"]:\n",
    "            return OpenAI(\n",
    "                api_key=self.api_key,\n",
    "                base_url=self.base_url or \"https://api-inference.huggingface.co/v1\"\n",
    "            )\n",
    "        elif self.provider == \"ollama\":\n",
    "            return OpenAI(\n",
    "                api_key=\"ollama\",  # Ollama doesn't require auth\n",
    "                base_url=self.base_url or \"http://localhost:11434/v1\"\n",
    "            )\n",
    "        elif self.provider == \"vllm\":\n",
    "            return OpenAI(\n",
    "                api_key=\"vllm\",  # vLLM doesn't require auth\n",
    "                base_url=self.base_url or \"http://localhost:8000/v1\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {self.provider}\")\n",
    "    \n",
    "    def chat_completion(self, messages: List[Dict[str, str]], model: str = None, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Create chat completion with unified interface.\"\"\"\n",
    "        # Use appropriate model for each provider\n",
    "        if not model:\n",
    "            model = self._get_default_model()\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=kwargs.get('temperature', 0.7),\n",
    "                max_tokens=kwargs.get('max_tokens', 150),\n",
    "                top_p=kwargs.get('top_p', 1.0),\n",
    "                frequency_penalty=kwargs.get('frequency_penalty', 0),\n",
    "                presence_penalty=kwargs.get('presence_penalty', 0),\n",
    "                stream=kwargs.get('stream', False)\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'provider': self.provider,\n",
    "                'model': model,\n",
    "                'response': response.choices[0].message.content,\n",
    "                'usage': {\n",
    "                    'prompt_tokens': getattr(response.usage, 'prompt_tokens', 0),\n",
    "                    'completion_tokens': getattr(response.usage, 'completion_tokens', 0),\n",
    "                    'total_tokens': getattr(response.usage, 'total_tokens', 0)\n",
    "                },\n",
    "                'latency': kwargs.get('start_time', time.time()) - time.time()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'provider': self.provider,\n",
    "                'error': str(e),\n",
    "                'model': model\n",
    "            }\n",
    "    \n",
    "    def _get_default_model(self) -> str:\n",
    "        \"\"\"Get default model for each provider.\"\"\"\n",
    "        defaults = {\n",
    "            'openai': 'gpt-3.5-turbo',\n",
    "            'huggingface': 'microsoft/DialoGPT-medium',\n",
    "            'hf': 'microsoft/DialoGPT-medium',\n",
    "            'ollama': 'llama2',\n",
    "            'vllm': 'microsoft/DialoGPT-medium'\n",
    "        }\n",
    "        return defaults.get(self.provider, 'gpt-3.5-turbo')\n",
    "\n",
    "# Test the universal client\n",
    "print(\"Testing Universal AI Client...\")\n",
    "\n",
    "# Test with available providers\n",
    "providers_to_test = []\n",
    "if OPENAI_API_KEY != \"your-openai-key-here\":\n",
    "    providers_to_test.append((\"openai\", OPENAI_API_KEY))\n",
    "if HF_TOKEN != \"your-huggingface-token-here\":\n",
    "    providers_to_test.append((\"huggingface\", HF_TOKEN))\n",
    "\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "\n",
    "for provider, api_key in providers_to_test:\n",
    "    try:\n",
    "        print(f\"\\n--- Testing {provider.upper()} ---\")\n",
    "        client = UniversalAIClient(provider, api_key)\n",
    "        result = client.chat_completion(test_messages)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"✅ Response: {result['response'][:100]}...\")\n",
    "            print(f\"Tokens used: {result['usage']['total_tokens']}\")\n",
    "        else:\n",
    "            print(f\"❌ Error: {result['error']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing {provider}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: HuggingFace Inference Provider Management\n",
    "\n",
    "Implement failover and provider selection with HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceProviderManager:\n",
    "    \"\"\"Manages HuggingFace inference providers with failover support.\"\"\"\n",
    "    \n",
    "    def __init__(self, token: str, preferred_providers: List[str] = None):\n",
    "        self.token = token\n",
    "        self.preferred_providers = preferred_providers or [\"auto\"]\n",
    "        self.client = self._initialize_client()\n",
    "        self.provider_performance = {}\n",
    "    \n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initialize HuggingFace Inference Client.\"\"\"\n",
    "        try:\n",
    "            from huggingface_hub import InferenceClient\n",
    "            return InferenceClient(token=self.token)\n",
    "        except ImportError:\n",
    "            print(\"HuggingFace Hub not installed. Install with: pip install huggingface_hub\")\n",
    "            return None\n",
    "    \n",
    "    def chat_completion_with_failover(self, messages: List[Dict[str, str]], \n",
    "                                    model: str, max_retries: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Attempt chat completion with provider failover.\"\"\"\n",
    "        if not self.client:\n",
    "            return {'success': False, 'error': 'HuggingFace client not available'}\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Try with specified providers\n",
    "                for provider in self.preferred_providers:\n",
    "                    try:\n",
    "                        start_time = time.time()\n",
    "                        response = self.client.chat_completion(\n",
    "                            messages=messages,\n",
    "                            model=model,\n",
    "                            provider=provider,\n",
    "                            max_tokens=150,\n",
    "                            temperature=0.7\n",
    "                        )\n",
    "                        \n",
    "                        latency = time.time() - start_time\n",
    "                        self._record_provider_performance(provider, latency, True)\n",
    "                        \n",
    "                        return {\n",
    "                            'success': True,\n",
    "                            'provider': provider,\n",
    "                            'response': response.choices[0].message.content,\n",
    "                            'latency': latency,\n",
    "                            'attempt': attempt + 1\n",
    "                        }\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Provider {provider} failed: {e}\")\n",
    "                        self._record_provider_performance(provider, 0, False)\n",
    "                        continue\n",
    "                \n",
    "                # Fallback to auto selection\n",
    "                start_time = time.time()\n",
    "                response = self.client.chat_completion(\n",
    "                    messages=messages,\n",
    "                    model=model,\n",
    "                    provider=\"auto\",\n",
    "                    max_tokens=150,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                \n",
    "                latency = time.time() - start_time\n",
    "                self._record_provider_performance(\"auto\", latency, True)\n",
    "                \n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'provider': 'auto',\n",
    "                    'response': response.choices[0].message.content,\n",
    "                    'latency': latency,\n",
    "                    'attempt': attempt + 1\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    return {\n",
    "                        'success': False,\n",
    "                        'error': str(e),\n",
    "                        'attempts': max_retries\n",
    "                    }\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        \n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'All providers failed after maximum retries',\n",
    "            'attempts': max_retries\n",
    "        }\n",
    "    \n",
    "    def _record_provider_performance(self, provider: str, latency: float, success: bool):\n",
    "        \"\"\"Record provider performance metrics.\"\"\"\n",
    "        if provider not in self.provider_performance:\n",
    "            self.provider_performance[provider] = {\n",
    "                'total_requests': 0,\n",
    "                'successful_requests': 0,\n",
    "                'total_latency': 0,\n",
    "                'latencies': []\n",
    "            }\n",
    "        \n",
    "        self.provider_performance[provider]['total_requests'] += 1\n",
    "        if success:\n",
    "            self.provider_performance[provider]['successful_requests'] += 1\n",
    "            self.provider_performance[provider]['total_latency'] += latency\n",
    "            self.provider_performance[provider]['latencies'].append(latency)\n",
    "    \n",
    "    def get_provider_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get performance statistics for all providers.\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        for provider, data in self.provider_performance.items():\n",
    "            if data['total_requests'] > 0:\n",
    "                success_rate = (data['successful_requests'] / data['total_requests']) * 100\n",
    "                avg_latency = data['total_latency'] / data['successful_requests'] if data['successful_requests'] > 0 else 0\n",
    "                \n",
    "                stats[provider] = {\n",
    "                    'total_requests': data['total_requests'],\n",
    "                    'successful_requests': data['successful_requests'],\n",
    "                    'success_rate': success_rate,\n",
    "                    'average_latency': avg_latency,\n",
    "                    'p50_latency': np.median(data['latencies']) if data['latencies'] else 0,\n",
    "                    'p95_latency': np.percentile(data['latencies'], 95) if len(data['latencies']) > 1 else 0\n",
    "                }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Test HuggingFace provider management\n",
    "if HF_TOKEN != \"your-huggingface-token-here\":\n",
    "    print(\"\\n--- Testing HuggingFace Provider Management ---\")\n",
    "    \n",
    "    hf_manager = HuggingFaceProviderManager(\n",
    "        token=HF_TOKEN,\n",
    "        preferred_providers=[\"hf-inference\", \"auto\"]\n",
    "    )\n",
    "    \n",
    "    test_messages = [\n",
    "        {\"role\": \"user\", \"content\": \"What are the benefits of renewable energy?\"}\n",
    "    ]\n",
    "    \n",
    "    # Test multiple requests to gather statistics\n",
    "    for i in range(3):\n",
    "        result = hf_manager.chat_completion_with_failover(\n",
    "            test_messages,\n",
    "            model=\"microsoft/DialoGPT-medium\"\n",
    "        )\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"✅ Request {i+1}: Provider={result['provider']}, Latency={result['latency']:.2f}s\")\n",
    "        else:\n",
    "            print(f\"❌ Request {i+1}: {result['error']}\")\n",
    "    \n",
    "    # Show provider statistics\n",
    "    stats = hf_manager.get_provider_statistics()\n",
    "    print(\"\\nProvider Statistics:\")\n",
    "    for provider, data in stats.items():\n",
    "        print(f\"{provider}: {data['success_rate']:.1f}% success, {data['average_latency']:.2f}s avg latency\")\n",
    "else:\n",
    "    print(\"HuggingFace token not configured. Skipping provider management test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Local Ollama Integration\n",
    "\n",
    "Test integration with local Ollama instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ollama_integration():\n",
    "    \"\"\"Test integration with local Ollama instance.\"\"\"\n",
    "    try:\n",
    "        # Check if Ollama is running\n",
    "        ollama_response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "        if ollama_response.status_code == 200:\n",
    "            print(\"Ollama is running!\")\n",
    "            models = ollama_response.json().get('models', [])\n",
    "            print(f\"Available models: {[model['name'] for model in models]}\")\n",
    "            \n",
    "            # Test with a simple prompt\n",
    "            test_payload = {\n",
    "                \"model\": \"llama2\" if any('llama2' in m['name'] for m in models) else models[0]['name'],\n",
    "                \"prompt\": \"Generate a simple JSON object with 'name' and 'age' fields.\",\n",
    "                \"format\": \"json\",\n",
    "                \"stream\": False\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                \"http://localhost:11434/api/generate\",\n",
    "                json=test_payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                print(f\"Ollama response: {result.get('response', 'No response')}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Ollama API error: {response.status_code}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"Ollama is not responding properly\")\n",
    "            return False\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Ollama is not running. Start it with: ollama serve\")\n",
    "        return False\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Ollama connection timed out\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Ollama: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test Ollama integration\n",
    "print(\"Testing Ollama integration...\")\n",
    "ollama_available = test_ollama_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Performance Benchmarking Framework\n",
    "\n",
    "Create a comprehensive benchmarking framework for different inference engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceBenchmark:\n",
    "    \"\"\"Comprehensive benchmarking framework for AI inference engines.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        self.test_prompts = self.load_test_prompts()\n",
    "    \n",
    "    def load_test_prompts(self):\n",
    "        \"\"\"Load diverse test prompts for benchmarking.\"\"\"\n",
    "        return [\n",
    "            \"Write a short story about artificial intelligence.\",\n",
    "            \"Explain quantum computing in simple terms.\",\n",
    "            \"What are the benefits of renewable energy?\",\n",
    "            \"How does machine learning work?\",\n",
    "            \"Describe the process of photosynthesis.\",\n",
    "            \"What are the key principles of software engineering?\",\n",
    "            \"Explain the difference between SQL and NoSQL databases.\",\n",
    "            \"How do neural networks learn?\",\n",
    "            \"What is the importance of data structures?\",\n",
    "            \"Describe a sustainable city of the future.\"\n",
    "        ]\n",
    "    \n",
    "    def benchmark_openai(self):\n",
    "        \"\"\"Benchmark OpenAI API.\"\"\"\n",
    "        try:\n",
    "            from openai import OpenAI\n",
    "            client = OpenAI()\n",
    "            \n",
    "            results = {\n",
    "                'latencies': [],\n",
    "                'token_counts': [],\n",
    "                'throughput': [],\n",
    "                'cost_estimates': []\n",
    "            }\n",
    "            \n",
    "            for prompt in self.test_prompts:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=150\n",
    "                )\n",
    "                \n",
    "                latency = time.time() - start_time\n",
    "                token_count = response.usage.total_tokens\n",
    "                \n",
    "                results['latencies'].append(latency)\n",
    "                results['token_counts'].append(token_count)\n",
    "                results['throughput'].append(token_count / latency)\n",
    "                results['cost_estimates'].append(token_count * 0.002 / 1000)  # Approximate cost\n",
    "            \n",
    "            return self.summarize_results(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def benchmark_huggingface(self):\n",
    "        \"\"\"Benchmark HuggingFace Inference API.\"\"\"\n",
    "        try:\n",
    "            from huggingface_hub import InferenceClient\n",
    "            client = InferenceClient()\n",
    "            \n",
    "            results = {\n",
    "                'latencies': [],\n",
    "                'token_counts': [],\n",
    "                'throughput': []\n",
    "            }\n",
    "            \n",
    "            for prompt in self.test_prompts:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                response = client.text_generation(\n",
    "                    prompt,\n",
    "                    model=\"microsoft/DialoGPT-medium\",\n",
    "                    max_new_tokens=150\n",
    "                )\n",
    "                \n",
    "                latency = time.time() - start_time\n",
    "                token_count = len(response.split())\n",
    "                \n",
    "                results['latencies'].append(latency)\n",
    "                results['token_counts'].append(token_count)\n",
    "                results['throughput'].append(token_count / latency)\n",
    "            \n",
    "            return self.summarize_results(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def summarize_results(self, results: Dict[str, List]) -> Dict[str, Any]:\n",
    "        \"\"\"Summarize benchmark results.\"\"\"\n",
    "        import numpy as np\n",
    "        \n",
    "        summary = {}\n",
    "        for metric, values in results.items():\n",
    "            if values:\n",
    "                summary[metric] = {\n",
    "                    'mean': np.mean(values),\n",
    "                    'median': np.median(values),\n",
    "                    'std': np.std(values),\n",
    "                    'min': np.min(values),\n",
    "                    'max': np.max(values)\n",
    "                }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def run_comprehensive_benchmark(self):\n",
    "        \"\"\"Run comprehensive benchmark across all available providers.\"\"\"\n",
    "        print(\"Running comprehensive benchmark...\")\n",
    "        \n",
    "        # Test OpenAI if available\n",
    "        if OPENAI_API_KEY != \"your-openai-key-here\":\n",
    "            print(\"\\n--- Benchmarking OpenAI ---\")\n",
    "            openai_results = self.benchmark_openai()\n",
    "            if 'error' not in openai_results:\n",
    "                self.results['openai'] = openai_results\n",
    "                print(\"✅ OpenAI benchmark completed\")\n",
    "            else:\n",
    "                print(f\"❌ OpenAI benchmark failed: {openai_results['error']}\")\n",
    "        \n",
    "        # Test HuggingFace if available\n",
    "        if HF_TOKEN != \"your-huggingface-token-here\":\n",
    "            print(\"\\n--- Benchmarking HuggingFace ---\")\n",
    "            hf_results = self.benchmark_huggingface()\n",
    "            if 'error' not in hf_results:\n",
    "                self.results['huggingface'] = hf_results\n",
    "                print(\"✅ HuggingFace benchmark completed\")\n",
    "            else:\n",
    "                print(f\"❌ HuggingFace benchmark failed: {hf_results['error']}\")\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "# Run comprehensive benchmark\n",
    "benchmark = InferenceBenchmark()\n",
    "results = benchmark.run_comprehensive_benchmark()\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BENCHMARK RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for provider, metrics in results.items():\n",
    "    print(f\"\\n{provider.upper()}:\")\n",
    "    for metric, stats in metrics.items():\n",
    "        print(f\"  {metric}:\")\n",
    "        print(f\"    Mean: {stats['mean']:.3f}\")\n",
    "        print(f\"    Median: {stats['median']:.3f}\")\n",
    "        print(f\"    Std Dev: {stats['std']:.3f}\")\n",
    "        print(f\"    Range: {stats['min']:.3f} - {stats['max']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Production Deployment Architecture\n",
    "\n",
    "Design a production-ready deployment architecture with monitoring and scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionDeployment:\n",
    "    \"\"\"Production-ready deployment architecture with monitoring and scaling.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.health_status = {}\n",
    "        self.request_metrics = {\n",
    "            'total_requests': 0,\n",
    "            'successful_requests': 0,\n",
    "            'failed_requests': 0,\n",
    "            'average_latency': 0,\n",
    "            'p95_latency': 0,\n",
    "            'p99_latency': 0\n",
    "        }\n",
    "        self.latencies = []\n",
    "    \n",
    "    def health_check(self, provider: str, client) -> Dict[str, Any]:\n",
    "        \"\"\"Perform health check on a provider.\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Simple health check request\n",
    "            response = client.chat_completion([\n",
    "                {\"role\": \"user\", \"content\": \"Health check\"}\n",
    "            ])\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            health_status = {\n",
    "                'status': 'healthy' if response['success'] else 'unhealthy',\n",
    "                'latency': latency,\n",
    "                'timestamp': time.time(),\n",
    "                'error': response.get('error', None)\n",
    "            }\n",
    "            \n",
    "            self.health_status[provider] = health_status\n",
    "            return health_status\n",
    "            \n",
    "        except Exception as e:\n",
    "            health_status = {\n",
    "                'status': 'unhealthy',\n",
    "                'latency': -1,\n",
    "                'timestamp': time.time(),\n",
    "                'error': str(e)\n",
    "            }\n",
    "            self.health_status[provider] = health_status\n",
    "            return health_status\n",
    "    \n",
    "    def record_request_metrics(self, success: bool, latency: float):\n",
    "        \"\"\"Record request metrics for monitoring.\"\"\"\n",
    "        self.request_metrics['total_requests'] += 1\n",
    "        \n",
    "        if success:\n",
    "            self.request_metrics['successful_requests'] += 1\n",
    "            self.latencies.append(latency)\n",
    "            \n",
    "            # Update latency metrics\n",
    "            if self.latencies:\n",
    "                import numpy as np\n",
    "                self.request_metrics['average_latency'] = np.mean(self.latencies)\n",
    "                self.request_metrics['p95_latency'] = np.percentile(self.latencies, 95)\n",
    "                self.request_metrics['p99_latency'] = np.percentile(self.latencies, 99)\n",
    "        else:\n",
    "            self.request_metrics['failed_requests'] += 1\n",
    "    \n",
    "    def get_system_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current system metrics.\"\"\"\n",
    "        import psutil\n",
    "        \n",
    "        return {\n",
    "            'cpu_percent': psutil.cpu_percent(interval=1),\n",
    "            'memory_percent': psutil.virtual_memory().percent,\n",
    "            'disk_usage': psutil.disk_usage('/').percent,\n",
    "            'network_connections': len(psutil.net_connections()),\n",
    "            'process_count': len(psutil.pids())\n",
    "        }\n",
    "    \n",
    "    def generate_deployment_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive deployment report.\"\"\"\n",
    "        success_rate = (self.request_metrics['successful_requests'] / \n",
    "                       max(self.request_metrics['total_requests'], 1)) * 100\n",
    "        \n",
    "        return {\n",
    "            'timestamp': time.time(),\n",
    "            'health_status': self.health_status,\n",
    "            'request_metrics': self.request_metrics,\n",
    "            'system_metrics': self.get_system_metrics(),\n",
    "            'success_rate': success_rate,\n",
    "            'recommendations': self.generate_recommendations()\n",
    "        }\n",
    "    \n",
    "    def generate_recommendations(self) -> List[str]:\n",
    "        \"\"\"Generate deployment recommendations based on metrics.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Check success rate\n",
    "        success_rate = (self.request_metrics['successful_requests'] / \n",
    "                       max(self.request_metrics['total_requests'], 1)) * 100\n",
    "        \n",
    "        if success_rate < 95:\n",
    "            recommendations.append(\"Consider implementing circuit breaker pattern for failed requests\")\n",
    "        \n",
    "        # Check latency\n",
    "        if self.request_metrics['p95_latency'] > 2.0:\n",
    "            recommendations.append(\"P95 latency is high. Consider scaling up resources or optimizing model\")\n",
    "        \n",
    "        # Check system resources\n",
    "        system_metrics = self.get_system_metrics()\n",
    "        \n",
    "        if system_metrics['cpu_percent'] > 80:\n",
    "            recommendations.append(\"High CPU usage detected. Consider horizontal scaling\")\n",
    "        \n",
    "        if system_metrics['memory_percent'] > 85:\n",
    "            recommendations.append(\"High memory usage detected. Consider increasing memory allocation\")\n",
    "        \n        return recommendations\n",
    "\n",
    "# Test production deployment monitoring\n",
    "print(\"Testing Production Deployment Monitoring...\")\n",
    "\n",
    "deployment = ProductionDeployment()\n",
    "\n",
    "# Test with available providers\n",
    "if OPENAI_API_KEY != \"your-openai-key-here\":\n",
    "    print(\"\\n--- Testing OpenAI Health Check ---\")\n",
    "    openai_client = UniversalAIClient(\"openai\", OPENAI_API_KEY)\n",
    "    health_status = deployment.health_check(\"openai\", openai_client)\n",
    "    print(f\"OpenAI Health: {health_status['status']}, Latency: {health_status['latency']:.3f}s\")\n",
    "    \n",
    "    # Record some test metrics\n",
    "    deployment.record_request_metrics(True, 0.5)\n",
    "    deployment.record_request_metrics(True, 0.7)\n",
    "    deployment.record_request_metrics(False, 1.2)\n",
    "\n",
    "# Generate deployment report\n",
    "report = deployment.generate_deployment_report()\n",
    "print(\"\\n--- Deployment Report ---\")\n",
    "print(f\"Success Rate: {report['success_rate']:.1f}%\")\n",
    "print(f\"Total Requests: {report['request_metrics']['total_requests']}\")\n",
    "print(f\"Average Latency: {report['request_metrics']['average_latency']:.3f}s\")\n",
    "print(f\"P95 Latency: {report['request_metrics']['p95_latency']:.3f}s\")\n",
    "\n",
    "if report['recommendations']:\n",
    "    print(\"\\nRecommendations:\")\n",
    "    for rec in report['recommendations']:\n",
    "        print(f\"  - {rec}\")\n",
    "else:\n",
    "    print(\"\\nNo recommendations - system is performing well!\")\n",
    "\n",
    "print(\"\\nSystem Metrics:\")\n",
    "for metric, value in report['system_metrics'].items():\n",
    "    print(f\"  {metric}: {value}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed the Model Interfaces and Deployment lab. Here's what you've learned:\n",
    "\n",
    "### Key Skills Acquired:\n",
    "- ✅ Universal OpenAI-compatible client implementation\n",
    "- ✅ HuggingFace provider management with failover\n",
    "- ✅ Local Ollama integration and testing\n",
    "- ✅ Comprehensive performance benchmarking\n",
    "- ✅ Production deployment monitoring and health checks\n",
    "- ✅ System metrics collection and analysis\n",
    "\n",
    "### Best Practices:\n",
    "- Always implement proper error handling and retry mechanisms\n",
    "- Use provider failover for high availability\n",
    "- Monitor system resources and performance metrics\n",
    "- Implement health checks for all services\n",
    "- Use environment variables for sensitive configuration\n",
    "- Design for scalability from the start\n",
    "\n",
    "### Production Considerations:\n",
    "- **Security**: Implement proper authentication and authorization\n",
    "- **Monitoring**: Set up comprehensive logging and alerting\n",
    "- **Scaling**: Design for horizontal scaling with load balancing\n",
    "- **Backup**: Implement backup and disaster recovery procedures\n",
    "- **Compliance**: Ensure compliance with data privacy regulations\n",
    "\n",
    "### Next Steps:\n",
    "1. Set up a local Ollama instance and test with different models\n",
    "2. Implement a load balancer for multiple inference engines\n",
    "3. Add comprehensive logging and monitoring dashboards\n",
    "4. Create automated deployment pipelines\n",
    "5. Implement A/B testing for model performance comparison\n",
    "6. Add support for streaming responses\n",
    "7. Implement rate limiting and quota management\n",
    "8. Set up automated scaling based on demand\n",
    "\n",
    "Remember: Production deployment requires careful planning, monitoring, and continuous optimization. Always test thoroughly in staging environments before deploying to production!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
