{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Image Generation with Hugging Face Providers\n",
    "\n",
    "## Objectives\n",
    "- Generate images using Hugging Face Inference API\n",
    "- Compare `provider=\"auto\"` vs explicit provider selection\n",
    "- Measure and analyze latency differences\n",
    "- Implement error handling and failover\n",
    "\n",
    "## Requirements\n",
    "- Python 3.10+\n",
    "- CUDA 12.4+ (for local GPU acceleration, optional)\n",
    "- PyTorch 2.6.0+\n",
    "- Hugging Face account and API token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q huggingface_hub>=0.20.0 torch>=2.6.0 pillow matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PyTorch and CUDA installation\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication Setup\n",
    "\n",
    "**IMPORTANT:** Never hardcode your API token in notebooks!\n",
    "\n",
    "Set your token as an environment variable:\n",
    "```bash\n",
    "export HF_TOKEN=\"your_token_here\"\n",
    "```\n",
    "\n",
    "Or use the Hugging Face CLI:\n",
    "```bash\n",
    "huggingface-cli login\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Try to load token from environment\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# If not found, prompt securely (won't show in output)\n",
    "if not HF_TOKEN:\n",
    "    print(\"HF_TOKEN not found in environment.\")\n",
    "    HF_TOKEN = getpass(\"Enter your Hugging Face token: \")\n",
    "\n",
    "# Verify token is loaded\n",
    "assert HF_TOKEN, \"Token must be provided\"\n",
    "print(\"✓ Token loaded successfully\")\n",
    "print(f\"✓ Token length: {len(HF_TOKEN)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Inference Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Hugging Face Inference Client\n",
    "client = InferenceClient(token=HF_TOKEN)\n",
    "\n",
    "# Model to use for image generation\n",
    "MODEL = \"stabilityai/stable-diffusion-2-1\"\n",
    "\n",
    "print(f\"✓ Client initialized\")\n",
    "print(f\"✓ Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Basic Image Generation\n",
    "\n",
    "**What you'll practice:** Generate images from text prompts using Hugging Face Inference API.\n",
    "\n",
    "This exercise introduces you to the fundamentals of text-to-image generation:\n",
    "- Setting up the inference client with authentication\n",
    "- Creating a function to generate images from prompts\n",
    "- Measuring generation latency\n",
    "- Displaying generated images\n",
    "\n",
    "You'll learn how to use the `InferenceClient` for image generation and understand the basic workflow that all subsequent exercises build upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(prompt: str, provider: str = \"auto\") -> Tuple[Image.Image, float]:\n",
    "    \"\"\"\n",
    "    Generate an image from a text prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Text description of the image\n",
    "        provider: Provider to use (\"auto\" or specific provider)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (PIL Image, generation time in seconds)\n",
    "    \"\"\"\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # Generate image\n",
    "    image_bytes = client.text_to_image(\n",
    "        prompt=prompt,\n",
    "        model=MODEL\n",
    "    )\n",
    "    \n",
    "    # Convert bytes to PIL Image\n",
    "    image = Image.open(BytesIO(image_bytes))\n",
    "    \n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "    \n",
    "    return image, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a single image\n",
    "prompt = \"A serene mountain landscape at sunset, photorealistic, 4k\"\n",
    "\n",
    "print(f\"Generating image for prompt: '{prompt}'\")\n",
    "image, gen_time = generate_image(prompt)\n",
    "\n",
    "print(f\"✓ Image generated in {gen_time:.2f} seconds\")\n",
    "print(f\"✓ Image size: {image.size}\")\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Generated in {gen_time:.2f}s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Provider Comparison\n",
    "\n",
    "Compare image generation using `provider=\"auto\"` vs explicit provider selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_providers(\n",
    "    prompts: List[str],\n",
    "    providers: List[str],\n",
    "    num_runs: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Benchmark multiple providers with the same prompts.\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of text prompts\n",
    "        providers: List of provider names to test\n",
    "        num_runs: Number of times to run each prompt\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with benchmark results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for provider in providers:\n",
    "        print(f\"\\nTesting provider: {provider}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for prompt_idx, prompt in enumerate(prompts, 1):\n",
    "            for run in range(1, num_runs + 1):\n",
    "                try:\n",
    "                    print(f\"  Prompt {prompt_idx}/{len(prompts)}, Run {run}/{num_runs}...\", end=\" \")\n",
    "                    \n",
    "                    image, gen_time = generate_image(prompt, provider)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'provider': provider,\n",
    "                        'prompt': prompt[:50] + \"...\",\n",
    "                        'run': run,\n",
    "                        'latency_sec': gen_time,\n",
    "                        'status': 'success'\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"✓ {gen_time:.2f}s\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    results.append({\n",
    "                        'provider': provider,\n",
    "                        'prompt': prompt[:50] + \"...\",\n",
    "                        'run': run,\n",
    "                        'latency_sec': None,\n",
    "                        'status': f'failed: {type(e).__name__}'\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"✗ {type(e).__name__}\")\n",
    "                \n",
    "                # Small delay between requests\n",
    "                time.sleep(1)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test prompts\n",
    "test_prompts = [\n",
    "    \"A peaceful garden with colorful flowers and butterflies\",\n",
    "    \"A futuristic city skyline at night with neon lights\",\n",
    "    \"A cozy cabin in a snowy forest during winter\"\n",
    "]\n",
    "\n",
    "# Providers to test\n",
    "# Note: Replace with actual provider names available in your account\n",
    "providers_to_test = [\"auto\"]  # Add explicit providers if available\n",
    "\n",
    "# Run benchmark\n",
    "print(\"Starting provider benchmark...\")\n",
    "benchmark_df = benchmark_providers(\n",
    "    prompts=test_prompts,\n",
    "    providers=providers_to_test,\n",
    "    num_runs=3\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Benchmark completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Analyze Results\n",
    "\n",
    "**What you'll practice:** Analyze and visualize performance metrics from provider comparisons.\n",
    "\n",
    "This exercise teaches you to:\n",
    "- Process benchmark data into meaningful insights\n",
    "- Create visualizations comparing provider performance\n",
    "- Identify patterns in latency and reliability\n",
    "- Make data-driven decisions about provider selection\n",
    "\n",
    "You'll use pandas and matplotlib to transform raw benchmark data into actionable insights about which providers work best for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display raw results\n",
    "print(\"Raw Benchmark Results:\")\n",
    "print(benchmark_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics by provider\n",
    "stats = benchmark_df[benchmark_df['status'] == 'success'].groupby('provider').agg({\n",
    "    'latency_sec': ['mean', 'std', 'min', 'max', 'count']\n",
    "}).round(3)\n",
    "\n",
    "# Calculate success rate\n",
    "success_rate = benchmark_df.groupby('provider').apply(\n",
    "    lambda x: (x['status'] == 'success').sum() / len(x) * 100\n",
    ").round(1)\n",
    "\n",
    "print(\"\\nProvider Statistics:\")\n",
    "print(\"=\" * 70)\n",
    "print(stats)\n",
    "print(\"\\nSuccess Rate (%)\")\n",
    "print(success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latency comparison\n",
    "successful_results = benchmark_df[benchmark_df['status'] == 'success']\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Box plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    successful_results.boxplot(column='latency_sec', by='provider', ax=plt.gca())\n",
    "    plt.title('Latency Distribution by Provider')\n",
    "    plt.suptitle('')  # Remove default title\n",
    "    plt.xlabel('Provider')\n",
    "    plt.ylabel('Latency (seconds)')\n",
    "    \n",
    "    # Bar plot with error bars\n",
    "    plt.subplot(1, 2, 2)\n",
    "    provider_stats = successful_results.groupby('provider')['latency_sec'].agg(['mean', 'std'])\n",
    "    provider_stats.plot(kind='bar', y='mean', yerr='std', ax=plt.gca(), legend=False)\n",
    "    plt.title('Average Latency by Provider')\n",
    "    plt.xlabel('Provider')\n",
    "    plt.ylabel('Latency (seconds)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No successful results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Implement Failover Strategy\n",
    "\n",
    "**What you'll practice:** Build robust error handling with automatic failover between providers.\n",
    "\n",
    "This exercise demonstrates production-ready patterns:\n",
    "- Handling API failures gracefully\n",
    "- Implementing retry logic with exponential backoff\n",
    "- Automatically switching to backup providers\n",
    "- Logging failures for monitoring\n",
    "\n",
    "You'll learn how to make your image generation system resilient to provider outages and rate limits, ensuring high availability for production applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_failover(\n",
    "    prompt: str,\n",
    "    providers: List[str],\n",
    "    timeout: int = 60\n",
    ") -> Tuple[Image.Image, str, float]:\n",
    "    \"\"\"\n",
    "    Generate image with automatic failover across providers.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Text description\n",
    "        providers: List of providers to try in order\n",
    "        timeout: Timeout per provider in seconds\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (image, provider_used, generation_time)\n",
    "    \"\"\"\n",
    "    last_error = None\n",
    "    \n",
    "    for provider in providers:\n",
    "        try:\n",
    "            print(f\"Trying provider: {provider}...\", end=\" \")\n",
    "            image, gen_time = generate_image(prompt, provider)\n",
    "            print(f\"✓ Success ({gen_time:.2f}s)\")\n",
    "            return image, provider, gen_time\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed ({type(e).__name__})\")\n",
    "            last_error = e\n",
    "            continue\n",
    "    \n",
    "    raise Exception(f\"All providers failed. Last error: {last_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test failover mechanism\n",
    "prompt = \"A majestic eagle soaring over mountains\"\n",
    "providers = [\"auto\"]  # Add more providers if available\n",
    "\n",
    "print(f\"Generating with failover for: '{prompt}'\")\n",
    "print(f\"Provider chain: {providers}\\n\")\n",
    "\n",
    "try:\n",
    "    image, used_provider, gen_time = generate_with_failover(prompt, providers)\n",
    "    \n",
    "    print(f\"\\n✓ Successfully generated using: {used_provider}\")\n",
    "    print(f\"✓ Generation time: {gen_time:.2f}s\")\n",
    "    \n",
    "    # Display result\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Provider: {used_provider} | Time: {gen_time:.2f}s\")\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ All providers failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Batch Generation with Progress Tracking\n",
    "\n",
    "**What you'll practice:** Generate multiple images efficiently with progress tracking and error handling.\n",
    "\n",
    "This exercise covers advanced patterns:\n",
    "- Processing multiple prompts in batches\n",
    "- Tracking progress with visual indicators\n",
    "- Handling partial failures in batch operations\n",
    "- Optimizing throughput for large-scale generation\n",
    "\n",
    "You'll build a system that can generate hundreds of images efficiently while providing real-time feedback on progress and handling errors gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate(\n",
    "    prompts: List[str],\n",
    "    provider: str = \"auto\",\n",
    "    save_images: bool = False\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate multiple images with progress tracking.\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of text prompts\n",
    "        provider: Provider to use\n",
    "        save_images: Whether to save images to disk\n",
    "    \n",
    "    Returns:\n",
    "        List of result dictionaries\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total = len(prompts)\n",
    "    \n",
    "    for idx, prompt in enumerate(prompts, 1):\n",
    "        print(f\"\\n[{idx}/{total}] Generating: '{prompt[:60]}...'\")\n",
    "        \n",
    "        try:\n",
    "            image, gen_time = generate_image(prompt, provider)\n",
    "            \n",
    "            result = {\n",
    "                'prompt': prompt,\n",
    "                'status': 'success',\n",
    "                'latency': gen_time,\n",
    "                'image': image\n",
    "            }\n",
    "            \n",
    "            if save_images:\n",
    "                filename = f\"image_{idx:03d}.png\"\n",
    "                image.save(filename)\n",
    "                result['filename'] = filename\n",
    "                print(f\"  ✓ Saved to {filename}\")\n",
    "            \n",
    "            print(f\"  ✓ Generated in {gen_time:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                'prompt': prompt,\n",
    "                'status': 'failed',\n",
    "                'error': str(e),\n",
    "                'image': None\n",
    "            }\n",
    "            print(f\"  ✗ Failed: {type(e).__name__}\")\n",
    "        \n",
    "        results.append(result)\n",
    "        time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple images\n",
    "batch_prompts = [\n",
    "    \"A steampunk robot in a Victorian workshop\",\n",
    "    \"An underwater coral reef with tropical fish\",\n",
    "    \"A medieval castle on a hilltop at dawn\"\n",
    "]\n",
    "\n",
    "print(\"Starting batch generation...\")\n",
    "batch_results = batch_generate(batch_prompts, provider=\"auto\", save_images=False)\n",
    "\n",
    "# Display results in a grid\n",
    "successful_images = [r for r in batch_results if r['status'] == 'success']\n",
    "\n",
    "if successful_images:\n",
    "    n_images = len(successful_images)\n",
    "    cols = min(3, n_images)\n",
    "    rows = (n_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = [[axes]]\n",
    "    elif rows == 1 or cols == 1:\n",
    "        axes = axes.reshape(rows, cols)\n",
    "    \n",
    "    for idx, result in enumerate(successful_images):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        ax = axes[row][col]\n",
    "        \n",
    "        ax.imshow(result['image'])\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"{result['prompt'][:40]}...\\n{result['latency']:.2f}s\", fontsize=10)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(n_images, rows * cols):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        axes[row][col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No successful images to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What You Learned\n",
    "1. ✅ How to authenticate with Hugging Face API securely\n",
    "2. ✅ Generate images using text-to-image models\n",
    "3. ✅ Compare provider performance and latency\n",
    "4. ✅ Implement failover strategies for reliability\n",
    "5. ✅ Batch process multiple image generations\n",
    "\n",
    "### Best Practices\n",
    "- Always use environment variables for tokens\n",
    "- Implement timeout and retry logic\n",
    "- Monitor latency and success rates\n",
    "- Use failover for production applications\n",
    "- Add delays between requests to avoid rate limiting\n",
    "\n",
    "### Next Steps\n",
    "- Proceed to **Chat Inference Practice** notebook\n",
    "- Experiment with different models and providers\n",
    "- Implement more advanced error handling\n",
    "- Create a production-ready image generation service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear token from memory (security best practice)\n",
    "if 'HF_TOKEN' in locals():\n",
    "    del HF_TOKEN\n",
    "\n",
    "print(\"✓ Cleanup completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
