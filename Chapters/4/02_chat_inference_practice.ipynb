{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Chat Inference with Hugging Face Providers\n",
    "\n",
    "## Objectives\n",
    "- Perform chat inference using Hugging Face models\n",
    "- Compare provider performance for conversational AI\n",
    "- Measure latency and token throughput\n",
    "- Implement streaming responses\n",
    "\n",
    "## Requirements\n",
    "- Python 3.10+\n",
    "- PyTorch 2.6.0+\n",
    "- CUDA 12.4+ (optional)\n",
    "- Hugging Face API token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q huggingface_hub>=0.20.0 torch>=2.6.0 pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from huggingface_hub import InferenceClient\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Generator\n",
    "import json\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load token securely\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    HF_TOKEN = getpass(\"Enter your Hugging Face token: \")\n",
    "\n",
    "assert HF_TOKEN, \"Token required\"\n",
    "print(\"✓ Token loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "client = InferenceClient(token=HF_TOKEN)\n",
    "\n",
    "# Model for chat\n",
    "CHAT_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "print(f\"✓ Using model: {CHAT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Basic Chat Completion\n",
    "\n",
    "**What you'll practice:** Perform basic chat inference using Hugging Face models.\n",
    "\n",
    "This exercise introduces you to conversational AI:\n",
    "- Setting up chat completion requests with message history\n",
    "- Understanding the message format (system, user, assistant roles)\n",
    "- Measuring response latency\n",
    "- Handling chat completion responses\n",
    "\n",
    "You'll learn the fundamental pattern for all conversational AI interactions, which forms the basis for more advanced features like multi-turn conversations and streaming responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion(\n",
    "    messages: List[Dict[str, str]],\n",
    "    model: str = CHAT_MODEL,\n",
    "    max_tokens: int = 500,\n",
    "    temperature: float = 0.7\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Perform chat completion.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dicts with 'role' and 'content'\n",
    "        model: Model to use\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (response_text, latency_seconds)\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    response = client.chat_completion(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    latency = time.perf_counter() - start\n",
    "    response_text = response.choices[0].message.content\n",
    "    \n",
    "    return response_text, latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat example\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain machine learning in simple terms.\"}\n",
    "]\n",
    "\n",
    "print(\"Sending chat request...\")\n",
    "response, latency = chat_completion(messages)\n",
    "\n",
    "print(f\"\\n✓ Response received in {latency:.2f}s\")\n",
    "print(f\"\\nAssistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Multi-Turn Conversation\n",
    "\n",
    "**What you'll practice:** Build conversations that maintain context across multiple exchanges.\n",
    "\n",
    "This exercise teaches you to:\n",
    "- Maintain conversation history across multiple turns\n",
    "- Format messages correctly for context preservation\n",
    "- Handle conversation state management\n",
    "- Create natural dialogue flows\n",
    "\n",
    "You'll learn how to build chatbots and assistants that remember previous parts of the conversation, enabling more natural and useful interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatSession:\n",
    "    \"\"\"Manage a multi-turn conversation\"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt: str = \"You are a helpful assistant.\"):\n",
    "        self.messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt}\n",
    "        ]\n",
    "        self.latencies = []\n",
    "    \n",
    "    def send(self, user_message: str) -> str:\n",
    "        \"\"\"Send a message and get response\"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        response, latency = chat_completion(self.messages)\n",
    "        self.latencies.append(latency)\n",
    "        \n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_history(self) -> List[Dict]:\n",
    "        \"\"\"Get conversation history\"\"\"\n",
    "        return [m for m in self.messages if m[\"role\"] != \"system\"]\n",
    "    \n",
    "    def print_conversation(self):\n",
    "        \"\"\"Print formatted conversation\"\"\"\n",
    "        for msg in self.get_history():\n",
    "            role = msg[\"role\"].capitalize()\n",
    "            print(f\"\\n{role}: {msg['content']}\")\n",
    "            print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat session\n",
    "session = ChatSession(\"You are an expert in artificial intelligence.\")\n",
    "\n",
    "# Multi-turn conversation\n",
    "questions = [\n",
    "    \"What is deep learning?\",\n",
    "    \"How does it differ from traditional machine learning?\",\n",
    "    \"Can you give a practical example?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\n[Turn {i}] User: {question}\")\n",
    "    response = session.send(question)\n",
    "    print(f\"Assistant: {response[:200]}...\")\n",
    "    print(f\"Latency: {session.latencies[-1]:.2f}s\")\n",
    "\n",
    "print(f\"\\n✓ Conversation completed\")\n",
    "print(f\"Average latency: {sum(session.latencies) / len(session.latencies):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Provider Performance Comparison\n",
    "\n",
    "**What you'll practice:** Compare different providers' performance for chat inference.\n",
    "\n",
    "This exercise demonstrates how to:\n",
    "- Benchmark multiple providers with the same prompts\n",
    "- Measure latency, throughput, and token generation rates\n",
    "- Analyze performance differences between providers\n",
    "- Make informed decisions about provider selection\n",
    "\n",
    "You'll learn to systematically evaluate different AI providers to choose the best one for your specific use case, balancing cost, speed, and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_chat_providers(\n",
    "    test_messages: List[List[Dict]],\n",
    "    providers: List[str] = [\"auto\"],\n",
    "    num_runs: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Benchmark chat performance across providers.\n",
    "    \n",
    "    Args:\n",
    "        test_messages: List of message lists to test\n",
    "        providers: Providers to test\n",
    "        num_runs: Runs per test case\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for provider in providers:\n",
    "        print(f\"\\nTesting provider: {provider}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for msg_idx, messages in enumerate(test_messages, 1):\n",
    "            for run in range(1, num_runs + 1):\n",
    "                try:\n",
    "                    print(f\"  Test {msg_idx}/{len(test_messages)}, Run {run}...\", end=\" \")\n",
    "                    \n",
    "                    response, latency = chat_completion(messages)\n",
    "                    tokens = len(response.split())  # Approximate token count\n",
    "                    \n",
    "                    results.append({\n",
    "                        'provider': provider,\n",
    "                        'test_case': msg_idx,\n",
    "                        'run': run,\n",
    "                        'latency_sec': latency,\n",
    "                        'tokens': tokens,\n",
    "                        'tokens_per_sec': tokens / latency if latency > 0 else 0,\n",
    "                        'status': 'success'\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"✓ {latency:.2f}s, {tokens} tokens\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    results.append({\n",
    "                        'provider': provider,\n",
    "                        'test_case': msg_idx,\n",
    "                        'run': run,\n",
    "                        'latency_sec': None,\n",
    "                        'tokens': None,\n",
    "                        'tokens_per_sec': None,\n",
    "                        'status': f'failed: {type(e).__name__}'\n",
    "                    })\n",
    "                    print(f\"✗ {type(e).__name__}\")\n",
    "                \n",
    "                time.sleep(1)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test cases\n",
    "test_cases = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is Python?\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a coding expert.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain list comprehensions in Python.\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a data science tutor.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the difference between supervised and unsupervised learning?\"}\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Run benchmark\n",
    "print(\"Starting chat benchmark...\")\n",
    "chat_benchmark_df = benchmark_chat_providers(\n",
    "    test_messages=test_cases,\n",
    "    providers=[\"auto\"],\n",
    "    num_runs=3\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Benchmark completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "successful = chat_benchmark_df[chat_benchmark_df['status'] == 'success']\n",
    "\n",
    "if len(successful) > 0:\n",
    "    stats = successful.groupby('provider').agg({\n",
    "        'latency_sec': ['mean', 'std', 'min', 'max'],\n",
    "        'tokens_per_sec': ['mean', 'std']\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nProvider Statistics:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(stats)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Latency\n",
    "    successful.boxplot(column='latency_sec', by='provider', ax=ax1)\n",
    "    ax1.set_title('Latency Distribution')\n",
    "    ax1.set_ylabel('Seconds')\n",
    "    \n",
    "    # Throughput\n",
    "    successful.boxplot(column='tokens_per_sec', by='provider', ax=ax2)\n",
    "    ax2.set_title('Token Throughput')\n",
    "    ax2.set_ylabel('Tokens/Second')\n",
    "    \n",
    "    plt.suptitle('')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No successful results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_stream(\n",
    "    messages: List[Dict[str, str]],\n",
    "    model: str = CHAT_MODEL\n",
    ") -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Stream chat completion tokens.\n",
    "    \n",
    "    Yields:\n",
    "        Token strings as they arrive\n",
    "    \"\"\"\n",
    "    stream = client.chat_completion(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        stream=True,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            yield chunk.choices[0].delta.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test streaming\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a creative writer.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short story about a robot learning to paint.\"}\n",
    "]\n",
    "\n",
    "print(\"Streaming response:\\n\")\n",
    "print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "full_response = \"\"\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "for token in chat_stream(messages):\n",
    "    print(token, end=\"\", flush=True)\n",
    "    full_response += token\n",
    "\n",
    "elapsed = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"\\n\\n✓ Streamed in {elapsed:.2f}s\")\n",
    "print(f\"✓ Total tokens: {len(full_response.split())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Error Handling and Retry Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_retry(\n",
    "    messages: List[Dict],\n",
    "    max_retries: int = 3,\n",
    "    backoff_factor: float = 2.0\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Chat completion with exponential backoff retry.\n",
    "    \n",
    "    Args:\n",
    "        messages: Chat messages\n",
    "        max_retries: Maximum retry attempts\n",
    "        backoff_factor: Multiplier for delay\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (response, latency, attempts)\n",
    "    \"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt}/{max_retries}...\", end=\" \")\n",
    "            response, latency = chat_completion(messages)\n",
    "            print(\"✓ Success\")\n",
    "            return response, latency, attempt\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed: {type(e).__name__}\")\n",
    "            \n",
    "            if attempt < max_retries:\n",
    "                delay = backoff_factor ** (attempt - 1)\n",
    "                print(f\"  Retrying in {delay:.1f}s...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise Exception(f\"Failed after {max_retries} attempts: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retry logic\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    response, latency, attempts = chat_with_retry(messages)\n",
    "    print(f\"\\nResponse: {response}\")\n",
    "    print(f\"Latency: {latency:.2f}s\")\n",
    "    print(f\"Attempts: {attempts}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAll retries failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "1. ✅ Performed chat completions with Hugging Face models\n",
    "2. ✅ Managed multi-turn conversations\n",
    "3. ✅ Benchmarked provider performance\n",
    "4. ✅ Implemented streaming responses\n",
    "5. ✅ Added robust error handling\n",
    "\n",
    "### Best Practices\n",
    "- Maintain conversation context in messages list\n",
    "- Use streaming for better UX in interactive apps\n",
    "- Implement retry logic with exponential backoff\n",
    "- Monitor token throughput and latency\n",
    "- Handle errors gracefully\n",
    "\n",
    "### Next Steps\n",
    "- Complete the **Provider Benchmarking** notebook\n",
    "- Proceed to **Section 6: Local Inference Endpoints**\n",
    "- Build a production chat application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "if 'HF_TOKEN' in locals():\n",
    "    del HF_TOKEN\n",
    "print(\"✓ Cleanup completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
