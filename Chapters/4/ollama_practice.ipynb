{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Ollama Setup and Practice\n",
    "\n",
    "## Objectives\n",
    "- Install and configure Ollama\n",
    "- Pull and run models using CLI\n",
    "- Test REST API endpoints\n",
    "- Use OpenAI-compatible interface\n",
    "- Measure performance metrics\n",
    "\n",
    "## Requirements\n",
    "- Python 3.10+\n",
    "- CUDA 12.4+ (for GPU acceleration)\n",
    "- PyTorch 2.6.0+\n",
    "- 8GB+ VRAM recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Installation and Setup\n",
    "\n",
    "### Install Ollama\n",
    "\n",
    "Run this in your terminal (not in notebook):\n",
    "\n",
    "```bash\n",
    "# Linux\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# macOS\n",
    "brew install ollama\n",
    "\n",
    "# Verify\n",
    "ollama --version\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python dependencies\n",
    "!pip install -q requests openai pandas matplotlib torch>=2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "import subprocess\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Verify Ollama Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Ollama is installed\n",
    "try:\n",
    "    result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)\n",
    "    print(\"✓ Ollama installed\")\n",
    "    print(f\"Version: {result.stdout.strip()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"✗ Ollama not found. Please install it first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Ollama server is running\n",
    "def check_ollama_server(url=\"http://localhost:11434\"):\n",
    "    \"\"\"Check if Ollama server is accessible\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{url}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"✓ Ollama server is running\")\n",
    "            return True\n",
    "    except requests.exceptions.RequestException:\n",
    "        pass\n",
    "    \n",
    "    print(\"✗ Ollama server not running\")\n",
    "    print(\"Start it with: ollama serve\")\n",
    "    return False\n",
    "\n",
    "server_running = check_ollama_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model Management\n",
    "\n",
    "### Pull a Model\n",
    "\n",
    "Run in terminal:\n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models\n",
    "def list_ollama_models():\n",
    "    \"\"\"List all downloaded Ollama models\"\"\"\n",
    "    url = \"http://localhost:11434/api/tags\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'models' in data and data['models']:\n",
    "            print(\"Available models:\")\n",
    "            for model in data['models']:\n",
    "                name = model.get('name', 'unknown')\n",
    "                size = model.get('size', 0) / (1024**3)  # Convert to GB\n",
    "                print(f\"  - {name} ({size:.2f} GB)\")\n",
    "            return data['models']\n",
    "        else:\n",
    "            print(\"No models found. Pull a model first:\")\n",
    "            print(\"  ollama pull llama3.2:3b\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models: {e}\")\n",
    "        return []\n",
    "\n",
    "models = list_ollama_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: REST API Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "MODEL_NAME = \"llama3.2:3b\"  # Change if you pulled a different model\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Server URL: {OLLAMA_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_generate(prompt: str, model: str = MODEL_NAME, stream: bool = False) -> tuple:\n",
    "    \"\"\"\n",
    "    Generate text using Ollama API.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (response_text, latency_seconds)\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_URL}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": stream\n",
    "    }\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()\n",
    "    latency = time.perf_counter() - start\n",
    "    \n",
    "    result = response.json()\n",
    "    return result.get('response', ''), latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic generation\n",
    "prompt = \"Explain machine learning in one sentence.\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"Generating...\\n\")\n",
    "\n",
    "response, latency = ollama_generate(prompt)\n",
    "\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"\\nLatency: {latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Chat API Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_chat(messages: List[Dict], model: str = MODEL_NAME) -> tuple:\n",
    "    \"\"\"\n",
    "    Chat using Ollama API.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dicts with 'role' and 'content'\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (response_text, latency_seconds)\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_URL}/api/chat\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()\n",
    "    latency = time.perf_counter() - start\n",
    "    \n",
    "    result = response.json()\n",
    "    return result['message']['content'], latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chat\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Python programming language?\"}\n",
    "]\n",
    "\n",
    "print(\"Sending chat request...\\n\")\n",
    "response, latency = ollama_chat(messages)\n",
    "\n",
    "print(f\"Assistant: {response}\")\n",
    "print(f\"\\nLatency: {latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: OpenAI-Compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client pointing to Ollama\n",
    "client = OpenAI(\n",
    "    base_url=f\"{OLLAMA_URL}/v1\",\n",
    "    api_key=\"ollama\"  # Required but not used\n",
    ")\n",
    "\n",
    "print(\"✓ OpenAI client configured for Ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with OpenAI client\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a coding expert.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a Python function to calculate factorial.\"}\n",
    "]\n",
    "\n",
    "print(\"Generating with OpenAI client...\\n\")\n",
    "start = time.perf_counter()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "latency = time.perf_counter() - start\n",
    "\n",
    "print(f\"Response:\\n{response.choices[0].message.content}\")\n",
    "print(f\"\\nLatency: {latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test streaming with OpenAI client\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write a haiku about programming.\"}\n",
    "]\n",
    "\n",
    "print(\"Streaming response:\\n\")\n",
    "print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "start = time.perf_counter()\n",
    "full_response = \"\"\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        print(content, end=\"\", flush=True)\n",
    "        full_response += content\n",
    "\n",
    "latency = time.perf_counter() - start\n",
    "print(f\"\\n\\n✓ Streamed in {latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_ollama(prompts: List[str], num_runs: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Benchmark Ollama performance.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt_idx, prompt in enumerate(prompts, 1):\n",
    "        print(f\"\\nPrompt {prompt_idx}/{len(prompts)}: {prompt[:50]}...\")\n",
    "        \n",
    "        for run in range(1, num_runs + 1):\n",
    "            try:\n",
    "                print(f\"  Run {run}/{num_runs}...\", end=\" \")\n",
    "                \n",
    "                messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                response, latency = ollama_chat(messages)\n",
    "                \n",
    "                tokens = len(response.split())\n",
    "                tokens_per_sec = tokens / latency if latency > 0 else 0\n",
    "                \n",
    "                results.append({\n",
    "                    'prompt_idx': prompt_idx,\n",
    "                    'run': run,\n",
    "                    'latency_sec': latency,\n",
    "                    'tokens': tokens,\n",
    "                    'tokens_per_sec': tokens_per_sec,\n",
    "                    'status': 'success'\n",
    "                })\n",
    "                \n",
    "                print(f\"✓ {latency:.2f}s, {tokens} tokens, {tokens_per_sec:.1f} tok/s\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    'prompt_idx': prompt_idx,\n",
    "                    'run': run,\n",
    "                    'latency_sec': None,\n",
    "                    'tokens': None,\n",
    "                    'tokens_per_sec': None,\n",
    "                    'status': f'failed: {type(e).__name__}'\n",
    "                })\n",
    "                print(f\"✗ {type(e).__name__}\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark\n",
    "test_prompts = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Explain the difference between supervised and unsupervised learning.\",\n",
    "    \"Write a Python function to reverse a string.\"\n",
    "]\n",
    "\n",
    "print(\"Starting Ollama benchmark...\")\n",
    "benchmark_df = benchmark_ollama(test_prompts, num_runs=3)\n",
    "\n",
    "print(\"\\n✓ Benchmark completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "successful = benchmark_df[benchmark_df['status'] == 'success']\n",
    "\n",
    "if len(successful) > 0:\n",
    "    stats = successful.agg({\n",
    "        'latency_sec': ['mean', 'std', 'min', 'max'],\n",
    "        'tokens_per_sec': ['mean', 'std']\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nPerformance Statistics:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(stats)\n",
    "    \n",
    "    success_rate = (len(successful) / len(benchmark_df)) * 100\n",
    "    print(f\"\\nSuccess Rate: {success_rate:.1f}%\")\n",
    "else:\n",
    "    print(\"No successful results to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "if len(successful) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Latency distribution\n",
    "    ax1.hist(successful['latency_sec'], bins=10, edgecolor='black')\n",
    "    ax1.set_xlabel('Latency (seconds)')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Latency Distribution')\n",
    "    ax1.axvline(successful['latency_sec'].mean(), color='red', \n",
    "                linestyle='--', label=f\"Mean: {successful['latency_sec'].mean():.2f}s\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Throughput\n",
    "    ax2.hist(successful['tokens_per_sec'], bins=10, edgecolor='black', color='green')\n",
    "    ax2.set_xlabel('Tokens per Second')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Throughput Distribution')\n",
    "    ax2.axvline(successful['tokens_per_sec'].mean(), color='red',\n",
    "                linestyle='--', label=f\"Mean: {successful['tokens_per_sec'].mean():.1f} tok/s\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Multi-Turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaChat:\n",
    "    \"\"\"Manage multi-turn conversations with Ollama\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = MODEL_NAME, system_prompt: str = None):\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "        if system_prompt:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    def send(self, user_message: str) -> str:\n",
    "        \"\"\"Send a message and get response\"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        response, latency = ollama_chat(self.messages, self.model)\n",
    "        \n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.messages = []\n",
    "    \n",
    "    def get_history(self):\n",
    "        \"\"\"Get conversation history\"\"\"\n",
    "        return self.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-turn conversation\n",
    "chat = OllamaChat(system_prompt=\"You are a helpful Python tutor.\")\n",
    "\n",
    "questions = [\n",
    "    \"What are list comprehensions?\",\n",
    "    \"Can you show me an example?\",\n",
    "    \"How is it different from a for loop?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\n[Turn {i}]\")\n",
    "    print(f\"User: {question}\")\n",
    "    response = chat.send(question)\n",
    "    print(f\"Assistant: {response[:200]}...\")\n",
    "\n",
    "print(f\"\\n✓ Conversation completed ({len(chat.get_history())} messages)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "1. ✅ Installed and configured Ollama\n",
    "2. ✅ Pulled and managed models\n",
    "3. ✅ Used REST API for generation and chat\n",
    "4. ✅ Tested OpenAI-compatible interface\n",
    "5. ✅ Implemented streaming responses\n",
    "6. ✅ Benchmarked performance\n",
    "7. ✅ Built multi-turn conversations\n",
    "\n",
    "### Key Metrics\n",
    "- Average latency\n",
    "- Token throughput (tokens/second)\n",
    "- Success rate\n",
    "- Memory usage\n",
    "\n",
    "### Next Steps\n",
    "- Complete **vLLM Practice** notebook\n",
    "- Compare Ollama vs vLLM performance\n",
    "- Build a production chatbot\n",
    "- Explore model customization with Modelfiles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
