{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource Monitoring and Troubleshooting - Practice Notebook\n",
    "\n",
    "**Prerequisites**: Python 3.10+, PyTorch 2.6.0+, CUDA 12.4+\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Monitor GPU Resources**: Track memory usage and utilization in real-time\n",
    "2. **Detect Memory Leaks**: Identify and fix common PyTorch memory issues\n",
    "3. **Monitor System Resources**: Track CPU, RAM, and disk usage\n",
    "4. **Implement Retry Strategies**: Build robust error handling with exponential backoff\n",
    "5. **Profile Performance**: Use PyTorch Profiler to find bottlenecks\n",
    "6. **Visualize Metrics**: Create real-time monitoring dashboards\n",
    "\n",
    "## Why Resource Monitoring Matters in AI/ML\n",
    "\n",
    "Resource monitoring is crucial for:\n",
    "\n",
    "- **Performance Optimization**: GPU resources are expensive. Identifying bottlenecks helps you train faster and cheaper.\n",
    "- **Cost Management**: Cloud GPU instances can cost $1-10+ per hour. Efficient resource use saves money.\n",
    "- **Reliability**: Out-of-memory errors can crash training jobs that took hours. Prevention is key.\n",
    "- **Debugging**: Understanding resource usage patterns helps diagnose mysterious failures.\n",
    "- **Production Readiness**: Production ML systems need monitoring to ensure SLAs and detect issues.\n",
    "\n",
    "## How This Notebook Works\n",
    "\n",
    "Each exercise follows this pattern:\n",
    "1. **Introduction**: Explains the concept and why it matters\n",
    "2. **Exercise**: A TODO section for you to try implementing\n",
    "3. **Solution**: Complete working code with detailed comments\n",
    "4. **Key Insights**: Important takeaways from the exercise\n",
    "\n",
    "ðŸ’¡ **Tip**: Try completing each TODO before looking at the solution\\!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Environment Verification\n",
    "\n",
    "**Purpose**: Import essential libraries and verify your environment is properly configured.\n",
    "\n",
    "**Key Libraries**:\n",
    "- **torch**: PyTorch deep learning framework with CUDA support\n",
    "- **psutil**: Cross-platform library for system and process monitoring\n",
    "- **matplotlib**: For creating visualizations of resource usage\n",
    "- **numpy**: Numerical computing (used for data manipulation)\n",
    "\n",
    "**What to Check**:\n",
    "- Python version (should be 3.10+)\n",
    "- PyTorch version (should be 2.6.0+)\n",
    "- CUDA availability (True if you have a GPU)\n",
    "\n",
    "If CUDA shows False but you have a GPU, check:\n",
    "1. NVIDIA drivers are installed\n",
    "2. PyTorch was installed with CUDA support\n",
    "3. CUDA version matches PyTorch requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, psutil, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import wraps\n",
    "import random\n",
    "\n",
    "print(f'Python: {sys.version}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: GPU Monitoring\n",
    "\n",
    "**Purpose**: Learn to monitor GPU resources programmatically using PyTorch's CUDA API.\n",
    "\n",
    "### Why GPU Monitoring Matters\n",
    "\n",
    "GPUs are the most expensive resource in deep learning:\n",
    "- A single NVIDIA A100 GPU costs $10,000-15,000\n",
    "- Cloud GPU instances: $1-10+ per hour\n",
    "- Training large models can take weeks of GPU time\n",
    "\n",
    "Monitoring helps you:\n",
    "- **Detect underutilization**: Are you using 10% of GPU when you could use 80%?\n",
    "- **Prevent OOM errors**: Know when you're approaching memory limits\n",
    "- **Optimize batch sizes**: Find the sweet spot between speed and memory\n",
    "- **Debug memory leaks**: Track memory growth over time\n",
    "\n",
    "### What You'll Monitor\n",
    "\n",
    "1. **Device Name**: Which GPU model you're using (e.g., RTX 3090, A100)\n",
    "2. **Total Memory**: Maximum GPU memory available (e.g., 24GB, 40GB)\n",
    "3. **Allocated Memory**: Memory currently in use by PyTorch tensors\n",
    "4. **Reserved Memory**: Memory reserved by PyTorch's caching allocator\n",
    "5. **Free Memory**: Available memory for new allocations\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Allocated vs Reserved**: PyTorch reserves memory in chunks for efficiency. Allocated is what's actually used.\n",
    "- **Memory Fragmentation**: Even with free memory, large allocations might fail due to fragmentation.\n",
    "- **Multiple GPUs**: If you have multiple GPUs, monitor each one separately.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement `monitor_gpu()` function that:\n",
    "1. Checks if CUDA is available\n",
    "2. Iterates through all available GPUs\n",
    "3. Displays memory statistics for each GPU\n",
    "\n",
    "**Hints**:\n",
    "- Use `torch.cuda.is_available()` to check for CUDA\n",
    "- Use `torch.cuda.device_count()` for number of GPUs\n",
    "- Use `torch.cuda.get_device_properties(i).total_memory` for total memory\n",
    "- Use `torch.cuda.memory_allocated(i)` for used memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement GPU monitoring\n",
    "def monitor_gpu():\n",
    "    if not torch.cuda.is_available():\n",
    "        print('CUDA not available')\n",
    "        return\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "monitor_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_gpu():\n",
    "    if not torch.cuda.is_available():\n",
    "        print('CUDA not available')\n",
    "        return\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        print(f'  Total: {total:.2f} GB')\n",
    "        print(f'  Allocated: {allocated:.2f} GB')\n",
    "        print(f'  Reserved: {reserved:.2f} GB')\n",
    "\n",
    "monitor_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Detecting and Fixing Memory Leaks\n",
    "\n",
    "**Purpose**: Learn to identify and fix the most common memory leak in PyTorch code.\n",
    "\n",
    "### The Problem: Hidden Computation Graphs\n",
    "\n",
    "PyTorch uses **automatic differentiation** (autograd) which builds a computation graph:\n",
    "- Every tensor operation is recorded\n",
    "- Gradients are computed by traversing this graph backward\n",
    "- The graph is stored in memory until explicitly freed\n",
    "\n",
    "**Common Mistake**: Storing tensors that still have computation graphs attached:\n",
    "\n",
    "```python\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    loss = model(data).sum()\n",
    "    losses.append(loss)  # LEAK\\! Stores entire graph\n",
    "```\n",
    "\n",
    "### Why This Causes Memory Leaks\n",
    "\n",
    "When you append a tensor with `requires_grad=True` to a list:\n",
    "1. The tensor keeps a reference to the computation graph\n",
    "2. The graph includes all intermediate tensors and operations\n",
    "3. These never get garbage collected\n",
    "4. Memory usage grows linearly with training steps\n",
    "\n",
    "**Real Impact**: A model training for 10,000 steps might leak 10GB+ of memory\\!\n",
    "\n",
    "### The Solution: Detach from Graph\n",
    "\n",
    "Use `.item()` to extract the scalar value:\n",
    "\n",
    "```python\n",
    "losses.append(loss.item())  # FIX\\! Only stores the number\n",
    "```\n",
    "\n",
    "Alternatives:\n",
    "- `loss.detach()` - detaches but keeps as tensor\n",
    "- `loss.cpu().numpy()` - converts to numpy array\n",
    "- Don't store it at all if not needed\n",
    "\n",
    "### Your Task\n",
    "\n",
    "1. Run `train_with_leak()` and observe memory growth\n",
    "2. Identify the line causing the leak\n",
    "3. Fix it in `train_fixed()`\n",
    "4. Verify memory stays constant\n",
    "\n",
    "**What to Observe**:\n",
    "- Memory should grow steadily in the leaky version\n",
    "- Memory should stabilize in the fixed version\n",
    "- The difference can be dramatic (100s of MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix the memory leak\n",
    "def train_with_leak():\n",
    "    model = torch.nn.Linear(1000,1000).cuda() if torch.cuda.is_available() else torch.nn.Linear(1000,1000)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    losses = []  # Memory leak\\!\n",
    "    \n",
    "    for i in range(50):\n",
    "        x = torch.randn(32,1000).cuda() if torch.cuda.is_available() else torch.randn(32,1000)\n",
    "        y = model(x).sum()\n",
    "        optimizer.zero_grad()\n",
    "        y.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(y)  # LEAK: stores entire graph\n",
    "        \n",
    "        if i%10==0 and torch.cuda.is_available():\n",
    "            print(f'Step {i}: {torch.cuda.memory_allocated()/1024**2:.1f} MB')\n",
    "\n",
    "train_with_leak()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixed():\n",
    "    model = torch.nn.Linear(1000,1000).cuda() if torch.cuda.is_available() else torch.nn.Linear(1000,1000)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(50):\n",
    "        x = torch.randn(32,1000).cuda() if torch.cuda.is_available() else torch.randn(32,1000)\n",
    "        y = model(x).sum()\n",
    "        optimizer.zero_grad()\n",
    "        y.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(y.item())  # FIX: use .item() to get scalar\n",
    "        \n",
    "        if i%10==0 and torch.cuda.is_available():\n",
    "            print(f'Step {i}: {torch.cuda.memory_allocated()/1024**2:.1f} MB')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "train_fixed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: System Resource Monitoring\n",
    "\n",
    "**Purpose**: Monitor CPU, RAM, and disk usage to ensure your entire system is healthy.\n",
    "\n",
    "### Beyond GPU: Why Monitor System Resources?\n",
    "\n",
    "While GPU is critical for training, bottlenecks can occur elsewhere:\n",
    "\n",
    "**CPU Bottlenecks**:\n",
    "- Data loading and preprocessing\n",
    "- DataLoader workers processing images/text\n",
    "- Augmentation pipelines\n",
    "- If CPU is at 100%, your GPU might be idle waiting for data\\!\n",
    "\n",
    "**RAM Issues**:\n",
    "- Large datasets need to fit in RAM\n",
    "- Running out of RAM causes swapping (very slow)\n",
    "- Multiple processes competing for memory\n",
    "- System might kill your process (OOM killer)\n",
    "\n",
    "**Disk Space**:\n",
    "- Checkpoints can be huge (10GB+ for large models)\n",
    "- Logs and cached datasets accumulate\n",
    "- Running out of disk crashes training\n",
    "\n",
    "### Understanding the Metrics\n",
    "\n",
    "**CPU Usage**:\n",
    "- 0-100% per core (or overall)\n",
    "- High usage during data loading is normal\n",
    "- Sustained 100% might indicate bottleneck\n",
    "\n",
    "**Memory**:\n",
    "- Total: Physical RAM installed\n",
    "- Used: Currently allocated memory\n",
    "- Available: Free + reclaimable cache\n",
    "- Percent: Used / Total\n",
    "\n",
    "**Disk**:\n",
    "- Total/Used/Free in bytes\n",
    "- Percent full\n",
    "- Watch for >90% usage (danger zone)\n",
    "\n",
    "### The psutil Library\n",
    "\n",
    "`psutil` (Python system and process utilities) provides:\n",
    "- Cross-platform compatibility (Linux, Windows, macOS)\n",
    "- Real-time monitoring\n",
    "- Per-process and system-wide stats\n",
    "- Easy-to-use API\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement `monitor_system()` to display:\n",
    "1. CPU usage percentage and core count\n",
    "2. RAM total, used, and available (in GB)\n",
    "3. Disk total, used, and free (in GB)\n",
    "\n",
    "**Hints**:\n",
    "- `psutil.cpu_percent(interval=1)` - CPU usage (wait 1 sec for accurate reading)\n",
    "- `psutil.virtual_memory()` - RAM stats\n",
    "- `psutil.disk_usage('/')` - Disk stats for root partition\n",
    "- Convert bytes to GB: `bytes / 1024**3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement system monitoring\n",
    "def monitor_system():\n",
    "    # Monitor CPU, RAM, disk\n",
    "    pass\n",
    "\n",
    "monitor_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_system():\n",
    "    print('='*50)\n",
    "    print('System Resources')\n",
    "    print('='*50)\n",
    "    \n",
    "    # CPU\n",
    "    print(f'\\nCPU Usage: {psutil.cpu_percent(interval=1)}%')\n",
    "    print(f'CPU Cores: {psutil.cpu_count()}')\n",
    "    \n",
    "    # Memory\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f'\\nTotal RAM: {mem.total/1024**3:.1f} GB')\n",
    "    print(f'Used RAM: {mem.used/1024**3:.1f} GB ({mem.percent}%)')\n",
    "    print(f'Available RAM: {mem.available/1024**3:.1f} GB')\n",
    "    \n",
    "    # Disk\n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f'\\nTotal Disk: {disk.total/1024**3:.1f} GB')\n",
    "    print(f'Used Disk: {disk.used/1024**3:.1f} GB ({disk.percent}%)')\n",
    "    print(f'Free Disk: {disk.free/1024**3:.1f} GB')\n",
    "\n",
    "monitor_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Retry Logic with Exponential Backoff\n",
    "\n",
    "**Purpose**: Build robust error handling for unreliable operations (network, APIs, remote storage).\n",
    "\n",
    "### The Problem: Transient Failures\n",
    "\n",
    "In production ML systems, failures happen:\n",
    "- **Network timeouts**: Downloading models from Hugging Face\n",
    "- **API rate limits**: Calling external APIs (OpenAI, cloud services)\n",
    "- **Temporary overloads**: Remote storage systems under stress\n",
    "- **Cloud instability**: Brief service interruptions\n",
    "\n",
    "Without retry logic, a single transient error crashes your entire training job.\n",
    "\n",
    "### Naive Retry (DON'T DO THIS)\n",
    "\n",
    "```python\n",
    "for i in range(5):\n",
    "    try:\n",
    "        return download_model()\n",
    "    except:\n",
    "        time.sleep(1)  # Wait same time every retry\n",
    "```\n",
    "\n",
    "**Problems**:\n",
    "- Fixed delay doesn't adapt to problem severity\n",
    "- Can overwhelm already-struggling services\n",
    "- No jitter (synchronized retries from multiple clients)\n",
    "\n",
    "### Exponential Backoff: The Right Way\n",
    "\n",
    "**Pattern**: Wait longer after each failure\n",
    "\n",
    "```\n",
    "Attempt 1: Wait 1 second\n",
    "Attempt 2: Wait 2 seconds\n",
    "Attempt 3: Wait 4 seconds\n",
    "Attempt 4: Wait 8 seconds\n",
    "Attempt 5: Wait 16 seconds\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "1. **Self-healing**: Gives services time to recover\n",
    "2. **Adaptive**: Longer waits for persistent problems\n",
    "3. **Prevents cascade failures**: Doesn't overwhelm struggling services\n",
    "\n",
    "### Adding Jitter\n",
    "\n",
    "Jitter = small random delay added to prevent synchronized retries:\n",
    "\n",
    "```python\n",
    "delay = base_delay * (2 ** attempt)\n",
    "jitter = random.uniform(0, delay * 0.1)  # 10% jitter\n",
    "actual_delay = delay + jitter\n",
    "```\n",
    "\n",
    "**Why jitter matters**: If 1000 clients all retry at exact same time, they create a thundering herd.\n",
    "\n",
    "### Real-World Examples\n",
    "\n",
    "- **AWS APIs**: Use exponential backoff (official recommendation)\n",
    "- **Google Cloud**: Built-in retry with backoff\n",
    "- **Hugging Face Hub**: Retries downloads automatically\n",
    "- **HTTP libraries**: requests, httpx support retry strategies\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement a decorator that:\n",
    "1. Catches exceptions from wrapped function\n",
    "2. Retries with exponential backoff\n",
    "3. Adds random jitter\n",
    "4. Gives up after max_retries\n",
    "\n",
    "**Parameters to implement**:\n",
    "- `max_retries`: Maximum number of attempts\n",
    "- `base_delay`: Initial wait time (e.g., 1 second)\n",
    "- `exponential_base`: Multiplier for each retry (typically 2)\n",
    "\n",
    "**Test it** with `unreliable_function()` that fails randomly 70% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement retry decorator\n",
    "def retry_with_backoff(max_retries=3, base_delay=1):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Your code here\n",
    "            pass\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@retry_with_backoff(max_retries=5)\n",
    "def unreliable_function():\n",
    "    if random.random() < 0.7:\n",
    "        raise ConnectionError('Random failure')\n",
    "    return 'Success\\!'\n",
    "\n",
    "try:\n",
    "    result = unreliable_function()\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f'Failed: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_with_backoff(max_retries=3, base_delay=1, exp_base=2):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        raise\n",
    "                    delay = min(base_delay * (exp_base ** attempt), 60)\n",
    "                    jitter = random.uniform(0, delay * 0.1)\n",
    "                    sleep_time = delay + jitter\n",
    "                    print(f'Attempt {attempt+1} failed: {e}')\n",
    "                    print(f'Retrying in {sleep_time:.2f}s...')\n",
    "                    time.sleep(sleep_time)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@retry_with_backoff(max_retries=5, base_delay=0.5)\n",
    "def unreliable_function():\n",
    "    if random.random() < 0.7:\n",
    "        raise ConnectionError('Random failure')\n",
    "    return 'Success\\!'\n",
    "\n",
    "try:\n",
    "    result = unreliable_function()\n",
    "    print(f'\\nâœ… {result}')\n",
    "except Exception as e:\n",
    "    print(f'\\nâŒ Failed: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Performance Profiling with PyTorch Profiler\n",
    "\n",
    "**Purpose**: Use PyTorch Profiler to identify performance bottlenecks and optimize your code.\n",
    "\n",
    "### The Performance Mystery\n",
    "\n",
    "Your training is slow. But why?\n",
    "- Is it the model forward pass?\n",
    "- Is it the backward pass (gradients)?\n",
    "- Is it the optimizer step?\n",
    "- Is it data transfer between CPU and GPU?\n",
    "- Is it a specific layer (attention, convolution)?\n",
    "\n",
    "**Guessing is wasteful**. Profiling gives you data.\n",
    "\n",
    "### What PyTorch Profiler Measures\n",
    "\n",
    "**CPU Time**:\n",
    "- Python overhead\n",
    "- Data preprocessing\n",
    "- Kernel launches\n",
    "- Memory allocations\n",
    "\n",
    "**CUDA Time** (GPU):\n",
    "- Actual GPU kernel execution\n",
    "- Memory transfers (CPU â†” GPU)\n",
    "- Synchronization overhead\n",
    "\n",
    "**Memory**:\n",
    "- Allocations and deallocations\n",
    "- Peak memory usage\n",
    "- Memory bandwidth\n",
    "\n",
    "### Reading Profiler Output\n",
    "\n",
    "The profiler shows a table like:\n",
    "\n",
    "```\n",
    "Name                    CPU Time    CUDA Time    Calls\n",
    "aten::linear            10.5ms      50.2ms       100\n",
    "aten::relu              2.1ms       5.3ms        50\n",
    "aten::backward          5.2ms       25.1ms       50\n",
    "```\n",
    "\n",
    "**How to interpret**:\n",
    "- **High CPU time**: Python overhead, data loading\n",
    "- **High CUDA time**: GPU computation bottleneck\n",
    "- **Many calls**: Might benefit from fusion\n",
    "- **CPU > CUDA**: GPU is idle, CPU is bottleneck\n",
    "\n",
    "### Common Bottlenecks and Solutions\n",
    "\n",
    "**1. Data Loading Bottleneck**\n",
    "- Symptom: GPU utilization < 50%, high CPU time\n",
    "- Solution: More DataLoader workers, prefetch, faster disk\n",
    "\n",
    "**2. Small Batch Size**\n",
    "- Symptom: Low GPU utilization, many kernel launches\n",
    "- Solution: Increase batch size, gradient accumulation\n",
    "\n",
    "**3. CPU-GPU Transfers**\n",
    "- Symptom: High time in memory copy operations\n",
    "- Solution: Pin memory, avoid unnecessary .cpu()/.cuda() calls\n",
    "\n",
    "**4. Slow Operations**\n",
    "- Symptom: One operation dominates CUDA time\n",
    "- Solution: Optimize that operation, use fused kernels, consider alternatives\n",
    "\n",
    "### Profiling Best Practices\n",
    "\n",
    "1. **Warm up**: Run a few iterations before profiling (CUDA initialization overhead)\n",
    "2. **Profile production workload**: Use real batch sizes and model architecture\n",
    "3. **Focus on hot spots**: Optimize the top 3-5 operations first (80/20 rule)\n",
    "4. **Profile iteratively**: Profile â†’ Optimize â†’ Profile again\n",
    "\n",
    "### Your Task\n",
    "\n",
    "This exercise profiles a simple neural network:\n",
    "1. Creates a 3-layer model\n",
    "2. Runs 10 forward and backward passes\n",
    "3. Collects CPU and CUDA timing\n",
    "4. Displays the top 10 operations\n",
    "\n",
    "**What to look for**:\n",
    "- Which operations take the most time?\n",
    "- Is time spent on forward pass or backward pass?\n",
    "- Are there unexpected slow operations?\n",
    "\n",
    "**Extension ideas**:\n",
    "- Export trace to Chrome: `prof.export_chrome_trace('trace.json')`\n",
    "- View in chrome://tracing for visual timeline\n",
    "- Compare with different batch sizes\n",
    "- Profile your own models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.profiler as profiler\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1000, 500),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(500, 10)\n",
    ").to(device)\n",
    "\n",
    "data = torch.randn(32, 1000).to(device)\n",
    "\n",
    "activities = [profiler.ProfilerActivity.CPU]\n",
    "if torch.cuda.is_available():\n",
    "    activities.append(profiler.ProfilerActivity.CUDA)\n",
    "\n",
    "with profiler.profile(activities=activities, record_shapes=True) as prof:\n",
    "    for _ in range(10):\n",
    "        output = model(data)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "\n",
    "print('\\nTop 10 operations by CPU time:')\n",
    "print(prof.key_averages().table(sort_by='cpu_time_total', row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Real-Time Monitoring Dashboard\n",
    "\n",
    "**Purpose**: Visualize resource usage and training metrics in real-time to understand system behavior.\n",
    "\n",
    "### Why Visualize During Training?\n",
    "\n",
    "Numbers in logs are hard to interpret:\n",
    "```\n",
    "Step 100: Loss=2.3, GPU=4521MB, CPU=45%\n",
    "Step 200: Loss=2.1, GPU=4523MB, CPU=47%\n",
    "Step 300: Loss=1.9, GPU=4890MB, CPU=44%\n",
    "```\n",
    "\n",
    "**Questions you can't easily answer**:\n",
    "- Is memory growing steadily (leak) or stable?\n",
    "- Are there sudden spikes?\n",
    "- Is loss decreasing smoothly or jumping around?\n",
    "- What's the relationship between resource usage and training progress?\n",
    "\n",
    "**Visualization makes patterns obvious**.\n",
    "\n",
    "### What to Monitor During Training\n",
    "\n",
    "**GPU Memory**:\n",
    "- Should stabilize after a few steps\n",
    "- Steady growth = memory leak\n",
    "- Spikes = large batches or special operations\n",
    "- Plateaus = OOM about to happen\n",
    "\n",
    "**CPU Usage**:\n",
    "- High during data loading\n",
    "- Should correlate with training steps\n",
    "- Sustained 100% = bottleneck\n",
    "- Near 0% = GPU training, no data loading\n",
    "\n",
    "**Training Loss**:\n",
    "- Should decrease over time\n",
    "- Smooth curve = good learning rate\n",
    "- Spiky = learning rate too high or batch size too small\n",
    "- Flat = learning stopped (bad)\n",
    "\n",
    "### Real-World Monitoring Tools\n",
    "\n",
    "In production, use specialized tools:\n",
    "\n",
    "**Tensorboard**:\n",
    "```python\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "writer.add_scalar('Loss/train', loss, step)\n",
    "writer.add_scalar('GPU/memory', gpu_mem, step)\n",
    "```\n",
    "\n",
    "**Weights & Biases (wandb)**:\n",
    "```python\n",
    "import wandb\n",
    "wandb.init(project='my-project')\n",
    "wandb.log({'loss': loss, 'gpu_memory': gpu_mem})\n",
    "```\n",
    "\n",
    "**MLflow**:\n",
    "```python\n",
    "import mlflow\n",
    "mlflow.log_metric('loss', loss, step=step)\n",
    "```\n",
    "\n",
    "### Interpreting the Plots\n",
    "\n",
    "**Healthy Training**:\n",
    "- GPU memory: Flat after warmup\n",
    "- CPU usage: Moderate, consistent\n",
    "- Loss: Smooth decrease\n",
    "\n",
    "**Memory Leak**:\n",
    "- GPU memory: Linear increase\n",
    "- Eventually: OOM crash\n",
    "\n",
    "**CPU Bottleneck**:\n",
    "- CPU usage: 100%\n",
    "- GPU memory: Stable but low\n",
    "- Training: Slower than expected\n",
    "\n",
    "**Learning Issues**:\n",
    "- Loss: Not decreasing\n",
    "- Loss: Exploding (NaN)\n",
    "- Loss: Very spiky\n",
    "\n",
    "### Your Task\n",
    "\n",
    "This exercise:\n",
    "1. Trains a simple model for 30 steps\n",
    "2. Collects GPU memory, CPU usage, and loss at each step\n",
    "3. Creates three side-by-side plots\n",
    "4. Uses matplotlib for visualization\n",
    "\n",
    "**What to observe**:\n",
    "- GPU memory should stabilize quickly\n",
    "- CPU usage will vary (data loading + training)\n",
    "- Loss should decrease (though it's random data, so might not)\n",
    "\n",
    "**Try experimenting**:\n",
    "- Change `num_steps` to see longer trends\n",
    "- Introduce a memory leak (remove `.item()`) and watch GPU plot\n",
    "- Use larger batch size and see GPU memory increase\n",
    "- Add more DataLoader workers and watch CPU usage\n",
    "\n",
    "### Production Tip\n",
    "\n",
    "For long training runs:\n",
    "- Log metrics to files (CSV, JSON)\n",
    "- Use proper logging frameworks (TensorBoard, wandb)\n",
    "- Set up alerts for anomalies (sudden memory spike, loss explosion)\n",
    "- Monitor remotely (web dashboards)\n",
    "- Save checkpoints regularly based on metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_training(num_steps=30):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(1000, 500),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(500, 10)\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    gpu_mem, cpu_usage, losses = [], [], []\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        data = torch.randn(32, 1000).to(device)\n",
    "        target = torch.randint(0, 10, (32,)).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem.append(torch.cuda.memory_allocated() / 1024**2)\n",
    "        cpu_usage.append(psutil.cpu_percent())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    if gpu_mem:\n",
    "        axes[0].plot(gpu_mem, 'b-')\n",
    "        axes[0].set_title('GPU Memory (MB)')\n",
    "        axes[0].set_xlabel('Step')\n",
    "        axes[0].grid(True)\n",
    "    \n",
    "    axes[1].plot(cpu_usage, 'g-')\n",
    "    axes[1].set_title('CPU Usage (%)')\n",
    "    axes[1].set_xlabel('Step')\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    axes[2].plot(losses, 'r-')\n",
    "    axes[2].set_title('Training Loss')\n",
    "    axes[2].set_xlabel('Step')\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "monitor_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Practiced:\n",
    "\n",
    "1. âœ… **GPU Monitoring**: Track memory and utilization\n",
    "2. âœ… **Memory Leaks**: Detect and fix PyTorch memory issues\n",
    "3. âœ… **System Resources**: Monitor CPU, RAM, disk\n",
    "4. âœ… **Retry Logic**: Implement exponential backoff\n",
    "5. âœ… **Profiling**: Identify performance bottlenecks\n",
    "6. âœ… **Visualization**: Real-time monitoring dashboard\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- Use `torch.cuda.memory_allocated()` to track GPU memory\n",
    "- Always use `.item()` to detach scalars from computation graph\n",
    "- `psutil` provides cross-platform system monitoring\n",
    "- Exponential backoff prevents overwhelming failing services\n",
    "- PyTorch Profiler reveals performance bottlenecks\n",
    "- Visualize metrics to understand system behavior\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Apply these techniques to your own projects\n",
    "2. Set up monitoring in production environments\n",
    "3. Create alerts for resource thresholds\n",
    "4. Review [01_resource_monitoring.md](./01_resource_monitoring.md) for more details\n",
    "5. Continue to [02_dockerization_practice.ipynb](./02_dockerization_practice.ipynb)\n",
    "\n",
    "### Useful Functions:\n",
    "\n",
    "```python\n",
    "# GPU monitoring\n",
    "torch.cuda.memory_allocated()\n",
    "torch.cuda.memory_reserved()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# System monitoring\n",
    "psutil.cpu_percent()\n",
    "psutil.virtual_memory()\n",
    "psutil.disk_usage('/')\n",
    "\n",
    "# Profiling\n",
    "torch.profiler.profile()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
