{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dockerization Practice Notebook\n",
    "\n",
    "**Prerequisites**: Docker installed, Python 3.10+, PyTorch 2.6.0+, CUDA 12.4+\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Write Dockerfiles**: Create container images for AI/ML projects from scratch\n",
    "2. **GPU Support**: Configure CUDA and NVIDIA runtime for deep learning\n",
    "3. **Multi-Service Apps**: Orchestrate training, inference, and development environments\n",
    "4. **Optimization**: Reduce image sizes with multi-stage builds\n",
    "5. **Best Practices**: Security, caching, and production-ready configurations\n",
    "\n",
    "## Why Docker for AI/ML?\n",
    "\n",
    "**The Dependency Hell Problem**:\n",
    "- Different projects need different Python versions\n",
    "- PyTorch 1.x vs 2.x incompatibilities\n",
    "- CUDA version mismatches\n",
    "- System library conflicts\n",
    "- \"Works on my machine\" syndrome\n",
    "\n",
    "**Docker Solves This**:\n",
    "- **Reproducibility**: Same environment everywhere (dev, staging, prod)\n",
    "- **Isolation**: Each project has its own dependencies\n",
    "- **Portability**: Build once, run anywhere (laptop, cloud, HPC)\n",
    "- **Version Control**: Dockerfile tracks environment changes with code\n",
    "- **Scalability**: Easy to deploy thousands of containers\n",
    "\n",
    "**Real-World Impact**:\n",
    "- Companies save weeks of environment setup time\n",
    "- Research reproducibility improves dramatically\n",
    "- CI/CD pipelines become reliable\n",
    "- Cloud deployments are simplified\n",
    "\n",
    "## Docker vs Virtual Machines\n",
    "\n",
    "| Feature | Docker Container | Virtual Machine |\n",
    "|---------|-----------------|------------------|\n",
    "| Size | MBs | GBs |\n",
    "| Startup | Seconds | Minutes |\n",
    "| Performance | Near-native | Overhead |\n",
    "| Isolation | Process-level | Hardware-level |\n",
    "\n",
    "**For AI/ML**: Docker is faster, lighter, and sufficient for most use cases.\n",
    "\n",
    "## How This Notebook Works\n",
    "\n",
    "Most cells use **`%%writefile`** magic command:\n",
    "- Creates files directly on your disk\n",
    "- You can then build Docker images from these files\n",
    "- Run actual containers to test your work\n",
    "\n",
    "**Note**: Docker commands should be run in your **terminal**, not in this notebook.\n",
    "\n",
    "ðŸ’¡ **Tip**: Try completing TODO sections before viewing solutions\\!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Create a Basic Dockerfile\n",
    "\n",
    "**Purpose**: Learn the fundamental building blocks of Dockerfiles for PyTorch applications.\n",
    "\n",
    "### Understanding Dockerfiles\n",
    "\n",
    "A Dockerfile is a **recipe** for building a Docker image:\n",
    "- Written as a series of instructions (commands)\n",
    "- Each instruction creates a new layer\n",
    "- Layers are cached for faster rebuilds\n",
    "- Final image is a stack of all layers\n",
    "\n",
    "Think of it like a script that:\n",
    "1. Starts with a base operating system\n",
    "2. Installs software\n",
    "3. Copies your code\n",
    "4. Configures how to run it\n",
    "\n",
    "### Key Dockerfile Instructions\n",
    "\n",
    "**FROM**: Choose base image\n",
    "```dockerfile\n",
    "FROM python:3.10-slim  # Official Python image, minimal size\n",
    "```\n",
    "- Always the first instruction\n",
    "- Use official images when possible (security, maintenance)\n",
    "- `-slim` variant: Smaller size, fewer packages\n",
    "- `-alpine`: Even smaller, but compatibility issues\n",
    "\n",
    "**WORKDIR**: Set working directory\n",
    "```dockerfile\n",
    "WORKDIR /app  # All subsequent commands run here\n",
    "```\n",
    "- Like `cd /app` but also creates directory\n",
    "- Keeps image organized\n",
    "- Relative paths in COPY/ADD are relative to WORKDIR\n",
    "\n",
    "**RUN**: Execute commands during build\n",
    "```dockerfile\n",
    "RUN pip install torch  # Runs at build time\n",
    "```\n",
    "- Installs packages, downloads files, compiles code\n",
    "- Each RUN creates a new layer (affects image size)\n",
    "- Combine multiple commands with && to reduce layers\n",
    "\n",
    "**COPY**: Copy files from host to image\n",
    "```dockerfile\n",
    "COPY app.py .  # Copies app.py to /app/app.py\n",
    "```\n",
    "- Copies from build context (usually current directory)\n",
    "- Use .dockerignore to exclude files\n",
    "- Copying source code last improves cache efficiency\n",
    "\n",
    "**CMD**: Default command when container starts\n",
    "```dockerfile\n",
    "CMD [\"python\", \"app.py\"]  # Runs when container starts\n",
    "```\n",
    "- Only one CMD per Dockerfile (last one wins)\n",
    "- Can be overridden: `docker run myimage python other.py`\n",
    "- Use JSON array format for better signal handling\n",
    "\n",
    "### Why Use python:3.10-slim?\n",
    "\n",
    "| Image | Size | Use Case |\n",
    "|-------|------|----------|\n",
    "| python:3.10 | ~900MB | Full development, all tools |\n",
    "| python:3.10-slim | ~120MB | Production, minimal overhead |\n",
    "| python:3.10-alpine | ~50MB | Ultra-minimal, may have compatibility issues |\n",
    "\n",
    "**For ML**: slim is the sweet spot - small but compatible.\n",
    "\n",
    "### Layer Caching Strategy\n",
    "\n",
    "Docker caches layers. Order matters for efficiency:\n",
    "\n",
    "**Bad (slow rebuilds)**:\n",
    "```dockerfile\n",
    "COPY . .              # Changes frequently\n",
    "RUN pip install torch # Runs every code change\n",
    "```\n",
    "\n",
    "**Good (fast rebuilds)**:\n",
    "```dockerfile\n",
    "RUN pip install torch # Cached unless dependencies change\n",
    "COPY . .              # Only this layer rebuilds on code changes\n",
    "```\n",
    "\n",
    "**Best practice**: Copy requirements.txt first, install, then copy code.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Create a Dockerfile that:\n",
    "1. Uses `python:3.10-slim` as base (lightweight)\n",
    "2. Sets `/app` as working directory (organization)\n",
    "3. Installs `torch>=2.6.0` (deep learning framework)\n",
    "4. Copies `app.py` (your application)\n",
    "5. Runs `app.py` by default (when container starts)\n",
    "\n",
    "**Success criteria**: A buildable, minimal PyTorch container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this Dockerfile\n",
    "# This cell uses %%writefile magic command to create a file\n",
    "# Try to complete it before looking at the solution\n",
    "\n",
    "%%writefile Dockerfile.basic\n",
    "# TODO: Create a Dockerfile that:\n",
    "# 1. Uses python:3.10-slim as base image\n",
    "# 2. Sets /app as working directory\n",
    "# 3. Installs torch>=2.6.0\n",
    "# 4. Copies app.py to container\n",
    "# 5. Runs app.py as default command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile.basic\n",
    "# Base image: Using slim variant to reduce image size\n",
    "FROM python:3.10-slim\n",
    "\n",
    "# Set working directory inside container\n",
    "WORKDIR /app\n",
    "\n",
    "# Install PyTorch (--no-cache-dir reduces image size)\n",
    "RUN pip install --no-cache-dir torch>=2.6.0\n",
    "\n",
    "# Copy application code from host to container\n",
    "COPY app.py .\n",
    "\n",
    "# Default command when container starts\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Create a Sample Application\n",
    "\n",
    "**Purpose**: Build a diagnostic application to verify Docker environment setup.\n",
    "\n",
    "### Why Create a Test Application?\n",
    "\n",
    "Before deploying production models, verify:\n",
    "- Python version is correct\n",
    "- PyTorch installed successfully\n",
    "- CUDA is accessible (for GPU containers)\n",
    "- Basic tensor operations work\n",
    "\n",
    "**In production**: Similar health check scripts validate deployments.\n",
    "\n",
    "### What This Application Does\n",
    "\n",
    "**1. Environment Information**:\n",
    "```python\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "```\n",
    "- Confirms correct versions are installed\n",
    "- Useful for debugging version mismatches\n",
    "\n",
    "**2. CUDA Detection**:\n",
    "```python\n",
    "torch.cuda.is_available()\n",
    "```\n",
    "- Returns True if GPU is accessible\n",
    "- False might mean: no GPU, wrong driver, wrong PyTorch build\n",
    "\n",
    "**3. GPU Information** (if available):\n",
    "```python\n",
    "torch.cuda.get_device_name(0)\n",
    "```\n",
    "- Shows GPU model (e.g., \"NVIDIA GeForce RTX 3090\")\n",
    "- Helps verify correct GPU is being used\n",
    "\n",
    "**4. Computation Test**:\n",
    "```python\n",
    "x = torch.randn(1000, 1000).cuda()\n",
    "y = torch.randn(1000, 1000).cuda()\n",
    "z = torch.matmul(x, y)\n",
    "```\n",
    "- Matrix multiplication is a common ML operation\n",
    "- Tests both memory allocation and computation\n",
    "- Failure here means something is seriously wrong\n",
    "\n",
    "### CPU vs GPU Code Paths\n",
    "\n",
    "Notice the conditional logic:\n",
    "\n",
    "```python\n",
    "if torch.cuda.is_available():\n",
    "    x = torch.randn(1000, 1000).cuda()  # GPU\n",
    "else:\n",
    "    x = torch.randn(1000, 1000)  # CPU\n",
    "```\n",
    "\n",
    "**Why**:\n",
    "- CPU-only containers (for testing/development)\n",
    "- GPU containers (for training/inference)\n",
    "- Same code works in both environments\n",
    "\n",
    "### Container Health Checks\n",
    "\n",
    "This pattern extends to production:\n",
    "\n",
    "**Dockerfile health check**:\n",
    "```dockerfile\n",
    "HEALTHCHECK --interval=30s --timeout=3s \\\n",
    "  CMD python health_check.py || exit 1\n",
    "```\n",
    "\n",
    "**Kubernetes readiness probe**:\n",
    "```yaml\n",
    "readinessProbe:\n",
    "  exec:\n",
    "    command: [\"python\", \"health_check.py\"]\n",
    "```\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "**CPU container**:\n",
    "```\n",
    "Python: 3.10.x\n",
    "PyTorch: 2.6.0\n",
    "CUDA Available: False\n",
    "Running on CPU\n",
    "Testing CPU computation...\n",
    "âœ… CPU computation successful\n",
    "```\n",
    "\n",
    "**GPU container**:\n",
    "```\n",
    "Python: 3.10.x\n",
    "PyTorch: 2.6.0\n",
    "CUDA Available: True\n",
    "CUDA Version: 12.4\n",
    "GPU: NVIDIA GeForce RTX 3090\n",
    "Testing GPU computation...\n",
    "âœ… GPU computation successful\n",
    "```\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**If CUDA shows False** (but you have a GPU):\n",
    "1. Check NVIDIA drivers: `nvidia-smi`\n",
    "2. Verify nvidia-docker installed: `docker run --gpus all nvidia/cuda:12.4.0-base nvidia-smi`\n",
    "3. Check PyTorch CUDA version matches system CUDA\n",
    "4. Use `--gpus all` flag when running container\n",
    "\n",
    "**If computation fails**:\n",
    "1. Out of memory: Reduce matrix size\n",
    "2. CUDA error: Incompatible CUDA versions\n",
    "3. Permission denied: Check Docker GPU access\n",
    "\n",
    "### Your Task\n",
    "\n",
    "This cell creates `app.py` with:\n",
    "- Version information display\n",
    "- CUDA availability check\n",
    "- Conditional GPU/CPU code paths\n",
    "- Matrix multiplication test\n",
    "- Clear success/failure messages\n",
    "\n",
    "**No TODO** - this is a complete reference implementation you'll use in later exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\"\"\"Sample PyTorch application for Docker testing.\n",
    "\n",
    "This script validates that:\n",
    "1. Python version is correct\n",
    "2. PyTorch is installed properly\n",
    "3. CUDA is available (if running with GPU)\n",
    "4. Basic tensor operations work\n",
    "\"\"\"\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"PyTorch Docker Container Test\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display environment info\n",
    "print(f\"\\nPython: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Test GPU computation\n",
    "    print(\"\\nTesting GPU computation...\")\n",
    "    x = torch.randn(1000, 1000).cuda()\n",
    "    y = torch.randn(1000, 1000).cuda()\n",
    "    z = torch.matmul(x, y)\n",
    "    print(\"âœ… GPU computation successful\")\n",
    "else:\n",
    "    print(\"\\nRunning on CPU\")\n",
    "    print(\"Testing CPU computation...\")\n",
    "    x = torch.randn(1000, 1000)\n",
    "    y = torch.randn(1000, 1000)\n",
    "    z = torch.matmul(x, y)\n",
    "    print(\"âœ… CPU computation successful\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"All tests passed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Build and Run Docker Image\n",
    "\n",
    "**Purpose**: Learn Docker CLI commands to build images and run containers.\n",
    "\n",
    "### The Docker Build Process\n",
    "\n",
    "**What happens when you build**:\n",
    "1. Docker reads the Dockerfile\n",
    "2. Executes each instruction in order\n",
    "3. Creates a layer for each instruction\n",
    "4. Caches layers for faster rebuilds\n",
    "5. Tags the final image with a name\n",
    "\n",
    "**Build time** depends on:\n",
    "- Base image size (downloading if not cached)\n",
    "- Number of packages to install\n",
    "- Network speed (downloading dependencies)\n",
    "- Layer caching (faster if nothing changed)\n",
    "\n",
    "First build: 2-10 minutes. Cached rebuilds: seconds.\n",
    "\n",
    "### Understanding `docker build` Command\n",
    "\n",
    "```bash\n",
    "docker build -t pytorch-basic -f Dockerfile.basic .\n",
    "```\n",
    "\n",
    "**Breaking it down**:\n",
    "\n",
    "**`docker build`**: The build command\n",
    "\n",
    "**`-t pytorch-basic`**: Tag (name) the image\n",
    "- Format: `name:tag` (e.g., `pytorch-basic:v1.0`)\n",
    "- No tag specified = `latest` by default\n",
    "- Use descriptive names: `my-model-trainer`, `api-server`\n",
    "\n",
    "**`-f Dockerfile.basic`**: Specify which Dockerfile\n",
    "- Default: looks for file named `Dockerfile`\n",
    "- Use -f when you have multiple Dockerfiles\n",
    "- Examples: Dockerfile.dev, Dockerfile.prod, Dockerfile.gpu\n",
    "\n",
    "**`.`**: Build context (current directory)\n",
    "- Docker sends all files in this directory to build daemon\n",
    "- COPY commands copy from this context\n",
    "- Use .dockerignore to exclude large/unnecessary files\n",
    "- Can specify different path: `docker build -t myapp /path/to/context`\n",
    "\n",
    "### Build Output Explained\n",
    "\n",
    "You'll see:\n",
    "```\n",
    "Sending build context to Docker daemon  2.048kB\n",
    "Step 1/5 : FROM python:3.10-slim\n",
    " ---> abc123def456\n",
    "Step 2/5 : WORKDIR /app\n",
    " ---> Running in xyz789...\n",
    " ---> def456ghi789\n",
    "...\n",
    "Successfully built abc123def456\n",
    "Successfully tagged pytorch-basic:latest\n",
    "```\n",
    "\n",
    "**What each part means**:\n",
    "- **Sending context**: Uploading files to Docker\n",
    "- **Step X/Y**: Which Dockerfile instruction is executing\n",
    "- **Running in...**: Temporary container created for this step\n",
    "- **Successfully built**: Image ID (hash)\n",
    "- **Successfully tagged**: Your friendly name\n",
    "\n",
    "### Understanding `docker run` Command\n",
    "\n",
    "```bash\n",
    "docker run --rm pytorch-basic\n",
    "```\n",
    "\n",
    "**Breaking it down**:\n",
    "\n",
    "**`docker run`**: Create and start a container\n",
    "\n",
    "**`--rm`**: Remove container after it exits\n",
    "- Without this, stopped containers accumulate\n",
    "- Good for one-off tasks and testing\n",
    "- Use `docker ps -a` to see all containers\n",
    "- Clean up: `docker container prune`\n",
    "\n",
    "**`pytorch-basic`**: Image to run\n",
    "- Uses local image if available\n",
    "- Downloads from Docker Hub if not found\n",
    "- Can specify version: `pytorch-basic:v1.0`\n",
    "\n",
    "### Common `docker run` Options\n",
    "\n",
    "**Interactive mode**:\n",
    "```bash\n",
    "docker run -it --rm pytorch-basic bash\n",
    "```\n",
    "- `-it`: Interactive terminal\n",
    "- `bash`: Override CMD, run bash shell instead\n",
    "- Useful for debugging and exploration\n",
    "\n",
    "**Port mapping**:\n",
    "```bash\n",
    "docker run -p 8000:8000 --rm pytorch-basic\n",
    "```\n",
    "- `-p 8000:8000`: Map host port 8000 to container port 8000\n",
    "- Format: `-p HOST:CONTAINER`\n",
    "- Needed for web servers, APIs, Jupyter\n",
    "\n",
    "**Volume mounting**:\n",
    "```bash\n",
    "docker run -v $(pwd)/data:/app/data --rm pytorch-basic\n",
    "```\n",
    "- `-v HOST:CONTAINER`: Mount directory\n",
    "- Changes in container persist on host\n",
    "- Useful for data, models, configs\n",
    "\n",
    "**Environment variables**:\n",
    "```bash\n",
    "docker run -e MODEL_NAME=gpt2 --rm pytorch-basic\n",
    "```\n",
    "- `-e KEY=VALUE`: Set environment variable\n",
    "- Access in code: `os.getenv('MODEL_NAME')`\n",
    "\n",
    "### Verification Commands\n",
    "\n",
    "After building, verify your image:\n",
    "\n",
    "**List images**:\n",
    "```bash\n",
    "docker images\n",
    "docker images | grep pytorch-basic\n",
    "```\n",
    "Shows: name, tag, image ID, creation date, size\n",
    "\n",
    "**Inspect image**:\n",
    "```bash\n",
    "docker inspect pytorch-basic\n",
    "```\n",
    "Shows: layers, environment variables, CMD, ENTRYPOINT, etc.\n",
    "\n",
    "**Check image size**:\n",
    "```bash\n",
    "docker images pytorch-basic --format '{{.Size}}'\n",
    "```\n",
    "Important: Smaller images = faster deployment, lower costs\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Run these commands in your **terminal** (not Jupyter):\n",
    "\n",
    "1. Navigate to directory with Dockerfile and app.py\n",
    "2. Build the image (will take 2-5 minutes first time)\n",
    "3. Verify image was created\n",
    "4. Run a container from the image\n",
    "5. Observe the output from app.py\n",
    "\n",
    "**Success**: You see Python/PyTorch versions and successful computation message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Run these commands in your TERMINAL, not in this notebook\n",
    "# The ! prefix would execute them in Jupyter, but Docker commands work better in terminal\n",
    "\n",
    "# Step 1: Build the Docker image\n",
    "# docker build -t pytorch-basic -f Dockerfile.basic .\n",
    "\n",
    "# Step 2: Run the container\n",
    "# docker run --rm pytorch-basic\n",
    "\n",
    "# Step 3: List your images\n",
    "# docker images | grep pytorch-basic\n",
    "\n",
    "print(\"\"\"\\nTo build and run:\\n\n",
    "Terminal Commands:\n",
    "==================\n",
    "cd /home/wsl2ubt2204/Starter_tutorial/Chapters/5\n",
    "docker build -t pytorch-basic -f Dockerfile.basic .\n",
    "docker run --rm pytorch-basic\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Create GPU-Enabled Dockerfile\n",
    "\n",
    "**Purpose**: Configure Docker containers for GPU-accelerated deep learning.\n",
    "\n",
    "### Why GPU Containers Are Different\n",
    "\n",
    "GPUs require special setup:\n",
    "- **CUDA runtime** libraries (not just drivers)\n",
    "- **cuDNN** for optimized neural network operations\n",
    "- **NVIDIA Container Toolkit** on host system\n",
    "- **Matching versions** (CUDA, PyTorch, drivers)\n",
    "\n",
    "**The challenge**: Getting all versions aligned correctly.\n",
    "\n",
    "### Prerequisites for GPU Containers\n",
    "\n",
    "**On the host system (your machine)**:\n",
    "\n",
    "1. **NVIDIA GPU**: Obviously\\!\n",
    "2. **NVIDIA Driver**: Version 535+ for CUDA 12.x\n",
    "   - Check: `nvidia-smi`\n",
    "3. **nvidia-docker2**: Docker GPU support\n",
    "   - Install: See 02_dockerization.md\n",
    "4. **Docker configured**: Runtime set to nvidia\n",
    "\n",
    "**Common issue**: CPU Dockerfile works but GPU version fails due to missing prerequisites.\n",
    "\n",
    "### NVIDIA CUDA Base Images\n",
    "\n",
    "NVIDIA provides official images with CUDA pre-installed:\n",
    "\n",
    "**Image options**:\n",
    "```dockerfile\n",
    "nvidia/cuda:12.4.0-base-ubuntu22.04     # Minimal CUDA\n",
    "nvidia/cuda:12.4.0-runtime-ubuntu22.04  # + libraries\n",
    "nvidia/cuda:12.4.0-devel-ubuntu22.04    # + dev tools\n",
    "```\n",
    "\n",
    "**Which to use**:\n",
    "- **base**: Just CUDA, smallest\n",
    "- **runtime**: For inference, includes cuDNN â† **Use this**\n",
    "- **devel**: For compiling CUDA code, largest\n",
    "\n",
    "**Size comparison**:\n",
    "- base: ~200MB\n",
    "- runtime: ~1.5GB\n",
    "- devel: ~3GB\n",
    "\n",
    "### Installing Python on CUDA Images\n",
    "\n",
    "CUDA images are based on Ubuntu, not Python:\n",
    "\n",
    "```dockerfile\n",
    "# This is Ubuntu, not Python\\!\n",
    "FROM nvidia/cuda:12.4.0-runtime-ubuntu22.04\n",
    "\n",
    "# Must install Python ourselves\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    python3.10 \\\n",
    "    python3-pip\n",
    "```\n",
    "\n",
    "**Why not FROM python:3.10 with CUDA**?\n",
    "- Python images don't include CUDA\n",
    "- Installing CUDA manually is complex and error-prone\n",
    "- NVIDIA images are tested and optimized\n",
    "\n",
    "### PyTorch CUDA Version Matching\n",
    "\n",
    "**Critical**: PyTorch CUDA version must match container CUDA version\\!\n",
    "\n",
    "**Our setup**:\n",
    "- Container: CUDA 12.4\n",
    "- PyTorch: Must use cu124 build\n",
    "\n",
    "```dockerfile\n",
    "RUN pip install torch>=2.6.0 \\\n",
    "    --index-url https://download.pytorch.org/whl/cu124\n",
    "```\n",
    "\n",
    "**Index URLs**:\n",
    "- cu124: CUDA 12.4\n",
    "- cu121: CUDA 12.1\n",
    "- cu118: CUDA 11.8\n",
    "- cpu: CPU-only version\n",
    "\n",
    "**Wrong version = Runtime errors or no GPU detected\\!**\n",
    "\n",
    "### Setting Python as Default\n",
    "\n",
    "Ubuntu 22.04 has `python3` but not `python`:\n",
    "\n",
    "```dockerfile\n",
    "RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.10 1\n",
    "```\n",
    "\n",
    "**Why**:\n",
    "- Makes `python` command work (not just `python3`)\n",
    "- Avoids scripts failing with \"python: command not found\"\n",
    "- Standard practice in containerized environments\n",
    "\n",
    "### Running GPU Containers\n",
    "\n",
    "**Must use `--gpus` flag**:\n",
    "\n",
    "```bash\n",
    "docker run --gpus all --rm pytorch-gpu\n",
    "```\n",
    "\n",
    "**GPU access options**:\n",
    "```bash\n",
    "--gpus all              # All GPUs\n",
    "--gpus '\"device=0\"'     # GPU 0 only\n",
    "--gpus '\"device=0,1\"'   # GPU 0 and 1\n",
    "--gpus 2                # Any 2 GPUs\n",
    "```\n",
    "\n",
    "**Without --gpus**: Container runs but `torch.cuda.is_available()` returns False\\!\n",
    "\n",
    "### Troubleshooting GPU Access\n",
    "\n",
    "**Problem**: `torch.cuda.is_available()` returns False\n",
    "\n",
    "**Checklist**:\n",
    "1. âœ… Host GPU works: `nvidia-smi`\n",
    "2. âœ… nvidia-docker installed: `docker run --gpus all nvidia/cuda:12.4.0-base nvidia-smi`\n",
    "3. âœ… Used `--gpus all` flag when running\n",
    "4. âœ… CUDA versions match (container CUDA = PyTorch CUDA)\n",
    "5. âœ… PyTorch installed from correct index-url\n",
    "\n",
    "**Test GPU access in container**:\n",
    "```bash\n",
    "docker run --gpus all --rm pytorch-gpu nvidia-smi\n",
    "```\n",
    "Should show GPU info, not \"command not found\"\n",
    "\n",
    "### GPU Container Best Practices\n",
    "\n",
    "**1. Pin CUDA version**:\n",
    "- Use specific tag: `12.4.0-runtime` not `latest`\n",
    "- Ensures reproducibility\n",
    "\n",
    "**2. Match everything**:\n",
    "- Host CUDA/driver version\n",
    "- Container CUDA version\n",
    "- PyTorch CUDA build version\n",
    "\n",
    "**3. Set shared memory size**:\n",
    "```bash\n",
    "docker run --gpus all --shm-size=8g pytorch-gpu\n",
    "```\n",
    "- PyTorch DataLoader needs shared memory\n",
    "- Default 64MB often too small\n",
    "- Symptoms: DataLoader hangs or crashes\n",
    "\n",
    "**4. Monitor GPU in container**:\n",
    "```bash\n",
    "docker exec -it container_name nvidia-smi\n",
    "```\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Create a GPU-enabled Dockerfile that:\n",
    "1. Uses NVIDIA CUDA 12.4 runtime base\n",
    "2. Installs Python 3.10 and pip\n",
    "3. Makes python the default command\n",
    "4. Installs PyTorch with matching CUDA version\n",
    "5. Copies and runs app.py\n",
    "\n",
    "**Success criteria**: `torch.cuda.is_available()` returns True and GPU computation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this GPU-enabled Dockerfile\n",
    "%%writefile Dockerfile.gpu\n",
    "# TODO: Create a GPU-enabled Dockerfile\n",
    "# 1. Use nvidia/cuda:12.4.0-runtime-ubuntu22.04 as base\n",
    "# 2. Install Python 3.10 and pip\n",
    "# 3. Install PyTorch with CUDA 12.4 support\n",
    "# 4. Copy and run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile.gpu\n",
    "# Base: NVIDIA CUDA 12.4 runtime on Ubuntu 22.04\n",
    "# This includes CUDA libraries needed for GPU operations\n",
    "FROM nvidia/cuda:12.4.0-runtime-ubuntu22.04\n",
    "\n",
    "# Install Python 3.10 from Ubuntu repositories\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    python3.10 \\\n",
    "    python3-pip \\\n",
    "    && rm -rf /var/lib/apt/lists/*  # Clean up to reduce image size\n",
    "\n",
    "# Make Python 3.10 the default 'python' command\n",
    "RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.10 1\n",
    "RUN update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install PyTorch built for CUDA 12.4\n",
    "# Using specific wheel index ensures CUDA compatibility\n",
    "RUN pip install --no-cache-dir \\\n",
    "    torch>=2.6.0 \\\n",
    "    --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "COPY app.py .\n",
    "\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and run GPU container\n",
    "# The --gpus all flag exposes all GPUs to the container\n",
    "\n",
    "print(\"\"\"\\nTo build and run with GPU:\\n\n",
    "Terminal Commands:\n",
    "==================\n",
    "docker build -t pytorch-gpu -f Dockerfile.gpu .\n",
    "docker run --gpus all --rm pytorch-gpu\n",
    "\n",
    "# To use specific GPU:\n",
    "docker run --gpus '\"device=0\"' --rm pytorch-gpu\n",
    "\n",
    "# To check GPU access without running app:\n",
    "docker run --gpus all --rm pytorch-gpu nvidia-smi\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Create Docker Compose Configuration\n",
    "\n",
    "**Purpose**: Orchestrate multiple services (training, development, monitoring) with a single command.\n",
    "\n",
    "### The Multi-Container Challenge\n",
    "\n",
    "Real ML systems need multiple services:\n",
    "- Training container (runs model training)\n",
    "- Jupyter container (for experimentation)\n",
    "- Database (stores metrics, checkpoints)\n",
    "- Monitoring (tracks system health)\n",
    "- API server (for inference)\n",
    "\n",
    "**Without Docker Compose**:\n",
    "```bash\n",
    "docker run -d --name db postgres\n",
    "docker run -d --name jupyter ...\n",
    "docker run -d --name trainer ...\n",
    "# Manual networking, volumes, env vars\n",
    "```\n",
    "Managing this manually is error-prone and tedious.\n",
    "\n",
    "**With Docker Compose**:\n",
    "```bash\n",
    "docker-compose up  # Starts everything\n",
    "```\n",
    "One command, all services configured and networked automatically.\n",
    "\n",
    "### Docker Compose Basics\n",
    "\n",
    "**docker-compose.yml**: YAML file defining your application\n",
    "\n",
    "**Structure**:\n",
    "```yaml\n",
    "version: '3.8'  # Compose file version\n",
    "\n",
    "services:       # Define containers\n",
    "  service1:\n",
    "    # configuration\n",
    "  service2:\n",
    "    # configuration\n",
    "\n",
    "volumes:        # Persistent storage\n",
    "  data:\n",
    "\n",
    "networks:       # Custom networks\n",
    "  mynetwork:\n",
    "```\n",
    "\n",
    "### Service Configuration Options\n",
    "\n",
    "**Build context**:\n",
    "```yaml\n",
    "build:\n",
    "  context: .              # Where to find Dockerfile\n",
    "  dockerfile: Dockerfile.gpu  # Which Dockerfile to use\n",
    "```\n",
    "\n",
    "**Container name**:\n",
    "```yaml\n",
    "container_name: pytorch_trainer  # Friendly name\n",
    "```\n",
    "Without this, Compose generates names like `folder_service_1`\n",
    "\n",
    "**GPU access**:\n",
    "```yaml\n",
    "runtime: nvidia           # Enable NVIDIA runtime\n",
    "environment:\n",
    "  - NVIDIA_VISIBLE_DEVICES=all  # Expose all GPUs\n",
    "```\n",
    "Equivalent to `--gpus all` in docker run\n",
    "\n",
    "**Volumes**:\n",
    "```yaml\n",
    "volumes:\n",
    "  - ./data:/app/data      # Host:Container\n",
    "  - ./models:/app/models  # Shared between services\n",
    "```\n",
    "Multiple services can share volumes for data/model exchange\n",
    "\n",
    "**Shared memory**:\n",
    "```yaml\n",
    "shm_size: '8gb'  # For PyTorch DataLoader workers\n",
    "```\n",
    "Prevents \"Bus error\" with multi-worker DataLoader\n",
    "\n",
    "**Command override**:\n",
    "```yaml\n",
    "command: python train.py  # Override Dockerfile CMD\n",
    "```\n",
    "\n",
    "### Why This Configuration?\n",
    "\n",
    "**Trainer Service**:\n",
    "- Runs training scripts\n",
    "- Has GPU access\n",
    "- Shares volumes with Jupyter\n",
    "- Can save models to shared volume\n",
    "\n",
    "**Jupyter Service**:\n",
    "- Interactive development\n",
    "- Also has GPU access\n",
    "- Access trained models via shared volume\n",
    "- Exposed on port 8888 for browser access\n",
    "- Token disabled for convenience (set in production\\!)\n",
    "\n",
    "### Multi-Line Commands\n",
    "\n",
    "YAML multi-line with `>`:\n",
    "```yaml\n",
    "command: >\n",
    "  bash -c \"pip install jupyter &&\n",
    "  jupyter lab --ip=0.0.0.0 --port=8888\"\n",
    "```\n",
    "\n",
    "**Why**:\n",
    "- Install additional packages at runtime\n",
    "- Chain multiple commands\n",
    "- More flexible than baking everything into image\n",
    "\n",
    "### Service Dependencies\n",
    "\n",
    "```yaml\n",
    "depends_on:\n",
    "  - database\n",
    "  - redis\n",
    "```\n",
    "\n",
    "**What it does**:\n",
    "- Starts dependencies first\n",
    "- **Note**: Doesn't wait for service to be ready (just started)\n",
    "\n",
    "**For ML pipelines**:\n",
    "```yaml\n",
    "trainer:\n",
    "  depends_on:\n",
    "    - mlflow  # Metrics tracking\n",
    "```\n",
    "\n",
    "### Docker Compose Commands\n",
    "\n",
    "**Start all services (detached)**:\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "- `-d`: Runs in background\n",
    "- Without `-d`: Shows logs (Ctrl+C stops all)\n",
    "\n",
    "**View logs**:\n",
    "```bash\n",
    "docker-compose logs -f trainer    # Follow trainer logs\n",
    "docker-compose logs --tail=50 jupyter  # Last 50 lines\n",
    "```\n",
    "\n",
    "**Stop services**:\n",
    "```bash\n",
    "docker-compose stop   # Stop but don't remove\n",
    "docker-compose down   # Stop and remove containers\n",
    "```\n",
    "\n",
    "**Rebuild after changes**:\n",
    "```bash\n",
    "docker-compose up -d --build  # Rebuild images\n",
    "```\n",
    "\n",
    "**Scale services**:\n",
    "```bash\n",
    "docker-compose up -d --scale trainer=3  # 3 training containers\n",
    "```\n",
    "Useful for distributed training\n",
    "\n",
    "**Execute commands in running service**:\n",
    "```bash\n",
    "docker-compose exec trainer python check_gpu.py\n",
    "docker-compose exec jupyter bash\n",
    "```\n",
    "\n",
    "### Accessing Jupyter Lab\n",
    "\n",
    "After `docker-compose up -d`:\n",
    "\n",
    "1. Open browser\n",
    "2. Go to `http://localhost:8888`\n",
    "3. Start experimenting\\!\n",
    "\n",
    "**Files in `/app/notebooks`** inside container = `./notebooks` on host\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "**Security** (for production):\n",
    "```yaml\n",
    "command: >\n",
    "  jupyter lab\n",
    "  --NotebookApp.token='your-secure-token'\n",
    "  --NotebookApp.password='hashed-password'\n",
    "```\n",
    "\n",
    "**Resource limits**:\n",
    "```yaml\n",
    "deploy:\n",
    "  resources:\n",
    "    limits:\n",
    "      cpus: '4.0'\n",
    "      memory: 16G\n",
    "```\n",
    "\n",
    "**Restart policy**:\n",
    "```yaml\n",
    "restart: unless-stopped  # Auto-restart on failure\n",
    "```\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Create a docker-compose.yml with:\n",
    "1. **Trainer service**: GPU-enabled, runs training\n",
    "2. **Jupyter service**: GPU-enabled, port 8888, interactive\n",
    "3. **Shared volumes**: For data, models, notebooks\n",
    "4. **Proper networking**: Services can communicate\n",
    "\n",
    "**Success**: Both services start, Jupyter accessible in browser, shared volumes work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-compose.yml\n",
    "# TODO: Create a docker-compose.yml with:\n",
    "# 1. Training service with GPU support\n",
    "# 2. Jupyter Lab service on port 8888\n",
    "# 3. Shared volumes for data and models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-compose.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  # Model training service\n",
    "  trainer:\n",
    "    build:\n",
    "      context: .              # Build from current directory\n",
    "      dockerfile: Dockerfile.gpu\n",
    "    container_name: pytorch_trainer\n",
    "    runtime: nvidia           # Enable NVIDIA GPU support\n",
    "    environment:\n",
    "      - NVIDIA_VISIBLE_DEVICES=all  # Expose all GPUs\n",
    "    volumes:\n",
    "      - ./data:/app/data      # Mount data directory\n",
    "      - ./models:/app/models  # Mount models directory\n",
    "    shm_size: '8gb'          # Shared memory for DataLoader workers\n",
    "    command: python train.py # Override default CMD\n",
    "\n",
    "  # Jupyter Lab service for interactive development\n",
    "  jupyter:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile.gpu\n",
    "    container_name: jupyter_lab\n",
    "    runtime: nvidia\n",
    "    environment:\n",
    "      - NVIDIA_VISIBLE_DEVICES=all\n",
    "    ports:\n",
    "      - \"8888:8888\"          # Expose Jupyter on host port 8888\n",
    "    volumes:\n",
    "      - ./notebooks:/app/notebooks\n",
    "      - ./data:/app/data\n",
    "      - ./models:/app/models\n",
    "    command: >\n",
    "      bash -c \"pip install jupyter &&\n",
    "      jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token=''\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker Compose commands\n",
    "print(\"\"\"\\nDocker Compose Commands:\\n\n",
    "Start all services:\n",
    "  docker-compose up -d\n",
    "\n",
    "View logs:\n",
    "  docker-compose logs -f trainer\n",
    "  docker-compose logs -f jupyter\n",
    "\n",
    "Stop all services:\n",
    "  docker-compose down\n",
    "\n",
    "Rebuild and start:\n",
    "  docker-compose up -d --build\n",
    "\n",
    "Access Jupyter Lab:\n",
    "  http://localhost:8888\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Multi-Stage Build for Image Optimization\n",
    "\n",
    "**Purpose**: Dramatically reduce Docker image sizes by separating build and runtime environments.\n",
    "\n",
    "### The Image Size Problem\n",
    "\n",
    "**Typical ML image build**:\n",
    "```dockerfile\n",
    "FROM python:3.10\n",
    "RUN pip install torch transformers numpy pandas scikit-learn\n",
    "COPY . .\n",
    "```\n",
    "\n",
    "**Result**: 5-10GB image containing:\n",
    "- Build tools (gcc, make, pip)\n",
    "- Package manager caches\n",
    "- Source files from installations\n",
    "- Intermediate build artifacts\n",
    "- Everything, even if only needed during installation\n",
    "\n",
    "**Problems**:\n",
    "- Slow to download (minutes on slow connections)\n",
    "- Expensive to store (cloud storage costs)\n",
    "- More attack surface (extra software = more vulnerabilities)\n",
    "- Slower to start (container needs to extract GBs)\n",
    "\n",
    "### What Are Multi-Stage Builds?\n",
    "\n",
    "**Concept**: Use multiple FROM statements in one Dockerfile\n",
    "\n",
    "**Stage 1 (builder)**: Full environment with build tools\n",
    "- Install packages\n",
    "- Compile code\n",
    "- Download dependencies\n",
    "- All the heavy lifting\n",
    "\n",
    "**Stage 2 (runtime)**: Minimal environment\n",
    "- Copy only installed packages from Stage 1\n",
    "- No build tools\n",
    "- No caches\n",
    "- Just what's needed to run\n",
    "\n",
    "**Magic**: Stage 1 artifacts are discarded, only Stage 2 becomes the final image\\!\n",
    "\n",
    "### Size Comparison\n",
    "\n",
    "**Single-stage**:\n",
    "```dockerfile\n",
    "FROM python:3.10\n",
    "RUN pip install torch transformers\n",
    "```\n",
    "Size: ~5-7GB\n",
    "\n",
    "**Multi-stage**:\n",
    "```dockerfile\n",
    "FROM python:3.10 as builder\n",
    "RUN pip install --user torch transformers\n",
    "\n",
    "FROM python:3.10-slim\n",
    "COPY --from=builder /root/.local /root/.local\n",
    "```\n",
    "Size: ~2-3GB (40-60% smaller\\!)\n",
    "\n",
    "### How Multi-Stage Works\n",
    "\n",
    "**Stage 1: Builder**\n",
    "```dockerfile\n",
    "FROM python:3.10 as builder  # Named 'builder'\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install --user --no-cache-dir -r requirements.txt\n",
    "```\n",
    "\n",
    "**Key points**:\n",
    "- `as builder`: Names this stage\n",
    "- `--user`: Installs to /root/.local instead of system-wide\n",
    "- `--no-cache-dir`: Don't save pip cache (saves ~100-500MB)\n",
    "- Full `python:3.10` image (has compilers, headers)\n",
    "\n",
    "**Stage 2: Runtime**\n",
    "```dockerfile\n",
    "FROM python:3.10-slim  # Fresh, minimal base\n",
    "WORKDIR /app\n",
    "COPY --from=builder /root/.local /root/.local  # Magic line\\!\n",
    "ENV PATH=/root/.local/bin:$PATH\n",
    "COPY app.py .\n",
    "```\n",
    "\n",
    "**Key points**:\n",
    "- New FROM = new base, previous stage discarded\n",
    "- `--from=builder`: Copy from named stage\n",
    "- Only copies installed packages, not build tools\n",
    "- `ENV PATH`: Makes copied executables findable\n",
    "\n",
    "### The --user Flag Explained\n",
    "\n",
    "**Without --user**:\n",
    "```bash\n",
    "pip install torch  # Installs to /usr/local/lib/python3.10/...\n",
    "```\n",
    "Scattered across system directories, hard to copy cleanly\n",
    "\n",
    "**With --user**:\n",
    "```bash\n",
    "pip install --user torch  # Installs to /root/.local/\n",
    "```\n",
    "Everything in one directory:\n",
    "- /root/.local/lib/python3.10/site-packages/\n",
    "- /root/.local/bin/\n",
    "\n",
    "Easy to copy in one COPY command\\!\n",
    "\n",
    "### Why PATH Update Is Needed\n",
    "\n",
    "After copying /root/.local:\n",
    "```dockerfile\n",
    "ENV PATH=/root/.local/bin:$PATH\n",
    "```\n",
    "\n",
    "**Why**:\n",
    "- Executables like `torch-config` are in /root/.local/bin\n",
    "- Without PATH update, they won't be found\n",
    "- System will error with \"command not found\"\n",
    "\n",
    "### Advanced Multi-Stage Patterns\n",
    "\n",
    "**Multiple builders**:\n",
    "```dockerfile\n",
    "FROM node:18 as frontend-builder\n",
    "RUN npm install && npm run build\n",
    "\n",
    "FROM python:3.10 as backend-builder\n",
    "RUN pip install --user -r requirements.txt\n",
    "\n",
    "FROM python:3.10-slim\n",
    "COPY --from=frontend-builder /app/dist /app/static\n",
    "COPY --from=backend-builder /root/.local /root/.local\n",
    "```\n",
    "Combine artifacts from multiple build stages\\!\n",
    "\n",
    "**Compile from source**:\n",
    "```dockerfile\n",
    "FROM python:3.10 as builder\n",
    "RUN apt-get update && apt-get install -y build-essential\n",
    "RUN pip wheel --no-cache-dir --wheel-dir=/wheels torch\n",
    "\n",
    "FROM python:3.10-slim\n",
    "COPY --from=builder /wheels /wheels\n",
    "RUN pip install --no-cache /wheels/*\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**1. Use slim/alpine for final stage**:\n",
    "```dockerfile\n",
    "FROM python:3.10         # Builder: 900MB\n",
    "FROM python:3.10-slim    # Runtime: 120MB âœ…\n",
    "FROM python:3.10-alpine  # Runtime: 50MB (but compatibility issues)\n",
    "```\n",
    "\n",
    "**2. Order matters**:\n",
    "```dockerfile\n",
    "# Copy dependencies first (changes less often)\n",
    "COPY --from=builder /root/.local /root/.local\n",
    "# Copy code last (changes frequently)\n",
    "COPY app.py .\n",
    "```\n",
    "Better layer caching = faster rebuilds\n",
    "\n",
    "**3. Clean up in builder**:\n",
    "```dockerfile\n",
    "RUN pip install --user --no-cache-dir -r requirements.txt && \\\n",
    "    rm -rf /root/.cache\n",
    "```\n",
    "\n",
    "**4. Security**:\n",
    "- Fewer packages in final image = smaller attack surface\n",
    "- No compilers/build tools in production\n",
    "- Scan images: `docker scan myimage`\n",
    "\n",
    "### Real-World Impact\n",
    "\n",
    "**Company example**:\n",
    "- Before: 8GB image, 5 min deploy time\n",
    "- After multi-stage: 2GB image, 1 min deploy time\n",
    "- Savings: $1000s/month in bandwidth and storage\n",
    "\n",
    "**Research example**:\n",
    "- Sharing 10GB images on slow university networks: hours\n",
    "- 2GB images: minutes\n",
    "- Reproducibility improved (people actually download and test)\n",
    "\n",
    "### Verification\n",
    "\n",
    "**Compare sizes**:\n",
    "```bash\n",
    "docker images | grep pytorch\n",
    "pytorch-single    latest   6.2GB\n",
    "pytorch-multi     latest   2.1GB  # 65% smaller\\!\n",
    "```\n",
    "\n",
    "**Verify it works**:\n",
    "```bash\n",
    "docker run --rm pytorch-multi python -c \"import torch; print(torch.__version__)\"\n",
    "```\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Create a multi-stage Dockerfile:\n",
    "1. **Stage 1 (builder)**: Use full Python 3.10, install packages with --user\n",
    "2. **Stage 2 (runtime)**: Use Python 3.10-slim, copy only installed packages\n",
    "3. Update PATH so executables are found\n",
    "4. Copy application code\n",
    "\n",
    "**Success**: Image is significantly smaller but functionally identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile.multistage\n",
    "# TODO: Create a multi-stage Dockerfile\n",
    "# Stage 1: Build dependencies\n",
    "# Stage 2: Minimal runtime with only installed packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile.multistage\n",
    "# ============================================\n",
    "# Stage 1: Builder - Install dependencies\n",
    "# ============================================\n",
    "FROM python:3.10 as builder\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies in user directory\n",
    "# Using --user puts packages in /root/.local\n",
    "COPY requirements.txt .\n",
    "RUN pip install --user --no-cache-dir -r requirements.txt\n",
    "\n",
    "# ============================================\n",
    "# Stage 2: Runtime - Minimal final image\n",
    "# ============================================\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy only installed packages from builder stage\n",
    "# This excludes pip, setuptools, and other build tools\n",
    "COPY --from=builder /root/.local /root/.local\n",
    "\n",
    "# Update PATH to find installed packages\n",
    "ENV PATH=/root/.local/bin:$PATH\n",
    "\n",
    "# Copy application code\n",
    "COPY app.py .\n",
    "\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare image sizes\n",
    "print(\"\"\"\\nCompare single-stage vs multi-stage image sizes:\\n\n",
    "Build both:\n",
    "  docker build -t pytorch-single -f Dockerfile.basic .\n",
    "  docker build -t pytorch-multi -f Dockerfile.multistage .\n",
    "\n",
    "Check sizes:\n",
    "  docker images | grep pytorch\n",
    "\n",
    "You should see pytorch-multi is smaller!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Dependency Management with requirements.txt\n",
    "\n",
    "**Purpose**: Pin dependencies for reproducible builds and faster Docker caching.\n",
    "\n",
    "### Why requirements.txt?\n",
    "\n",
    "**Without it**:\n",
    "```dockerfile\n",
    "RUN pip install torch transformers numpy pandas\n",
    "```\n",
    "\n",
    "**Problems**:\n",
    "- Installs latest versions (breaks reproducibility)\n",
    "- Docker layer rebuilds on any Dockerfile change\n",
    "- No documentation of dependencies\n",
    "- Hard to update selectively\n",
    "\n",
    "**With requirements.txt**:\n",
    "```dockerfile\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- Version-controlled with code\n",
    "- Docker caches layer until requirements.txt changes\n",
    "- Clear documentation of dependencies\n",
    "- Easy to diff and review changes\n",
    "\n",
    "### Version Pinning Strategies\n",
    "\n",
    "**Exact version** (most strict):\n",
    "```\n",
    "torch==2.6.0\n",
    "```\n",
    "- Guarantees exact same version\n",
    "- Safest for production\n",
    "- Doesn't get bug fixes automatically\n",
    "\n",
    "**Compatible version** (recommended):\n",
    "```\n",
    "torch>=2.6.0\n",
    "```\n",
    "- Allows patch updates (2.6.1, 2.6.2)\n",
    "- Gets bug fixes\n",
    "- Won't break compatibility (semantic versioning)\n",
    "\n",
    "**Range** (flexible):\n",
    "```\n",
    "torch>=2.6.0,<3.0.0\n",
    "```\n",
    "- Allows minor updates\n",
    "- Avoids major version breaking changes\n",
    "\n",
    "**No pin** (dangerous):\n",
    "```\n",
    "torch  # Gets whatever is latest\n",
    "```\n",
    "- Never use in production\\!\n",
    "- \"Works on my machine\" guaranteed\n",
    "\n",
    "### Organizing requirements.txt\n",
    "\n",
    "**Good structure** (commented and grouped):\n",
    "```\n",
    "# Deep Learning Framework\n",
    "torch>=2.6.0\n",
    "torchvision>=0.19.0\n",
    "\n",
    "# Transformers and NLP\n",
    "transformers>=4.40.0\n",
    "tokenizers>=0.15.0\n",
    "\n",
    "# Data Science\n",
    "numpy>=1.24.0\n",
    "pandas>=2.0.0\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- Easy to understand what each group is for\n",
    "- Can comment out entire sections for testing\n",
    "- New team members understand dependencies\n",
    "\n",
    "### requirements.txt in Dockerfile\n",
    "\n",
    "**Optimal pattern** for Docker caching:\n",
    "```dockerfile\n",
    "# Copy requirements first (changes infrequently)\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy code last (changes frequently)\n",
    "COPY . .\n",
    "```\n",
    "\n",
    "**Why this order**:\n",
    "1. If only code changes: Cached dependencies, fast rebuild\n",
    "2. If requirements change: Reinstall dependencies, slower\n",
    "3. Maximizes cache hit rate\n",
    "\n",
    "**Bad order**:\n",
    "```dockerfile\n",
    "COPY . .  # Copies everything including requirements.txt\n",
    "RUN pip install -r requirements.txt  # Rebuilds on any file change\n",
    "```\n",
    "Every code edit = reinstall ALL dependencies\\!\n",
    "\n",
    "### Multiple requirements Files\n",
    "\n",
    "**Common pattern for different environments**:\n",
    "\n",
    "**requirements.txt** (production):\n",
    "```\n",
    "torch>=2.6.0\n",
    "transformers>=4.40.0\n",
    "```\n",
    "\n",
    "**requirements-dev.txt** (development):\n",
    "```\n",
    "-r requirements.txt  # Include production requirements\n",
    "pytest>=8.0.0\n",
    "black>=24.0.0\n",
    "jupyter>=1.0.0\n",
    "```\n",
    "\n",
    "**requirements-gpu.txt** (GPU builds):\n",
    "```\n",
    "torch>=2.6.0 --index-url https://download.pytorch.org/whl/cu124\n",
    "```\n",
    "\n",
    "### Generating requirements.txt\n",
    "\n",
    "**From current environment**:\n",
    "```bash\n",
    "pip freeze > requirements.txt\n",
    "```\n",
    "Captures exact versions of everything installed\n",
    "\n",
    "**Problem**: Includes transitive dependencies\n",
    "```\n",
    "torch==2.6.0\n",
    "nvidia-cublas-cu12==12.4.5.8  # Transitive dependency\n",
    "nvidia-cuda-cupti-cu12==12.4.127  # Transitive\n",
    "# ... 50+ more packages\n",
    "```\n",
    "\n",
    "**Better**: Manual requirements with top-level only:\n",
    "```\n",
    "torch>=2.6.0\n",
    "transformers>=4.40.0\n",
    "```\n",
    "Let pip resolve transitive dependencies\n",
    "\n",
    "### pip-compile for Lock Files\n",
    "\n",
    "**Advanced**: Use pip-tools\n",
    "\n",
    "**requirements.in** (what you want):\n",
    "```\n",
    "torch>=2.6.0\n",
    "transformers\n",
    "```\n",
    "\n",
    "**Generate locked requirements.txt**:\n",
    "```bash\n",
    "pip-compile requirements.in\n",
    "```\n",
    "\n",
    "**Output** (requirements.txt with all transitive deps pinned):\n",
    "```\n",
    "torch==2.6.0\n",
    "transformers==4.40.0\n",
    "tokenizers==0.15.0\n",
    "# ... all dependencies with exact versions\n",
    "```\n",
    "\n",
    "**Benefits**: Reproducible + documented + updateable\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "**1. Platform-specific packages**:\n",
    "```\n",
    "pywin32==306  # Only works on Windows\\!\n",
    "```\n",
    "Solution: Use environment markers\n",
    "```\n",
    "pywin32==306; sys_platform == 'win32'\n",
    "```\n",
    "\n",
    "**2. Missing index URLs**:\n",
    "```\n",
    "torch>=2.6.0  # Might get CPU version\n",
    "```\n",
    "Solution: Specify in Dockerfile\n",
    "```dockerfile\n",
    "RUN pip install -r requirements.txt \\\n",
    "    --index-url https://download.pytorch.org/whl/cu124\n",
    "```\n",
    "\n",
    "**3. Conflicting versions**:\n",
    "```\n",
    "packageA>=2.0  # Requires numpy<2.0\n",
    "packageB>=3.0  # Requires numpy>=2.0\n",
    "```\n",
    "pip will error. Solution: Check compatibility, adjust versions\n",
    "\n",
    "### Your Task\n",
    "\n",
    "This cell creates a sample requirements.txt with:\n",
    "- Deep learning frameworks (PyTorch, torchvision)\n",
    "- NLP libraries (transformers)\n",
    "- Data science tools (numpy, pandas, scikit-learn)\n",
    "- Version pins using >= for compatibility\n",
    "- Comments organizing by category\n",
    "\n",
    "**Use this pattern in your projects** for reproducible builds\\!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.docker.txt\n",
    "# Deep Learning Framework\n",
    "torch>=2.6.0\n",
    "torchvision>=0.19.0\n",
    "\n",
    "# Transformers for NLP\n",
    "transformers>=4.40.0\n",
    "\n",
    "# Data Science\n",
    "numpy>=1.24.0\n",
    "pandas>=2.0.0\n",
    "scikit-learn>=1.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: Optimizing Builds with .dockerignore\n",
    "\n",
    "**Purpose**: Exclude unnecessary files from Docker build context to speed up builds and reduce image sizes.\n",
    "\n",
    "### The Build Context Problem\n",
    "\n",
    "When you run `docker build .`, Docker sends **all files** in current directory to the Docker daemon:\n",
    "\n",
    "```bash\n",
    "docker build .\n",
    "Sending build context to Docker daemon  15.2GB  # Oops\\!\n",
    "```\n",
    "\n",
    "**What gets sent**:\n",
    "- All source code âœ… (needed)\n",
    "- Git history (.git/) âŒ (1-2GB, not needed)\n",
    "- Virtual environments (venv/) âŒ (500MB-2GB, not needed)\n",
    "- Jupyter checkpoints âŒ (100MB+, not needed)\n",
    "- Dataset files (data/) âŒ (GBs, mounted as volume instead)\n",
    "- Model checkpoints âŒ (GBs, should be in separate storage)\n",
    "\n",
    "**Impact**:\n",
    "- Build takes minutes just uploading context\n",
    "- Network bandwidth wasted\n",
    "- Larger images if files are COPYed\n",
    "- Slower CI/CD pipelines\n",
    "\n",
    "### What is .dockerignore?\n",
    "\n",
    "Like .gitignore but for Docker builds:\n",
    "\n",
    "**Create `.dockerignore` file**:\n",
    "```\n",
    "__pycache__\n",
    "*.pyc\n",
    ".git\n",
    "venv/\n",
    "```\n",
    "\n",
    "**These files won't be sent to Docker daemon**:\n",
    "- Faster builds\n",
    "- Smaller build context\n",
    "- Can't accidentally COPY sensitive files\n",
    "\n",
    "### Essential Patterns to Ignore\n",
    "\n",
    "**Python cache files** (100s of MB):\n",
    "```\n",
    "__pycache__\n",
    "*.pyc\n",
    "*.pyo\n",
    "*.pyd\n",
    ".Python\n",
    "```\n",
    "Regenerated automatically, no need to copy\n",
    "\n",
    "**Virtual environments** (GBs):\n",
    "```\n",
    ".env\n",
    ".venv\n",
    "venv/\n",
    "ENV/\n",
    "env/\n",
    "```\n",
    "Container has its own Python environment\n",
    "\n",
    "**Git files** (GBs):\n",
    "```\n",
    ".git\n",
    ".gitignore\n",
    ".gitattributes\n",
    "```\n",
    "Git history not needed in container\n",
    "\n",
    "**IDE files** (MBs):\n",
    "```\n",
    ".vscode/\n",
    ".idea/\n",
    "*.swp\n",
    "*.swo\n",
    ".DS_Store\n",
    "```\n",
    "Editor-specific, not needed for running code\n",
    "\n",
    "**Jupyter notebooks** (optional):\n",
    "```\n",
    "*.ipynb\n",
    ".ipynb_checkpoints\n",
    "```\n",
    "Usually for development, not deployment\n",
    "\n",
    "**Testing files**:\n",
    "```\n",
    ".pytest_cache\n",
    ".coverage\n",
    "htmlcov/\n",
    ".tox/\n",
    "```\n",
    "Tests run in CI, not in container\n",
    "\n",
    "**Build artifacts**:\n",
    "```\n",
    "dist/\n",
    "build/\n",
    "*.egg-info/\n",
    ".eggs/\n",
    "```\n",
    "Rebuilt inside container\n",
    "\n",
    "**Documentation**:\n",
    "```\n",
    "*.md\n",
    "docs/\n",
    "README.md\n",
    "```\n",
    "Not needed for running code (unless serving docs)\n",
    "\n",
    "### Whitelisting Pattern\n",
    "\n",
    "**Ignore everything except specific files**:\n",
    "```\n",
    "# Ignore everything\n",
    "*\n",
    "\n",
    "# Except these\n",
    "\\!app.py\n",
    "\\!requirements.txt\n",
    "\\!src/\n",
    "```\n",
    "\n",
    "Useful for monorepos where you only need subset of files\n",
    "\n",
    "### Special Cases for ML Projects\n",
    "\n",
    "**Large datasets** (DON'T include in image):\n",
    "```\n",
    "data/\n",
    "datasets/\n",
    "*.csv\n",
    "*.parquet\n",
    "```\n",
    "**Why**: GBs of data â†’ huge images\n",
    "**Instead**: Mount as volume or download at runtime\n",
    "\n",
    "**Model checkpoints** (usually don't include):\n",
    "```\n",
    "*.pt\n",
    "*.pth\n",
    "*.ckpt\n",
    "*.safetensors\n",
    "models/\n",
    "checkpoints/\n",
    "```\n",
    "**Why**: Models are GBs, change frequently\n",
    "**Instead**: Mount volume or download from model registry\n",
    "\n",
    "**Exception**: Small models (<100MB) for inference containers\n",
    "\n",
    "**Logs** (never include):\n",
    "```\n",
    "*.log\n",
    "logs/\n",
    "```\n",
    "**Why**: Container logs should go to stdout/stderr\n",
    "\n",
    "### Verification\n",
    "\n",
    "**Check what's being sent**:\n",
    "```bash\n",
    "docker build --no-cache . 2>&1 | grep 'Sending build context'\n",
    "```\n",
    "\n",
    "**Before .dockerignore**:\n",
    "```\n",
    "Sending build context to Docker daemon  8.5GB\n",
    "```\n",
    "\n",
    "**After .dockerignore**:\n",
    "```\n",
    "Sending build context to Docker daemon  2.1MB  # 4000x smaller\\!\n",
    "```\n",
    "\n",
    "**List files in build context**:\n",
    "```bash\n",
    "docker run --rm -v $(pwd):/check alpine sh -c 'cd /check && du -sh * | sort -h'\n",
    "```\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "**1. .dockerignore in wrong location**:\n",
    "- Must be in build context root\n",
    "- If `docker build path/to/project`, put it in `path/to/project/.dockerignore`\n",
    "\n",
    "**2. Ignoring required files**:\n",
    "```\n",
    "*.py  # Oops\\! Ignores all Python files\n",
    "```\n",
    "Build succeeds but image is broken\n",
    "\n",
    "**3. Not testing .dockerignore**:\n",
    "- Always rebuild after adding .dockerignore\n",
    "- Verify image still works\n",
    "\n",
    "**4. Overusing wildcards**:\n",
    "```\n",
    "*test*  # Ignores testing/ but also pytest.py\\!\n",
    "```\n",
    "Be specific\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "**1. Start with comprehensive .dockerignore**:\n",
    "- Copy from this exercise\n",
    "- Adapt to your project\n",
    "- Keep it in version control\n",
    "\n",
    "**2. Regular audits**:\n",
    "```bash\n",
    "docker history myimage  # See what layers contain\n",
    "dive myimage            # Explore image layers (tool)\n",
    "```\n",
    "\n",
    "**3. CI/CD checks**:\n",
    "```bash\n",
    "# Fail if build context > 100MB\n",
    "SIZE=$(docker build . 2>&1 | grep 'Sending' | awk '{print $5}')\n",
    "if [ $SIZE -gt 100 ]; then\n",
    "  echo 'Build context too large\\!'\n",
    "  exit 1\n",
    "fi\n",
    "```\n",
    "\n",
    "**4. Security**:\n",
    "```\n",
    ".env         # Don't copy secrets\n",
    "*.key\n",
    "*.pem\n",
    "credentials/\n",
    "```\n",
    "\n",
    "### Real-World Impact\n",
    "\n",
    "**Startup example**:\n",
    "- Before: 10GB context, 15 min builds\n",
    "- After .dockerignore: 5MB context, 2 min builds\n",
    "- CI/CD pipeline: 85% faster\n",
    "\n",
    "**Research lab**:\n",
    "- Accidentally included 50GB dataset\n",
    "- Docker build OOM on CI server\n",
    "- .dockerignore fixed it immediately\n",
    "\n",
    "### Your Task\n",
    "\n",
    "This cell creates a comprehensive .dockerignore covering:\n",
    "- Python cache and compiled files\n",
    "- Virtual environments\n",
    "- Git files\n",
    "- IDE configurations\n",
    "- Jupyter notebooks and checkpoints\n",
    "- Testing artifacts\n",
    "- Build outputs\n",
    "- OS-specific files\n",
    "- Documentation\n",
    "\n",
    "**Copy this to your projects** and customize as needed\\!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .dockerignore\n",
    "# Python cache and compiled files\n",
    "__pycache__\n",
    "*.pyc\n",
    "*.pyo\n",
    "*.pyd\n",
    ".Python\n",
    "\n",
    "# Virtual environments\n",
    ".env\n",
    ".venv\n",
    "venv/\n",
    "ENV/\n",
    "\n",
    "# Git\n",
    ".git\n",
    ".gitignore\n",
    ".gitattributes\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints\n",
    "*.ipynb\n",
    "\n",
    "# Testing\n",
    ".pytest_cache\n",
    ".coverage\n",
    "htmlcov/\n",
    "\n",
    "# Build artifacts\n",
    "dist/\n",
    "build/\n",
    "*.egg-info/\n",
    "\n",
    "# IDE\n",
    ".vscode/\n",
    ".idea/\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# Documentation\n",
    "*.md\n",
    "docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Practiced:\n",
    "\n",
    "1. âœ… **Basic Dockerfiles**: Created CPU-based PyTorch containers\n",
    "2. âœ… **GPU Support**: Configured CUDA-enabled containers for deep learning\n",
    "3. âœ… **Docker Compose**: Orchestrated multi-service applications\n",
    "4. âœ… **Multi-Stage Builds**: Optimized image sizes\n",
    "5. âœ… **Best Practices**: Used .dockerignore and requirements.txt\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Dockerfiles** define how to build images\n",
    "- **GPU support** requires NVIDIA runtime and CUDA base images\n",
    "- **Docker Compose** simplifies multi-container orchestration\n",
    "- **Multi-stage builds** reduce production image sizes\n",
    "- **.dockerignore** improves build performance and security\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Review**: Study [02_dockerization.md](./02_dockerization.md) for detailed concepts\n",
    "2. **Practice**: Build and run these containers in your environment\n",
    "3. **Experiment**: Try different base images and configurations\n",
    "4. **Deploy**: Containerize your own AI projects\n",
    "\n",
    "### Useful Commands Reference:\n",
    "\n",
    "```bash\n",
    "# Build image\n",
    "docker build -t <name> -f <dockerfile> .\n",
    "\n",
    "# Run container\n",
    "docker run --rm <name>\n",
    "docker run --gpus all --rm <name>  # With GPU\n",
    "\n",
    "# Docker Compose\n",
    "docker-compose up -d\n",
    "docker-compose logs -f <service>\n",
    "docker-compose down\n",
    "\n",
    "# Management\n",
    "docker images\n",
    "docker ps\n",
    "docker system prune -a  # Clean up\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
