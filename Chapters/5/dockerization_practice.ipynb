{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dockerization Practice Notebook\n\n**Prerequisites**: Docker installed, Python 3.10+, PyTorch 2.6.0+, CUDA 12.4+\n\n## Learning Objectives\n\nBy the end of this notebook, you will:\n\n1. **Write Dockerfiles**: Create container images for AI/ML projects from scratch\n2. **GPU Support**: Configure CUDA and NVIDIA runtime for deep learning\n3. **Multi-Service Apps**: Orchestrate training, inference, and development environments\n4. **Optimization**: Reduce image sizes with multi-stage builds\n5. **Best Practices**: Security, caching, and production-ready configurations\n\n## Why Docker for AI/ML?\n\n**The Dependency Hell Problem**:\n- Different projects need different Python versions\n- PyTorch 1.x vs 2.x incompatibilities\n- CUDA version mismatches\n- System library conflicts\n- \"Works on my machine\" syndrome\n\n**Docker Solves This**:\n- **Reproducibility**: Same environment everywhere (dev, staging, prod)\n- **Isolation**: Each project has its own dependencies\n- **Portability**: Build once, run anywhere (laptop, cloud, HPC)\n- **Version Control**: Dockerfile tracks environment changes with code\n- **Scalability**: Easy to deploy thousands of containers\n\n**Real-World Impact**:\n- Companies save weeks of environment setup time\n- Research reproducibility improves dramatically\n- CI/CD pipelines become reliable\n- Cloud deployments are simplified\n\n## Docker vs Virtual Machines\n\n| Feature | Docker Container | Virtual Machine |\n|---------|-----------------|------------------|\n| Size | MBs | GBs |\n| Startup | Seconds | Minutes |\n| Performance | Near-native | Overhead |\n| Isolation | Process-level | Hardware-level |\n\n**For AI/ML**: Docker is faster, lighter, and sufficient for most use cases.\n\n## How This Notebook Works\n\nMost cells use **`%%writefile`** magic command:\n- Creates files directly on your disk\n- You can then build Docker images from these files\n- Run actual containers to test your work\n\n**Note**: Docker commands should be run in your **terminal**, not in this notebook.\n\n\ud83d\udca1 **Tip**: Try completing TODO sections before viewing solutions\\!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Create a Basic Dockerfile\n\n**Purpose**: Learn the fundamental building blocks of Dockerfiles for PyTorch applications.\n\n### Understanding Dockerfiles\n\nA Dockerfile is a **recipe** for building a Docker image:\n- Written as a series of instructions (commands)\n- Each instruction creates a new layer\n- Layers are cached for faster rebuilds\n- Final image is a stack of all layers\n\nThink of it like a script that:\n1. Starts with a base operating system\n2. Installs software\n3. Copies your code\n4. Configures how to run it\n\n### Key Dockerfile Instructions\n\n**FROM**: Choose base image\n```dockerfile\nFROM python:3.10-slim  # Official Python image, minimal size\n```\n- Always the first instruction\n- Use official images when possible (security, maintenance)\n- `-slim` variant: Smaller size, fewer packages\n- `-alpine`: Even smaller, but compatibility issues\n\n**WORKDIR**: Set working directory\n```dockerfile\nWORKDIR /app  # All subsequent commands run here\n```\n- Like `cd /app` but also creates directory\n- Keeps image organized\n- Relative paths in COPY/ADD are relative to WORKDIR\n\n**RUN**: Execute commands during build\n```dockerfile\nRUN pip install torch  # Runs at build time\n```\n- Installs packages, downloads files, compiles code\n- Each RUN creates a new layer (affects image size)\n- Combine multiple commands with && to reduce layers\n\n**COPY**: Copy files from host to image\n```dockerfile\nCOPY app.py .  # Copies app.py to /app/app.py\n```\n- Copies from build context (usually current directory)\n- Use .dockerignore to exclude files\n- Copying source code last improves cache efficiency\n\n**CMD**: Default command when container starts\n```dockerfile\nCMD [\"python\", \"app.py\"]  # Runs when container starts\n```\n- Only one CMD per Dockerfile (last one wins)\n- Can be overridden: `docker run myimage python other.py`\n- Use JSON array format for better signal handling\n\n### Why Use python:3.10-slim?\n\n| Image | Size | Use Case |\n|-------|------|----------|\n| python:3.10 | ~900MB | Full development, all tools |\n| python:3.10-slim | ~120MB | Production, minimal overhead |\n| python:3.10-alpine | ~50MB | Ultra-minimal, may have compatibility issues |\n\n**For ML**: slim is the sweet spot - small but compatible.\n\n### Layer Caching Strategy\n\nDocker caches layers. Order matters for efficiency:\n\n**Bad (slow rebuilds)**:\n```dockerfile\nCOPY . .              # Changes frequently\nRUN pip install torch # Runs every code change\n```\n\n**Good (fast rebuilds)**:\n```dockerfile\nRUN pip install torch # Cached unless dependencies change\nCOPY . .              # Only this layer rebuilds on code changes\n```\n\n**Best practice**: Copy requirements.txt first, install, then copy code.\n\n### Your Task\n\nCreate a Dockerfile that:\n1. Uses `python:3.10-slim` as base (lightweight)\n2. Sets `/app` as working directory (organization)\n3. Installs `torch>=2.6.0` (deep learning framework)\n4. Copies `app.py` (your application)\n5. Runs `app.py` by default (when container starts)\n\n**Success criteria**: A buildable, minimal PyTorch container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this Dockerfile\n",
    "# This cell uses %%writefile magic command to create a file\n",
    "# Try to complete it before looking at the solution\n",
    "\n",
    "%%writefile Dockerfile.basic\n",
    "# TODO: Create a Dockerfile that:\n",
    "# 1. Uses python:3.10-slim as base image\n",
    "# 2. Sets /app as working directory\n",
    "# 3. Installs torch>=2.6.0\n",
    "# 4. Copies app.py to container\n",
    "# 5. Runs app.py as default command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile.basic\n",
    "# Base image: Using slim variant to reduce image size\n",
    "FROM python:3.10-slim\n",
    "\n",
    "# Set working directory inside container\n",
    "WORKDIR /app\n",
    "\n",
    "# Install PyTorch (--no-cache-dir reduces image size)\n",
    "RUN pip install --no-cache-dir torch>=2.6.0\n",
    "\n",
    "# Copy application code from host to container\n",
    "COPY app.py .\n",
    "\n",
    "# Default command when container starts\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Create a Sample Application\n\n**Purpose**: Build a diagnostic application to verify Docker environment setup.\n\n### Why Create a Test Application?\n\nBefore deploying production models, verify:\n- Python version is correct\n- PyTorch installed successfully\n- CUDA is accessible (for GPU containers)\n- Basic tensor operations work\n\n**In production**: Similar health check scripts validate deployments.\n\n### What This Application Does\n\n**1. Environment Information**:\n```python\nprint(f\"Python: {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\n```\n- Confirms correct versions are installed\n- Useful for debugging version mismatches\n\n**2. CUDA Detection**:\n```python\ntorch.cuda.is_available()\n```\n- Returns True if GPU is accessible\n- False might mean: no GPU, wrong driver, wrong PyTorch build\n\n**3. GPU Information** (if available):\n```python\ntorch.cuda.get_device_name(0)\n```\n- Shows GPU model (e.g., \"NVIDIA GeForce RTX 3090\")\n- Helps verify correct GPU is being used\n\n**4. Computation Test**:\n```python\nx = torch.randn(1000, 1000).cuda()\ny = torch.randn(1000, 1000).cuda()\nz = torch.matmul(x, y)\n```\n- Matrix multiplication is a common ML operation\n- Tests both memory allocation and computation\n- Failure here means something is seriously wrong\n\n### CPU vs GPU Code Paths\n\nNotice the conditional logic:\n\n```python\nif torch.cuda.is_available():\n    x = torch.randn(1000, 1000).cuda()  # GPU\nelse:\n    x = torch.randn(1000, 1000)  # CPU\n```\n\n**Why**:\n- CPU-only containers (for testing/development)\n- GPU containers (for training/inference)\n- Same code works in both environments\n\n### Container Health Checks\n\nThis pattern extends to production:\n\n**Dockerfile health check**:\n```dockerfile\nHEALTHCHECK --interval=30s --timeout=3s \\\n  CMD python health_check.py || exit 1\n```\n\n**Kubernetes readiness probe**:\n```yaml\nreadinessProbe:\n  exec:\n    command: [\"python\", \"health_check.py\"]\n```\n\n### Expected Output\n\n**CPU container**:\n```\nPython: 3.10.x\nPyTorch: 2.6.0\nCUDA Available: False\nRunning on CPU\nTesting CPU computation...\n\u2705 CPU computation successful\n```\n\n**GPU container**:\n```\nPython: 3.10.x\nPyTorch: 2.6.0\nCUDA Available: True\nCUDA Version: 12.4\nGPU: NVIDIA GeForce RTX 3090\nTesting GPU computation...\n\u2705 GPU computation successful\n```\n\n### Troubleshooting\n\n**If CUDA shows False** (but you have a GPU):\n1. Check NVIDIA drivers: `nvidia-smi`\n2. Verify nvidia-docker installed: `docker run --gpus all nvidia/cuda:12.4.0-base nvidia-smi`\n3. Check PyTorch CUDA version matches system CUDA\n4. Use `--gpus all` flag when running container\n\n**If computation fails**:\n1. Out of memory: Reduce matrix size\n2. CUDA error: Incompatible CUDA versions\n3. Permission denied: Check Docker GPU access\n\n### Your Task\n\nThis cell creates `app.py` with:\n- Version information display\n- CUDA availability check\n- Conditional GPU/CPU code paths\n- Matrix multiplication test\n- Clear success/failure messages\n\n**No TODO** - this is a complete reference implementation you'll use in later exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\"\"\"Sample PyTorch application for Docker testing.\n",
    "\n",
    "This script validates that:\n",
    "1. Python version is correct\n",
    "2. PyTorch is installed properly\n",
    "3. CUDA is available (if running with GPU)\n",
    "4. Basic tensor operations work\n",
    "\"\"\"\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"PyTorch Docker Container Test\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display environment info\n",
    "print(f\"\\nPython: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Test GPU computation\n",
    "    print(\"\\nTesting GPU computation...\")\n",
    "    x = torch.randn(1000, 1000).cuda()\n",
    "    y = torch.randn(1000, 1000).cuda()\n",
    "    z = torch.matmul(x, y)\n",
    "    print(\"\u2705 GPU computation successful\")\n",
    "else:\n",
    "    print(\"\\nRunning on CPU\")\n",
    "    print(\"Testing CPU computation...\")\n",
    "    x = torch.randn(1000, 1000)\n",
    "    y = torch.randn(1000, 1000)\n",
    "    z = torch.matmul(x, y)\n",
    "    print(\"\u2705 CPU computation successful\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"All tests passed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Build and Run Docker Image\n\n**Purpose**: Learn Docker CLI commands to build images and run containers.\n\n### The Docker Build Process\n\n**What happens when you build**:\n1. Docker reads the Dockerfile\n2. Executes each instruction in order\n3. Creates a layer for each instruction\n4. Caches layers for faster rebuilds\n5. Tags the final image with a name\n\n**Build time** depends on:\n- Base image size (downloading if not cached)\n- Number of packages to install\n- Network speed (downloading dependencies)\n- Layer caching (faster if nothing changed)\n\nFirst build: 2-10 minutes. Cached rebuilds: seconds.\n\n### Understanding `docker build` Command\n\n```bash\ndocker build -t pytorch-basic -f Dockerfile.basic .\n```\n\n**Breaking it down**:\n\n**`docker build`**: The build command\n\n**`-t pytorch-basic`**: Tag (name) the image\n- Format: `name:tag` (e.g., `pytorch-basic:v1.0`)\n- No tag specified = `latest` by default\n- Use descriptive names: `my-model-trainer`, `api-server`\n\n**`-f Dockerfile.basic`**: Specify which Dockerfile\n- Default: looks for file named `Dockerfile`\n- Use -f when you have multiple Dockerfiles\n- Examples: Dockerfile.dev, Dockerfile.prod, Dockerfile.gpu\n\n**`.`**: Build context (current directory)\n- Docker sends all files in this directory to build daemon\n- COPY commands copy from this context\n- Use .dockerignore to exclude large/unnecessary files\n- Can specify different path: `docker build -t myapp /path/to/context`\n\n### Build Output Explained\n\nYou'll see:\n```\nSending build context to Docker daemon  2.048kB\nStep 1/5 : FROM python:3.10-slim\n ---> abc123def456\nStep 2/5 : WORKDIR /app\n ---> Running in xyz789...\n ---> def456ghi789\n...\nSuccessfully built abc123def456\nSuccessfully tagged pytorch-basic:latest\n```\n\n**What each part means**:\n- **Sending context**: Uploading files to Docker\n- **Step X/Y**: Which Dockerfile instruction is executing\n- **Running in...**: Temporary container created for this step\n- **Successfully built**: Image ID (hash)\n- **Successfully tagged**: Your friendly name\n\n### Understanding `docker run` Command\n\n```bash\ndocker run --rm pytorch-basic\n```\n\n**Breaking it down**:\n\n**`docker run`**: Create and start a container\n\n**`--rm`**: Remove container after it exits\n- Without this, stopped containers accumulate\n- Good for one-off tasks and testing\n- Use `docker ps -a` to see all containers\n- Clean up: `docker container prune`\n\n**`pytorch-basic`**: Image to run\n- Uses local image if available\n- Downloads from Docker Hub if not found\n- Can specify version: `pytorch-basic:v1.0`\n\n### Common `docker run` Options\n\n**Interactive mode**:\n```bash\ndocker run -it --rm pytorch-basic bash\n```\n- `-it`: Interactive terminal\n- `bash`: Override CMD, run bash shell instead\n- Useful for debugging and exploration\n\n**Port mapping**:\n```bash\ndocker run -p 8000:8000 --rm pytorch-basic\n```\n- `-p 8000:8000`: Map host port 8000 to container port 8000\n- Format: `-p HOST:CONTAINER`\n- Needed for web servers, APIs, Jupyter\n\n**Volume mounting**:\n```bash\ndocker run -v $(pwd)/data:/app/data --rm pytorch-basic\n```\n- `-v HOST:CONTAINER`: Mount directory\n- Changes in container persist on host\n- Useful for data, models, configs\n\n**Environment variables**:\n```bash\ndocker run -e MODEL_NAME=gpt2 --rm pytorch-basic\n```\n- `-e KEY=VALUE`: Set environment variable\n- Access in code: `os.getenv('MODEL_NAME')`\n\n### Verification Commands\n\nAfter building, verify your image:\n\n**List images**:\n```bash\ndocker images\ndocker images | grep pytorch-basic\n```\nShows: name, tag, image ID, creation date, size\n\n**Inspect image**:\n```bash\ndocker inspect pytorch-basic\n```\nShows: layers, environment variables, CMD, ENTRYPOINT, etc.\n\n**Check image size**:\n```bash\ndocker images pytorch-basic --format '{{.Size}}'\n```\nImportant: Smaller images = faster deployment, lower costs\n\n### Your Task\n\nRun these commands in your **terminal** (not Jupyter):\n\n1. Navigate to directory with Dockerfile and app.py\n2. Build the image (will take 2-5 minutes first time)\n3. Verify image was created\n4. Run a container from the image\n5. Observe the output from app.py\n\n**Success**: You see Python/PyTorch versions and successful computation message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Run these commands in your TERMINAL, not in this notebook\n",
    "# The ! prefix would execute them in Jupyter, but Docker commands work better in terminal\n",
    "\n",
    "# Step 1: Build the Docker image\n",
    "# docker build -t pytorch-basic -f Dockerfile.basic .\n",
    "\n",
    "# Step 2: Run the container\n",
    "# docker run --rm pytorch-basic\n",
    "\n",
    "# Step 3: List your images\n",
    "# docker images | grep pytorch-basic\n",
    "\n",
    "print(\"\"\"\\nTo build and run:\\n\n",
    "Terminal Commands:\n",
    "==================\n",
    "cd /home/wsl2ubt2204/Starter_tutorial/Chapters/5\n",
    "docker build -t pytorch-basic -f Dockerfile.basic .\n",
    "docker run --rm pytorch-basic\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Create GPU-Enabled Dockerfile\n\n**Purpose**: Configure Docker containers for GPU-accelerated deep learning.\n\n### Why GPU Containers Are Different\n\nGPUs require special setup:\n- **CUDA runtime** libraries (not just drivers)\n- **cuDNN** for optimized neural network operations\n- **NVIDIA Container Toolkit** on host system\n- **Matching versions** (CUDA, PyTorch, drivers)\n\n**The challenge**: Getting all versions aligned correctly.\n\n### Prerequisites for GPU Containers\n\n**On the host system (your machine)**:\n\n1. **NVIDIA GPU**: Obviously\\!\n2. **NVIDIA Driver**: Version 535+ for CUDA 12.x\n   - Check: `nvidia-smi`\n3. **nvidia-docker2**: Docker GPU support\n   - Install: See dockerization.md\n4. **Docker configured**: Runtime set to nvidia\n\n**Common issue**: CPU Dockerfile works but GPU version fails due to missing prerequisites.\n\n### NVIDIA CUDA Base Images\n\nNVIDIA provides official images with CUDA pre-installed:\n\n**Image options**:\n```dockerfile\nnvidia/cuda:12.4.0-base-ubuntu22.04     # Minimal CUDA\nnvidia/cuda:12.4.0-runtime-ubuntu22.04  # + libraries\nnvidia/cuda:12.4.0-devel-ubuntu22.04    # + dev tools\n```\n\n**Which to use**:\n- **base**: Just CUDA, smallest\n- **runtime**: For inference, includes cuDNN \u2190 **Use this**\n- **devel**: For compiling CUDA code, largest\n\n**Size comparison**:\n- base: ~200MB\n- runtime: ~1.5GB\n- devel: ~3GB\n\n### Installing Python on CUDA Images\n\nCUDA images are based on Ubuntu, not Python:\n\n```dockerfile\n# This is Ubuntu, not Python\\!\nFROM nvidia/cuda:12.4.0-runtime-ubuntu22.04\n\n# Must install Python ourselves\nRUN apt-get update && apt-get install -y \\\n    python3.10 \\\n    python3-pip\n```\n\n**Why not FROM python:3.10 with CUDA**?\n- Python images don't include CUDA\n- Installing CUDA manually is complex and error-prone\n- NVIDIA images are tested and optimized\n\n### PyTorch CUDA Version Matching\n\n**Critical**: PyTorch CUDA version must match container CUDA version\\!\n\n**Our setup**:\n- Container: CUDA 12.4\n- PyTorch: Must use cu124 build\n\n```dockerfile\nRUN pip install torch>=2.6.0 \\\n    --index-url https://download.pytorch.org/whl/cu124\n```\n\n**Index URLs**:\n- cu124: CUDA 12.4\n- cu121: CUDA 12.1\n- cu118: CUDA 11.8\n- cpu: CPU-only version\n\n**Wrong version = Runtime errors or no GPU detected\\!**\n\n### Setting Python as Default\n\nUbuntu 22.04 has `python3` but not `python`:\n\n```dockerfile\nRUN update-alternatives --install /usr/bin/python python /usr/bin/python3.10 1\n```\n\n**Why**:\n- Makes `python` command work (not just `python3`)\n- Avoids scripts failing with \"python: command not found\"\n- Standard practice in containerized environments\n\n### Running GPU Containers\n\n**Must use `--gpus` flag**:\n\n```bash\ndocker run --gpus all --rm pytorch-gpu\n```\n\n**GPU access options**:\n```bash\n--gpus all              # All GPUs\n--gpus '\"device=0\"'     # GPU 0 only\n--gpus '\"device=0,1\"'   # GPU 0 and 1\n--gpus 2                # Any 2 GPUs\n```\n\n**Without --gpus**: Container runs but `torch.cuda.is_available()` returns False\\!\n\n### Troubleshooting GPU Access\n\n**Problem**: `torch.cuda.is_available()` returns False\n\n**Checklist**:\n1. \u2705 Host GPU works: `nvidia-smi`\n2. \u2705 nvidia-docker installed: `docker run --gpus all nvidia/cuda:12.4.0-base nvidia-smi`\n3. \u2705 Used `--gpus all` flag when running\n4. \u2705 CUDA versions match (container CUDA = PyTorch CUDA)\n5. \u2705 PyTorch installed from correct index-url\n\n**Test GPU access in container**:\n```bash\ndocker run --gpus all --rm pytorch-gpu nvidia-smi\n```\nShould show GPU info, not \"command not found\"\n\n### GPU Container Best Practices\n\n**1. Pin CUDA version**:\n- Use specific tag: `12.4.0-runtime` not `latest`\n- Ensures reproducibility\n\n**2. Match everything**:\n- Host CUDA/driver version\n- Container CUDA version\n- PyTorch CUDA build version\n\n**3. Set shared memory size**:\n```bash\ndocker run --gpus all --shm-size=8g pytorch-gpu\n```\n- PyTorch DataLoader needs shared memory\n- Default 64MB often too small\n- Symptoms: DataLoader hangs or crashes\n\n**4. Monitor GPU in container**:\n```bash\ndocker exec -it container_name nvidia-smi\n```\n\n### Your Task\n\nCreate a GPU-enabled Dockerfile that:\n1. Uses NVIDIA CUDA 12.4 runtime base\n2. Installs Python 3.10 and pip\n3. Makes python the default command\n4. Installs PyTorch with matching CUDA version\n5. Copies and runs app.py\n\n**Success criteria**: `torch.cuda.is_available()` returns True and GPU computation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this GPU-enabled Dockerfile\n",
    "%%writefile Dockerfile.gpu\n",
    "# TODO: Create a GPU-enabled Dockerfile\n",
    "# 1. Use nvidia/cuda:12.4.0-runtime-ubuntu22.04 as base\n",
    "# 2. Install Python 3.10 and pip\n",
    "# 3. Install PyTorch with CUDA 12.4 support\n",
    "# 4. Copy and run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile.gpu\n",
    "# Base: NVIDIA CUDA 12.4 runtime on Ubuntu 22.04\n",
    "# This includes CUDA libraries needed for GPU operations\n",
    "FROM nvidia/cuda:12.4.0-runtime-ubuntu22.04\n",
    "\n",
    "# Install Python 3.10 from Ubuntu repositories\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    python3.10 \\\n",
    "    python3-pip \\\n",
    "    && rm -rf /var/lib/apt/lists/*  # Clean up to reduce image size\n",
    "\n",
    "# Make Python 3.10 the default 'python' command\n",
    "RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.10 1\n",
    "RUN update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install PyTorch built for CUDA 12.4\n",
    "# Using specific wheel index ensures CUDA compatibility\n",
    "RUN pip install --no-cache-dir \\\n",
    "    torch>=2.6.0 \\\n",
    "    --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "COPY app.py .\n",
    "\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and run GPU container\n",
    "# The --gpus all flag exposes all GPUs to the container\n",
    "\n",
    "print(\"\"\"\\nTo build and run with GPU:\\n\n",
    "Terminal Commands:\n",
    "==================\n",
    "docker build -t pytorch-gpu -f Dockerfile.gpu .\n",
    "docker run --gpus all --rm pytorch-gpu\n",
    "\n",
    "# To use specific GPU:\n",
    "docker run --gpus '\"device=0\"' --rm pytorch-gpu\n",
    "\n",
    "# To check GPU access without running app:\n",
    "docker run --gpus all --rm pytorch-gpu nvidia-smi\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Create Docker Compose Configuration\n\n**Purpose**: Orchestrate multiple services (training, development, monitoring) with a single command.\n\n### The Multi-Container Challenge\n\nReal ML systems need multiple services:\n- Training container (runs model training)\n- Jupyter container (for experimentation)\n- Database (stores metrics, checkpoints)\n- Monitoring (tracks system health)\n- API server (for inference)\n\n**Without Docker Compose**:\n```bash\ndocker run -d --name db postgres\ndocker run -d --name jupyter ...\ndocker run -d --name trainer ...\n# Manual networking, volumes, env vars\n```\nManaging this manually is error-prone and tedious.\n\n**With Docker Compose**:\n```bash\ndocker-compose up  # Starts everything\n```\nOne command, all services configured and networked automatically.\n\n### Docker Compose Basics\n\n**docker-compose.yml**: YAML file defining your application\n\n**Structure**:\n```yaml\nversion: '3.8'  # Compose file version\n\nservices:       # Define containers\n  service1:\n    # configuration\n  service2:\n    # configuration\n\nvolumes:        # Persistent storage\n  data:\n\nnetworks:       # Custom networks\n  mynetwork:\n```\n\n### Service Configuration Options\n\n**Build context**:\n```yaml\nbuild:\n  context: .              # Where to find Dockerfile\n  dockerfile: Dockerfile.gpu  # Which Dockerfile to use\n```\n\n**Container name**:\n```yaml\ncontainer_name: pytorch_trainer  # Friendly name\n```\nWithout this, Compose generates names like `folder_service_1`\n\n**GPU access**:\n```yaml\nruntime: nvidia           # Enable NVIDIA runtime\nenvironment:\n  - NVIDIA_VISIBLE_DEVICES=all  # Expose all GPUs\n```\nEquivalent to `--gpus all` in docker run\n\n**Volumes**:\n```yaml\nvolumes:\n  - ./data:/app/data      # Host:Container\n  - ./models:/app/models  # Shared between services\n```\nMultiple services can share volumes for data/model exchange\n\n**Shared memory**:\n```yaml\nshm_size: '8gb'  # For PyTorch DataLoader workers\n```\nPrevents \"Bus error\" with multi-worker DataLoader\n\n**Command override**:\n```yaml\ncommand: python train.py  # Override Dockerfile CMD\n```\n\n### Why This Configuration?\n\n**Trainer Service**:\n- Runs training scripts\n- Has GPU access\n- Shares volumes with Jupyter\n- Can save models to shared volume\n\n**Jupyter Service**:\n- Interactive development\n- Also has GPU access\n- Access trained models via shared volume\n- Exposed on port 8888 for browser access\n- Token disabled for convenience (set in production\\!)\n\n### Multi-Line Commands\n\nYAML multi-line with `>`:\n```yaml\ncommand: >\n  bash -c \"pip install jupyter &&\n  jupyter lab --ip=0.0.0.0 --port=8888\"\n```\n\n**Why**:\n- Install additional packages at runtime\n- Chain multiple commands\n- More flexible than baking everything into image\n\n### Service Dependencies\n\n```yaml\ndepends_on:\n  - database\n  - redis\n```\n\n**What it does**:\n- Starts dependencies first\n- **Note**: Doesn't wait for service to be ready (just started)\n\n**For ML pipelines**:\n```yaml\ntrainer:\n  depends_on:\n    - mlflow  # Metrics tracking\n```\n\n### Docker Compose Commands\n\n**Start all services (detached)**:\n```bash\ndocker-compose up -d\n```\n- `-d`: Runs in background\n- Without `-d`: Shows logs (Ctrl+C stops all)\n\n**View logs**:\n```bash\ndocker-compose logs -f trainer    # Follow trainer logs\ndocker-compose logs --tail=50 jupyter  # Last 50 lines\n```\n\n**Stop services**:\n```bash\ndocker-compose stop   # Stop but don't remove\ndocker-compose down   # Stop and remove containers\n```\n\n**Rebuild after changes**:\n```bash\ndocker-compose up -d --build  # Rebuild images\n```\n\n**Scale services**:\n```bash\ndocker-compose up -d --scale trainer=3  # 3 training containers\n```\nUseful for distributed training\n\n**Execute commands in running service**:\n```bash\ndocker-compose exec trainer python check_gpu.py\ndocker-compose exec jupyter bash\n```\n\n### Accessing Jupyter Lab\n\nAfter `docker-compose up -d`:\n\n1. Open browser\n2. Go to `http://localhost:8888`\n3. Start experimenting\\!\n\n**Files in `/app/notebooks`** inside container = `./notebooks` on host\n\n### Production Considerations\n\n**Security** (for production):\n```yaml\ncommand: >\n  jupyter lab\n  --NotebookApp.token='your-secure-token'\n  --NotebookApp.password='hashed-password'\n```\n\n**Resource limits**:\n```yaml\ndeploy:\n  resources:\n    limits:\n      cpus: '4.0'\n      memory: 16G\n```\n\n**Restart policy**:\n```yaml\nrestart: unless-stopped  # Auto-restart on failure\n```\n\n### Your Task\n\nCreate a docker-compose.yml with:\n1. **Trainer service**: GPU-enabled, runs training\n2. **Jupyter service**: GPU-enabled, port 8888, interactive\n3. **Shared volumes**: For data, models, notebooks\n4. **Proper networking**: Services can communicate\n\n**Success**: Both services start, Jupyter accessible in browser, shared volumes work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-compose.yml\n",
    "# TODO: Create a docker-compose.yml with:\n",
    "# 1. Training service with GPU support\n",
    "# 2. Jupyter Lab service on port 8888\n",
    "# 3. Shared volumes for data and models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-compose.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  # Model training service\n",
    "  trainer:\n",
    "    build:\n",
    "      context: .              # Build from current directory\n",
    "      dockerfile: Dockerfile.gpu\n",
    "    container_name: pytorch_trainer\n",
    "    runtime: nvidia           # Enable NVIDIA GPU support\n",
    "    environment:\n",
    "      - NVIDIA_VISIBLE_DEVICES=all  # Expose all GPUs\n",
    "    volumes:\n",
    "      - ./data:/app/data      # Mount data directory\n",
    "      - ./models:/app/models  # Mount models directory\n",
    "    shm_size: '8gb'          # Shared memory for DataLoader workers\n",
    "    command: python train.py # Override default CMD\n",
    "\n",
    "  # Jupyter Lab service for interactive development\n",
    "  jupyter:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile.gpu\n",
    "    container_name: jupyter_lab\n",
    "    runtime: nvidia\n",
    "    environment:\n",
    "      - NVIDIA_VISIBLE_DEVICES=all\n",
    "    ports:\n",
    "      - \"8888:8888\"          # Expose Jupyter on host port 8888\n",
    "    volumes:\n",
    "      - ./notebooks:/app/notebooks\n",
    "      - ./data:/app/data\n",
    "      - ./models:/app/models\n",
    "    command: >\n",
    "      bash -c \"pip install jupyter &&\n",
    "      jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token=''\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker Compose commands\n",
    "print(\"\"\"\\nDocker Compose Commands:\\n\n",
    "Start all services:\n",
    "  docker-compose up -d\n",
    "\n",
    "View logs:\n",
    "  docker-compose logs -f trainer\n",
    "  docker-compose logs -f jupyter\n",
    "\n",
    "Stop all services:\n",
    "  docker-compose down\n",
    "\n",
    "Rebuild and start:\n",
    "  docker-compose up -d --build\n",
    "\n",
    "Access Jupyter Lab:\n",
    "  http://localhost:8888\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Multi-Stage Build for Image Optimization\n\n**Purpose**: Dramatically reduce Docker image sizes by separating build and runtime environments.\n\n### The Image Size Problem\n\n**Typical ML image build**:\n```dockerfile\nFROM python:3.10\nRUN pip install torch transformers numpy pandas scikit-learn\nCOPY . .\n```\n\n**Result**: 5-10GB image containing:\n- Build tools (gcc, make, pip)\n- Package manager caches\n- Source files from installations\n- Intermediate build artifacts\n- Everything, even if only needed during installation\n\n**Problems**:\n- Slow to download (minutes on slow connections)\n- Expensive to store (cloud storage costs)\n- More attack surface (extra software = more vulnerabilities)\n- Slower to start (container needs to extract GBs)\n\n### What Are Multi-Stage Builds?\n\n**Concept**: Use multiple FROM statements in one Dockerfile\n\n**Stage 1 (builder)**: Full environment with build tools\n- Install packages\n- Compile code\n- Download dependencies\n- All the heavy lifting\n\n**Stage 2 (runtime)**: Minimal environment\n- Copy only installed packages from Stage 1\n- No build tools\n- No caches\n- Just what's needed to run\n\n**Magic**: Stage 1 artifacts are discarded, only Stage 2 becomes the final image\\!\n\n### Size Comparison\n\n**Single-stage**:\n```dockerfile\nFROM python:3.10\nRUN pip install torch transformers\n```\nSize: ~5-7GB\n\n**Multi-stage**:\n```dockerfile\nFROM python:3.10 as builder\nRUN pip install --user torch transformers\n\nFROM python:3.10-slim\nCOPY --from=builder /root/.local /root/.local\n```\nSize: ~2-3GB (40-60% smaller\\!)\n\n### How Multi-Stage Works\n\n**Stage 1: Builder**\n```dockerfile\nFROM python:3.10 as builder  # Named 'builder'\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --user --no-cache-dir -r requirements.txt\n```\n\n**Key points**:\n- `as builder`: Names this stage\n- `--user`: Installs to /root/.local instead of system-wide\n- `--no-cache-dir`: Don't save pip cache (saves ~100-500MB)\n- Full `python:3.10` image (has compilers, headers)\n\n**Stage 2: Runtime**\n```dockerfile\nFROM python:3.10-slim  # Fresh, minimal base\nWORKDIR /app\nCOPY --from=builder /root/.local /root/.local  # Magic line\\!\nENV PATH=/root/.local/bin:$PATH\nCOPY app.py .\n```\n\n**Key points**:\n- New FROM = new base, previous stage discarded\n- `--from=builder`: Copy from named stage\n- Only copies installed packages, not build tools\n- `ENV PATH`: Makes copied executables findable\n\n### The --user Flag Explained\n\n**Without --user**:\n```bash\npip install torch  # Installs to /usr/local/lib/python3.10/...\n```\nScattered across system directories, hard to copy cleanly\n\n**With --user**:\n```bash\npip install --user torch  # Installs to /root/.local/\n```\nEverything in one directory:\n- /root/.local/lib/python3.10/site-packages/\n- /root/.local/bin/\n\nEasy to copy in one COPY command\\!\n\n### Why PATH Update Is Needed\n\nAfter copying /root/.local:\n```dockerfile\nENV PATH=/root/.local/bin:$PATH\n```\n\n**Why**:\n- Executables like `torch-config` are in /root/.local/bin\n- Without PATH update, they won't be found\n- System will error with \"command not found\"\n\n### Advanced Multi-Stage Patterns\n\n**Multiple builders**:\n```dockerfile\nFROM node:18 as frontend-builder\nRUN npm install && npm run build\n\nFROM python:3.10 as backend-builder\nRUN pip install --user -r requirements.txt\n\nFROM python:3.10-slim\nCOPY --from=frontend-builder /app/dist /app/static\nCOPY --from=backend-builder /root/.local /root/.local\n```\nCombine artifacts from multiple build stages\\!\n\n**Compile from source**:\n```dockerfile\nFROM python:3.10 as builder\nRUN apt-get update && apt-get install -y build-essential\nRUN pip wheel --no-cache-dir --wheel-dir=/wheels torch\n\nFROM python:3.10-slim\nCOPY --from=builder /wheels /wheels\nRUN pip install --no-cache /wheels/*\n```\n\n### Best Practices\n\n**1. Use slim/alpine for final stage**:\n```dockerfile\nFROM python:3.10         # Builder: 900MB\nFROM python:3.10-slim    # Runtime: 120MB \u2705\nFROM python:3.10-alpine  # Runtime: 50MB (but compatibility issues)\n```\n\n**2. Order matters**:\n```dockerfile\n# Copy dependencies first (changes less often)\nCOPY --from=builder /root/.local /root/.local\n# Copy code last (changes frequently)\nCOPY app.py .\n```\nBetter layer caching = faster rebuilds\n\n**3. Clean up in builder**:\n```dockerfile\nRUN pip install --user --no-cache-dir -r requirements.txt && \\\n    rm -rf /root/.cache\n```\n\n**4. Security**:\n- Fewer packages in final image = smaller attack surface\n- No compilers/build tools in production\n- Scan images: `docker scan myimage`\n\n### Real-World Impact\n\n**Company example**:\n- Before: 8GB image, 5 min deploy time\n- After multi-stage: 2GB image, 1 min deploy time\n- Savings: $1000s/month in bandwidth and storage\n\n**Research example**:\n- Sharing 10GB images on slow university networks: hours\n- 2GB images: minutes\n- Reproducibility improved (people actually download and test)\n\n### Verification\n\n**Compare sizes**:\n```bash\ndocker images | grep pytorch\npytorch-single    latest   6.2GB\npytorch-multi     latest   2.1GB  # 65% smaller\\!\n```\n\n**Verify it works**:\n```bash\ndocker run --rm pytorch-multi python -c \"import torch; print(torch.__version__)\"\n```\n\n### Your Task\n\nCreate a multi-stage Dockerfile:\n1. **Stage 1 (builder)**: Use full Python 3.10, install packages with --user\n2. **Stage 2 (runtime)**: Use Python 3.10-slim, copy only installed packages\n3. Update PATH so executables are found\n4. Copy application code\n\n**Success**: Image is significantly smaller but functionally identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile.multistage\n",
    "# TODO: Create a multi-stage Dockerfile\n",
    "# Stage 1: Build dependencies\n",
    "# Stage 2: Minimal runtime with only installed packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile.multistage\n",
    "# ============================================\n",
    "# Stage 1: Builder - Install dependencies\n",
    "# ============================================\n",
    "FROM python:3.10 as builder\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies in user directory\n",
    "# Using --user puts packages in /root/.local\n",
    "COPY requirements.txt .\n",
    "RUN pip install --user --no-cache-dir -r requirements.txt\n",
    "\n",
    "# ============================================\n",
    "# Stage 2: Runtime - Minimal final image\n",
    "# ============================================\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy only installed packages from builder stage\n",
    "# This excludes pip, setuptools, and other build tools\n",
    "COPY --from=builder /root/.local /root/.local\n",
    "\n",
    "# Update PATH to find installed packages\n",
    "ENV PATH=/root/.local/bin:$PATH\n",
    "\n",
    "# Copy application code\n",
    "COPY app.py .\n",
    "\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare image sizes\n",
    "print(\"\"\"\\nCompare single-stage vs multi-stage image sizes:\\n\n",
    "Build both:\n",
    "  docker build -t pytorch-single -f Dockerfile.basic .\n",
    "  docker build -t pytorch-multi -f Dockerfile.multistage .\n",
    "\n",
    "Check sizes:\n",
    "  docker images | grep pytorch\n",
    "\n",
    "You should see pytorch-multi is smaller!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Dependency Management with requirements.txt\n\n**Purpose**: Pin dependencies for reproducible builds and faster Docker caching.\n\n### Why requirements.txt?\n\n**Without it**:\n```dockerfile\nRUN pip install torch transformers numpy pandas\n```\n\n**Problems**:\n- Installs latest versions (breaks reproducibility)\n- Docker layer rebuilds on any Dockerfile change\n- No documentation of dependencies\n- Hard to update selectively\n\n**With requirements.txt**:\n```dockerfile\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n```\n\n**Benefits**:\n- Version-controlled with code\n- Docker caches layer until requirements.txt changes\n- Clear documentation of dependencies\n- Easy to diff and review changes\n\n### Version Pinning Strategies\n\n**Exact version** (most strict):\n```\ntorch==2.6.0\n```\n- Guarantees exact same version\n- Safest for production\n- Doesn't get bug fixes automatically\n\n**Compatible version** (recommended):\n```\ntorch>=2.6.0\n```\n- Allows patch updates (2.6.1, 2.6.2)\n- Gets bug fixes\n- Won't break compatibility (semantic versioning)\n\n**Range** (flexible):\n```\ntorch>=2.6.0,<3.0.0\n```\n- Allows minor updates\n- Avoids major version breaking changes\n\n**No pin** (dangerous):\n```\ntorch  # Gets whatever is latest\n```\n- Never use in production\\!\n- \"Works on my machine\" guaranteed\n\n### Organizing requirements.txt\n\n**Good structure** (commented and grouped):\n```\n# Deep Learning Framework\ntorch>=2.6.0\ntorchvision>=0.19.0\n\n# Transformers and NLP\ntransformers>=4.40.0\ntokenizers>=0.15.0\n\n# Data Science\nnumpy>=1.24.0\npandas>=2.0.0\n```\n\n**Benefits**:\n- Easy to understand what each group is for\n- Can comment out entire sections for testing\n- New team members understand dependencies\n\n### requirements.txt in Dockerfile\n\n**Optimal pattern** for Docker caching:\n```dockerfile\n# Copy requirements first (changes infrequently)\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy code last (changes frequently)\nCOPY . .\n```\n\n**Why this order**:\n1. If only code changes: Cached dependencies, fast rebuild\n2. If requirements change: Reinstall dependencies, slower\n3. Maximizes cache hit rate\n\n**Bad order**:\n```dockerfile\nCOPY . .  # Copies everything including requirements.txt\nRUN pip install -r requirements.txt  # Rebuilds on any file change\n```\nEvery code edit = reinstall ALL dependencies\\!\n\n### Multiple requirements Files\n\n**Common pattern for different environments**:\n\n**requirements.txt** (production):\n```\ntorch>=2.6.0\ntransformers>=4.40.0\n```\n\n**requirements-dev.txt** (development):\n```\n-r requirements.txt  # Include production requirements\npytest>=8.0.0\nblack>=24.0.0\njupyter>=1.0.0\n```\n\n**requirements-gpu.txt** (GPU builds):\n```\ntorch>=2.6.0 --index-url https://download.pytorch.org/whl/cu124\n```\n\n### Generating requirements.txt\n\n**From current environment**:\n```bash\npip freeze > requirements.txt\n```\nCaptures exact versions of everything installed\n\n**Problem**: Includes transitive dependencies\n```\ntorch==2.6.0\nnvidia-cublas-cu12==12.4.5.8  # Transitive dependency\nnvidia-cuda-cupti-cu12==12.4.127  # Transitive\n# ... 50+ more packages\n```\n\n**Better**: Manual requirements with top-level only:\n```\ntorch>=2.6.0\ntransformers>=4.40.0\n```\nLet pip resolve transitive dependencies\n\n### pip-compile for Lock Files\n\n**Advanced**: Use pip-tools\n\n**requirements.in** (what you want):\n```\ntorch>=2.6.0\ntransformers\n```\n\n**Generate locked requirements.txt**:\n```bash\npip-compile requirements.in\n```\n\n**Output** (requirements.txt with all transitive deps pinned):\n```\ntorch==2.6.0\ntransformers==4.40.0\ntokenizers==0.15.0\n# ... all dependencies with exact versions\n```\n\n**Benefits**: Reproducible + documented + updateable\n\n### Common Pitfalls\n\n**1. Platform-specific packages**:\n```\npywin32==306  # Only works on Windows\\!\n```\nSolution: Use environment markers\n```\npywin32==306; sys_platform == 'win32'\n```\n\n**2. Missing index URLs**:\n```\ntorch>=2.6.0  # Might get CPU version\n```\nSolution: Specify in Dockerfile\n```dockerfile\nRUN pip install -r requirements.txt \\\n    --index-url https://download.pytorch.org/whl/cu124\n```\n\n**3. Conflicting versions**:\n```\npackageA>=2.0  # Requires numpy<2.0\npackageB>=3.0  # Requires numpy>=2.0\n```\npip will error. Solution: Check compatibility, adjust versions\n\n### Your Task\n\nThis cell creates a sample requirements.txt with:\n- Deep learning frameworks (PyTorch, torchvision)\n- NLP libraries (transformers)\n- Data science tools (numpy, pandas, scikit-learn)\n- Version pins using >= for compatibility\n- Comments organizing by category\n\n**Use this pattern in your projects** for reproducible builds\\!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.docker.txt\n",
    "# Deep Learning Framework\n",
    "torch>=2.6.0\n",
    "torchvision>=0.19.0\n",
    "\n",
    "# Transformers for NLP\n",
    "transformers>=4.40.0\n",
    "\n",
    "# Data Science\n",
    "numpy>=1.24.0\n",
    "pandas>=2.0.0\n",
    "scikit-learn>=1.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: Optimizing Builds with .dockerignore\n\n**Purpose**: Exclude unnecessary files from Docker build context to speed up builds and reduce image sizes.\n\n### The Build Context Problem\n\nWhen you run `docker build .`, Docker sends **all files** in current directory to the Docker daemon:\n\n```bash\ndocker build .\nSending build context to Docker daemon  15.2GB  # Oops\\!\n```\n\n**What gets sent**:\n- All source code \u2705 (needed)\n- Git history (.git/) \u274c (1-2GB, not needed)\n- Virtual environments (venv/) \u274c (500MB-2GB, not needed)\n- Jupyter checkpoints \u274c (100MB+, not needed)\n- Dataset files (data/) \u274c (GBs, mounted as volume instead)\n- Model checkpoints \u274c (GBs, should be in separate storage)\n\n**Impact**:\n- Build takes minutes just uploading context\n- Network bandwidth wasted\n- Larger images if files are COPYed\n- Slower CI/CD pipelines\n\n### What is .dockerignore?\n\nLike .gitignore but for Docker builds:\n\n**Create `.dockerignore` file**:\n```\n__pycache__\n*.pyc\n.git\nvenv/\n```\n\n**These files won't be sent to Docker daemon**:\n- Faster builds\n- Smaller build context\n- Can't accidentally COPY sensitive files\n\n### Essential Patterns to Ignore\n\n**Python cache files** (100s of MB):\n```\n__pycache__\n*.pyc\n*.pyo\n*.pyd\n.Python\n```\nRegenerated automatically, no need to copy\n\n**Virtual environments** (GBs):\n```\n.env\n.venv\nvenv/\nENV/\nenv/\n```\nContainer has its own Python environment\n\n**Git files** (GBs):\n```\n.git\n.gitignore\n.gitattributes\n```\nGit history not needed in container\n\n**IDE files** (MBs):\n```\n.vscode/\n.idea/\n*.swp\n*.swo\n.DS_Store\n```\nEditor-specific, not needed for running code\n\n**Jupyter notebooks** (optional):\n```\n*.ipynb\n.ipynb_checkpoints\n```\nUsually for development, not deployment\n\n**Testing files**:\n```\n.pytest_cache\n.coverage\nhtmlcov/\n.tox/\n```\nTests run in CI, not in container\n\n**Build artifacts**:\n```\ndist/\nbuild/\n*.egg-info/\n.eggs/\n```\nRebuilt inside container\n\n**Documentation**:\n```\n*.md\ndocs/\nREADME.md\n```\nNot needed for running code (unless serving docs)\n\n### Whitelisting Pattern\n\n**Ignore everything except specific files**:\n```\n# Ignore everything\n*\n\n# Except these\n\\!app.py\n\\!requirements.txt\n\\!src/\n```\n\nUseful for monorepos where you only need subset of files\n\n### Special Cases for ML Projects\n\n**Large datasets** (DON'T include in image):\n```\ndata/\ndatasets/\n*.csv\n*.parquet\n```\n**Why**: GBs of data \u2192 huge images\n**Instead**: Mount as volume or download at runtime\n\n**Model checkpoints** (usually don't include):\n```\n*.pt\n*.pth\n*.ckpt\n*.safetensors\nmodels/\ncheckpoints/\n```\n**Why**: Models are GBs, change frequently\n**Instead**: Mount volume or download from model registry\n\n**Exception**: Small models (<100MB) for inference containers\n\n**Logs** (never include):\n```\n*.log\nlogs/\n```\n**Why**: Container logs should go to stdout/stderr\n\n### Verification\n\n**Check what's being sent**:\n```bash\ndocker build --no-cache . 2>&1 | grep 'Sending build context'\n```\n\n**Before .dockerignore**:\n```\nSending build context to Docker daemon  8.5GB\n```\n\n**After .dockerignore**:\n```\nSending build context to Docker daemon  2.1MB  # 4000x smaller\\!\n```\n\n**List files in build context**:\n```bash\ndocker run --rm -v $(pwd):/check alpine sh -c 'cd /check && du -sh * | sort -h'\n```\n\n### Common Mistakes\n\n**1. .dockerignore in wrong location**:\n- Must be in build context root\n- If `docker build path/to/project`, put it in `path/to/project/.dockerignore`\n\n**2. Ignoring required files**:\n```\n*.py  # Oops\\! Ignores all Python files\n```\nBuild succeeds but image is broken\n\n**3. Not testing .dockerignore**:\n- Always rebuild after adding .dockerignore\n- Verify image still works\n\n**4. Overusing wildcards**:\n```\n*test*  # Ignores testing/ but also pytest.py\\!\n```\nBe specific\n\n### Production Best Practices\n\n**1. Start with comprehensive .dockerignore**:\n- Copy from this exercise\n- Adapt to your project\n- Keep it in version control\n\n**2. Regular audits**:\n```bash\ndocker history myimage  # See what layers contain\ndive myimage            # Explore image layers (tool)\n```\n\n**3. CI/CD checks**:\n```bash\n# Fail if build context > 100MB\nSIZE=$(docker build . 2>&1 | grep 'Sending' | awk '{print $5}')\nif [ $SIZE -gt 100 ]; then\n  echo 'Build context too large\\!'\n  exit 1\nfi\n```\n\n**4. Security**:\n```\n.env         # Don't copy secrets\n*.key\n*.pem\ncredentials/\n```\n\n### Real-World Impact\n\n**Startup example**:\n- Before: 10GB context, 15 min builds\n- After .dockerignore: 5MB context, 2 min builds\n- CI/CD pipeline: 85% faster\n\n**Research lab**:\n- Accidentally included 50GB dataset\n- Docker build OOM on CI server\n- .dockerignore fixed it immediately\n\n### Your Task\n\nThis cell creates a comprehensive .dockerignore covering:\n- Python cache and compiled files\n- Virtual environments\n- Git files\n- IDE configurations\n- Jupyter notebooks and checkpoints\n- Testing artifacts\n- Build outputs\n- OS-specific files\n- Documentation\n\n**Copy this to your projects** and customize as needed\\!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .dockerignore\n",
    "# Python cache and compiled files\n",
    "__pycache__\n",
    "*.pyc\n",
    "*.pyo\n",
    "*.pyd\n",
    ".Python\n",
    "\n",
    "# Virtual environments\n",
    ".env\n",
    ".venv\n",
    "venv/\n",
    "ENV/\n",
    "\n",
    "# Git\n",
    ".git\n",
    ".gitignore\n",
    ".gitattributes\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints\n",
    "*.ipynb\n",
    "\n",
    "# Testing\n",
    ".pytest_cache\n",
    ".coverage\n",
    "htmlcov/\n",
    "\n",
    "# Build artifacts\n",
    "dist/\n",
    "build/\n",
    "*.egg-info/\n",
    "\n",
    "# IDE\n",
    ".vscode/\n",
    ".idea/\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# Documentation\n",
    "*.md\n",
    "docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Practiced:\n",
    "\n",
    "1. \u2705 **Basic Dockerfiles**: Created CPU-based PyTorch containers\n",
    "2. \u2705 **GPU Support**: Configured CUDA-enabled containers for deep learning\n",
    "3. \u2705 **Docker Compose**: Orchestrated multi-service applications\n",
    "4. \u2705 **Multi-Stage Builds**: Optimized image sizes\n",
    "5. \u2705 **Best Practices**: Used .dockerignore and requirements.txt\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Dockerfiles** define how to build images\n",
    "- **GPU support** requires NVIDIA runtime and CUDA base images\n",
    "- **Docker Compose** simplifies multi-container orchestration\n",
    "- **Multi-stage builds** reduce production image sizes\n",
    "- **.dockerignore** improves build performance and security\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Review**: Study [dockerization.md](./dockerization.md) for detailed concepts\n",
    "2. **Practice**: Build and run these containers in your environment\n",
    "3. **Experiment**: Try different base images and configurations\n",
    "4. **Deploy**: Containerize your own AI projects\n",
    "\n",
    "### Useful Commands Reference:\n",
    "\n",
    "```bash\n",
    "# Build image\n",
    "docker build -t <name> -f <dockerfile> .\n",
    "\n",
    "# Run container\n",
    "docker run --rm <name>\n",
    "docker run --gpus all --rm <name>  # With GPU\n",
    "\n",
    "# Docker Compose\n",
    "docker-compose up -d\n",
    "docker-compose logs -f <service>\n",
    "docker-compose down\n",
    "\n",
    "# Management\n",
    "docker images\n",
    "docker ps\n",
    "docker system prune -a  # Clean up\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}